[
  {
    "path": "posts/2021-11-20-fitting-a-multiple-regression-with-torch/",
    "title": "Fitting a Multiple Regression with Torch",
    "description": "Learning torch by fitting a multiple regression model",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-11-20",
    "categories": [],
    "contents": "\r\nIn this post, I want to play around with the {torch} package a little bit by fitting a multiple regression model “by hand” (sort of) using torch and the Adam optimizer.\r\nA few warnings/disclaimers right up front:\r\nI’m using this post as a way to explore and learn how {torch} works. I’m by no means an expert. I’m sure there are more concise/more idiomatic ways to do these things. And if you know about them, I’d love for you to show me!\r\nI don’t think fitting a multiple regression (and particularly this multiple regression) through {torch} is really worthwhile. But it felt like a way to dig into the package a little bit in a way that didn’t involve loading MNIST and following a canned tutorial.\r\nThere are lots of data cleaning/exploration steps I’m just flat out skipping here.\r\nAll of that said, if you’re still with me, let’s dive in.\r\nLoading Data\r\nFor this project, I’m going to use some ultramarathon data from #TidyTuesday a few weeks ago. So the first step is loading that in and setting some plot options and whatnot.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc) #for ggplot theme\r\nlibrary(harrypotter) #for colors\r\nlibrary(janitor)\r\nlibrary(torch)\r\n\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 2, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\nultra_rankings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv')\r\nrace <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv')\r\n\r\n\r\n\r\nThen, let’s take a little peeksie at the data. The first dataframe, ultra_rankings, provides data for each runner in each race.\r\n\r\n\r\nglimpse(ultra_rankings)\r\n\r\n\r\nRows: 137,803\r\nColumns: 8\r\n$ race_year_id    <dbl> 68140, 68140, 68140, 68140, 68140, 68140, 68~\r\n$ rank            <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, N~\r\n$ runner          <chr> \"VERHEUL Jasper\", \"MOULDING JON\", \"RICHARDSO~\r\n$ time            <chr> \"26H 35M 25S\", \"27H 0M 29S\", \"28H 49M 7S\", \"~\r\n$ age             <dbl> 30, 43, 38, 55, 48, 31, 55, 40, 47, 29, 48, ~\r\n$ gender          <chr> \"M\", \"M\", \"M\", \"W\", \"W\", \"M\", \"W\", \"W\", \"M\",~\r\n$ nationality     <chr> \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"G~\r\n$ time_in_seconds <dbl> 95725, 97229, 103747, 111217, 117981, 118000~\r\n\r\nLet’s also peek at the race data, which provides data about races:\r\n\r\n\r\nglimpse(race)\r\n\r\n\r\nRows: 1,207\r\nColumns: 13\r\n$ race_year_id   <dbl> 68140, 72496, 69855, 67856, 70469, 66887, 678~\r\n$ event          <chr> \"Peak District Ultras\", \"UTMB®\", \"Grand Raid ~\r\n$ race           <chr> \"Millstone 100\", \"UTMB®\", \"Ultra Tour 160\", \"~\r\n$ city           <chr> \"Castleton\", \"Chamonix\", \"vielle-Aure\", \"Asen~\r\n$ country        <chr> \"United Kingdom\", \"France\", \"France\", \"Bulgar~\r\n$ date           <date> 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-~\r\n$ start_time     <time> 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:0~\r\n$ participation  <chr> \"solo\", \"Solo\", \"solo\", \"solo\", \"solo\", \"solo~\r\n$ distance       <dbl> 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163~\r\n$ elevation_gain <dbl> 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630~\r\n$ elevation_loss <dbl> -4520, -9930, -9980, -7500, -100, -9850, -546~\r\n$ aid_stations   <dbl> 10, 11, 13, 13, 12, 15, 5, 8, 13, 23, 13, 5, ~\r\n$ participants   <dbl> 150, 2300, 600, 150, 0, 300, 0, 200, 120, 100~\r\n\r\nExploring Data\r\nAgain, I’m skipping this, but you should definitely do some exploration before building a model. :)\r\nModeling with Torch\r\nSo, the idea here is to use {torch} to “manually” (sort of) estimate a multiple linear regression model. The first thing I’m going to do is refine a dataframe to use in the model. There are lots of possibilities here, but I’m going to choose to estimate model that predicts the winning time of a race from the race distance, the total elevation gain, and the total elevation loss. So let’s and filter our data down to what we’ll actually use in our model.\r\n\r\n\r\nultra_mod_df <- ultra_rankings %>%\r\n  left_join(race, by = \"race_year_id\") %>%\r\n  filter(rank == 1) %>%\r\n  select(time_in_seconds, distance, elevation_gain, elevation_loss)\r\n\r\n\r\n\r\nNext, I’m going to drop any observations with missing values on any of these variables. I’m also going to normalize the variables, because my understanding is that this matters quite a bit for optimizing via gradient descent (and it’s also good practice for linear models in general).\r\n\r\n\r\nultra_normed <- ultra_mod_df %>%\r\n  drop_na() %>%\r\n  mutate(across(1:4, function(x) {(mean(x) - x)/sd(x)}))\r\n\r\n\r\n\r\nCreating a Dataset\r\nRight, so, now we can get into the torch-y stuff. The first step is to use the dataset() constructor to build a dataset. According to the torch documentation, this requires following a few conventions. More specifically, we need to establish an initialize() function, a .getitem() function, and a .length() function.\r\nBasically, these do the following:\r\ninitialize() creates x (predictor) and y (outcome) tensors from the data;\r\n.getitem() provides a way to return the x and y values for an item when provided an index (or multiple indices) by the user;\r\n.length() tells us how many observations we have in the data\r\nWe can also define helper functions within dataset() as well (e.g. preprocessors for our data). I’m not going to do that here (since we’ve already lightly preprocessed our data), but I could if I wanted.\r\n\r\n\r\n#initializing dataset\r\n\r\nultra_dataset <- dataset(\r\n  \r\n  name = \"ultra_dataset\",\r\n  \r\n  initialize = function(df) {\r\n    self$x <- df %>%\r\n      select(-time_in_seconds) %>%\r\n      as.matrix() %>%\r\n      torch_tensor()\r\n    \r\n    self$y <- torch_tensor(df$time_in_seconds)\r\n    \r\n  },\r\n    \r\n    .getitem = function(i) {\r\n      x <- self$x[i, ]\r\n      y <- self$y[i]\r\n      \r\n      list(x, y)\r\n    },\r\n    \r\n    .length = function() {\r\n      self$y$size()[[1]]\r\n    }\r\n)\r\n\r\n\r\n\r\nLet’s see what this looks like. We’ll create a tensor dataset from the full ultra_normed data and then return its length:\r\n\r\n\r\nultra_tensor_df <- ultra_dataset(ultra_normed)\r\n\r\n\r\nultra_len <- ultra_tensor_df$.length()\r\n#note that this is the same as: length(ultra_tensor_df)\r\n\r\nultra_len\r\n\r\n\r\n[1] 1237\r\n\r\nWe can also pull out a single observation if we want, and the result will give us the values in the X tensor and the y tensor:\r\n\r\n\r\nultra_tensor_df$.getitem(1)\r\n\r\n\r\n[[1]]\r\ntorch_tensor\r\n-0.3624\r\n 0.2811\r\n-0.2866\r\n[ CPUFloatType{3} ]\r\n\r\n[[2]]\r\ntorch_tensor\r\n-0.809365\r\n[ CPUFloatType{} ]\r\n\r\n#note that 1 here refers to the index of the item\r\n\r\n\r\n\r\nNext, let’s make train and validation datasets.\r\n\r\n\r\nset.seed(0408)\r\ntrain_ids <- sample(1:ultra_len, floor(.8*ultra_len))\r\nvalid_ids <- setdiff(1:ultra_len, train_ids)\r\n\r\ntrn <- ultra_dataset(ultra_normed[train_ids, ])\r\nvld <- ultra_dataset(ultra_normed[valid_ids, ])\r\n\r\n\r\n\r\nThis would be the point where we could also define a dataloader to train on batches of the data, but I’m not going to do that here because we can just train on the entire dataset at once.\r\nDefining a Model\r\nNow, let’s define our model. Again, for our learning purposes today, this is just going to be a plain old multiple regression model. To implement this in {torch}, we can define the model as follows:\r\n\r\n\r\nlin_mod <- function(x, w, b) {\r\n  torch_mm(w, x) + b\r\n}\r\n\r\n\r\n\r\nIn this model, we’re taking a vector of weights (or slopes), w, multiplying it by our input matrix, x, and adding our bias (or intercept). The torch_mm() function lets us perform this matrix multiplication.\r\nNow that we’ve defined this model, let’s create our w and b parameters. Since this is a linear regression, each predictor in our model will have a single weight associated with it, and we’ll have a single intercept for the model. We’ll just use 1 as the starting value for our w parameters and 0 as the starting value for our b parameter.\r\n\r\n\r\n#defining parameters\r\nnum_feats <- 3\r\n\r\nw <- torch_ones(c(1, num_feats))\r\nb <- torch_zeros(1)\r\n\r\n\r\n\r\nNow we can do a quick test to make sure everything fits together. We’re not actually training our model at this point, but I want to just run a small sample of our training data through the model (with the parameter starting values) to make sure we don’t get any errors.\r\nNote that I need to transpose the X matrix for the multiplication to work.\r\n\r\n\r\naa <- trn$.getitem(1:10)\r\n\r\naa_x <- torch_transpose(aa[[1]], 1, 2)\r\n\r\nt_out <- lin_mod(aa_x, w, b)\r\n\r\nt_out\r\n\r\n\r\ntorch_tensor\r\n-0.2315 -0.4256 -0.7237 -0.2182 -0.2279 -0.2015 -0.2869 -0.2124 -0.5832 -0.2300\r\n[ CPUFloatType{1,10} ]\r\n\r\nGreat! This gives us a single output for each of our input observations, which is what we want.\r\nTraining the Model\r\nNow that we have a model and can feed data into the model, let’s train it.\r\nTraining the model involves using gradient descent, an optimizer, a loss function, and backpropagation to slowly tweak our parameters until they reach their optimal values (i.e. those that minimize loss). I’m not going to do a super deep dive into what all of that means, but basically in our training loop we’re going to:\r\nRun the data through the model and get predictions;\r\nMeasure how good our predictions are (via the loss function);\r\nCompute the gradient of the loss with respect to the parameters (via the backward() method);\r\nTell our optimizer to update the parameters (via optimizer$step());\r\nRepeat a bunch of times\r\nThat’s basically what the code below does. A few little extra things to point out, thought:\r\nIn addition to training the model on the training data, I’m also getting predictions on the validation data during each iteration of the training process. This won’t influence the training at all, but it’ll give us a look at how the model does on a holdout set of data throughout the entire process.\r\nThe torch_squeeze() function just removes an unnecessary dimension from the predictions tensors.\r\nI’ve also created lists to track training loss, validation loss, and parameter values throughout the fitting, and these get recorded on each pass through the training loop.\r\n\r\n\r\n#recreate our parameters with the requires_grad attribute\r\nw <- torch_zeros(c(1, num_feats), requires_grad = TRUE)\r\nb <- torch_zeros(1, requires_grad = TRUE)\r\n\r\n#put the parameters in a list\r\nparams <- list(w, b)\r\n\r\n#define our optimizer\r\noptimizer <- optim_adam(params, lr = .1)\r\n\r\n#create lists to track values during the training\r\nloss_tracking <- list()\r\nparams_tracking <- list()\r\nvld_loss_tracking <- list()\r\n\r\n#training loop\r\nfor (i in 1:1000) {\r\n  \r\n  optimizer$zero_grad()\r\n  \r\n  x <- torch_transpose(trn$x, 1, 2)\r\n  vld_x <- torch_transpose(vld$x, 1, 2)\r\n  \r\n  preds <- lin_mod(x, w, b)\r\n  vld_preds <- lin_mod(vld_x, w, b)\r\n  \r\n  preds <- torch_squeeze(preds)\r\n  vld_preds <- torch_squeeze(vld_preds)\r\n  \r\n  current_loss <- nnf_mse_loss(preds, trn$y)\r\n  vld_loss <- nnf_mse_loss(vld_preds, vld$y)\r\n  \r\n  loss_tracking[i] <- current_loss$item()\r\n  vld_loss_tracking[i] <- vld_loss$item()\r\n  params_tracking[i] <- list(c(as.numeric(params[[1]]), as.numeric(params[[2]])))\r\n  \r\n  current_loss$backward()\r\n  \r\n  optimizer$step()\r\n  \r\n}\r\n\r\n\r\n\r\nInvestigating our Results\r\nCool stuff – our model has finished training now. Let’s take a look at our final parameter values. In a little while, we’ll also compare these to values we get from fitting a multiple regression using the lm() function.\r\n\r\n\r\nbetas <- tibble(\r\n  term = c(names(ultra_normed)[2:4], \"intercept\"),\r\n  size = params_tracking[[1000]]\r\n)\r\n\r\nbetas\r\n\r\n\r\n# A tibble: 4 x 2\r\n  term              size\r\n  <chr>            <dbl>\r\n1 distance       -0.154 \r\n2 elevation_gain  0.906 \r\n3 elevation_loss  0.314 \r\n4 intercept       0.0144\r\n\r\nNext, let’s take a look at how the parameter values (minus the intercept) change throughout the training loop/fitting process.\r\n\r\n\r\ndescent_tibble <- function(i, inp) {\r\n  tibble(\r\n    iter = i,\r\n    distance = inp[[i]][1],\r\n    elevation_gain = inp[[i]][2],\r\n    elevation_loss = inp[[i]][3]\r\n  )\r\n}\r\n\r\nparams_fitting_tbl <- map_dfr(1:1000, ~descent_tibble(.x, params_tracking)) %>%\r\n  pivot_longer(cols = -iter)\r\n\r\nparams_fitting_tbl %>%\r\n  ggplot(aes(x = iter, y = value, color = name)) +\r\n  geom_line() +\r\n  scale_color_hp_d(option = \"HermioneGranger\")\r\n\r\n\r\n\r\n\r\nWe probably could have trained for fewer iterations, but it’s a small dataset and a simple model, so whatever.\r\nNow, let’s see what the coefficients of a “standard” multiple regression (fit using lm()) look like. This will serve as our “ground truth” and will tell us if our gradient descent fitting process arrived at the “right” coefficient values:\r\n\r\n\r\nmod_res <- lm(time_in_seconds ~ distance + elevation_gain + elevation_loss, data = ultra_normed[train_ids, ])\r\n\r\nmod_res\r\n\r\n\r\n\r\nCall:\r\nlm(formula = time_in_seconds ~ distance + elevation_gain + elevation_loss, \r\n    data = ultra_normed[train_ids, ])\r\n\r\nCoefficients:\r\n   (Intercept)        distance  elevation_gain  elevation_loss  \r\n       0.01438        -0.15403         0.90636         0.31391  \r\n\r\nGood stuff! If we look back up at the coefficients from our torch model, we can see that they’re (nearly) identical to those from this lm() model – which is what we want.\r\nAs a final step, let’s look at the loss of the model throughout the training process on both the training set and the validation set. This will give us a sense of how our model “learns” throughout the process.\r\nAs sort of an aside – we’d typically look at these metrics as a way to examine overfitting, which is a big problem for neural networks and more complex models. However, we’re not running a complex model. Linear models pretty much can’t overfit, so this is a less useful diagnostic here. But let’s take a look anyway.\r\n\r\n\r\n#checking out loss during training\r\nloss_metrics <- tibble(\r\n  iter = 1:1000,\r\n  trn_loss = unlist(loss_tracking),\r\n  vld_loss = unlist(vld_loss_tracking)\r\n) %>%\r\n  pivot_longer(\r\n    cols = -iter\r\n  )\r\n\r\nloss_metrics %>%\r\n  ggplot(aes(x = iter, y = value, color = name)) +\r\n  geom_line() +\r\n  scale_color_hp_d(option = \"HermioneGranger\")\r\n\r\n\r\n\r\n\r\nRight, so this is pretty much what we’d expect. Both losses drop in the first few iterations and then level off. The fact that both losses flatline indicates that we’re not overfitting, which again is what we expect with a linear model. We also expect our validation loss to be higher than the training loss, because the model hasn’t seen this data ever.\r\nConclusion\r\nThat’s it for now. We’ve learned how to write a ton of code to accomplish something we can do in a single-liner call to lm() :)\r\nI’m planning on digging into {torch} more and potentially writing a few more blogs once I get into actual neural networks with image and/or text data, but that’s for another day.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-20-fitting-a-multiple-regression-with-torch/fitting-a-multiple-regression-with-torch_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-11-20T06:35:16-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-13-2021-virginia-datathon-recap/",
    "title": "2021 Virginia Datathon Recap",
    "description": "Lessons learned and takeaways from the 2021 VA Datathon. Plus a Shiny app.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-10-13",
    "categories": [],
    "contents": "\r\nOverview\r\nThis past Thursday and Friday, a couple of friends (Morgan DeBusk-Lane and Mike Broda) and I had the opportunity to participate in the 2021 Virtual Virginia Datathon. This is an annual hackathon that I’ve participated in for the past few years in which Virginia’s state agencies curate a bunch of datasets relating to a particular theme and ask participating teams to develop some sort of solution. Which I imagine is how hackathons typically work, but I haven’t participated in any others.\r\nAnyway, the theme for this year’s datathon was “Addressing Hunger with Bits and Bytes,” and most of the data had to do with food insecurity, SNAP participation, free and reduced school meals, and the like. We focused in on one dataset provided – sites participating in the CACFP afterschool meals program. From this dataset, we created a Shiny app that allows users to enter their address and identify the closest site (in Virginia) participating in the afterschool meals program. Although we’ve un-deployed our app, you can find the Github repo with all of the code (and a lightly cleaned dataset) here.\r\nLessons Learned\r\nOne thing I appreciated about our approach to this year’s datathon is that it gave me the opportunity to practice with some skills/tools I’ve used before but certainly wouldn’t consider myself super proficient in. More specifically, I got to practice a bit with Shiny and with working with geographical data. Some things I learned/took away are:\r\nThe {leaflet} package is awesome, but I probably need to learn some Javascript. I’ve dabbled with leaflet before, but using it in this instance just reaffirmed how amazing it is. Creating a great-looking, interactive map requires like three lines of R code and a dataframe with some geometry in it. That’s it. And the map we created suited our purposes just fine (or at least it worked as a prototype). That said, when I dug into some of the functions, I think I really need to learn some JS if I want to fully take advantage of the features {leaflet} offers. I’ve also been working with the {reactable} package quite a bit lately, so between these two tools, that might be enough of a push to pick up some JS.\r\nThe {nngeo} package is also awesome. I’ve done a fair amount of geocoding and working with Census data as part of my job, so I’m reasonably familiar with tools like {tidycensus} and {tidygeocoder}. But I’ve only really had to do nearest neighbors with lat/long data once before, and although I figured it out, my code wasn’t super clean and I felt like I kind of stumbled my way through it. Fortunately, while we were working on this project, Mike found the {nngeo} package and its st_nn() function, which finds the nearest neighbor(s) to each row in X from a comparison dataset Y. So all I had to do was write a little wrapper around this function to tweak the inputs and outputs a little bit (you can see this in the get_closest_ind() function in the functions file in the Github repo).\r\nI ought to learn more about proxy functions in Shiny. I’ll begin this by saying that my understanding of proxy functions in Shiny is pretty minimal, but my general understanding is that they allow you to modify a specific aspect of a widget (a leaflet map, in this case) without recreating the entire output of the widget. So like you could change the colors of some markers or something. I think the filter functionality we included (allowing users to select all sites, school sites, or non-school sites) could be a candidate for using the leafletProxy() function, but I’m not sure. And given that we had a limited time to make a (very) rough prototype of an app, I didn’t feel like I had had enough time to play around with it on the fly. But it’s definitely something I want to dig into more when I have more time.\r\nOverall, I really enjoyed participating in the VA datathon this year because I felt like I got to expand my toolkit a little bit and work with tools that I don’t always use as part of my day job.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-13-2021-virginia-datathon-recap/img/datathon_logo.jpg",
    "last_modified": "2021-11-07T06:24:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-20-scooby-doo-eda/",
    "title": "Scooby Doo EDA",
    "description": "Stream-of-consciousness exploration and modeling",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-07-20",
    "categories": [],
    "contents": "\r\nFor this week’s (well, really last week’s) #TidyTuesday, I wanted to do a sort of stream-of-consciousness type EDA and modeling that I’ll put up as a blog post. One motivation for this is that I’m considering doing some data science streaming in the future, and so I want to get a feel for whether this is an approach I might be interested in taking with streaming. So, the narrative here might be a bit lacking.\r\nI’m going to shoot for spending an hour-ish on this, but I might end up doing more or less.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc)\r\nlibrary(harrypotter)\r\nlibrary(lubridate)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 2, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\nscooby_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-13/scoobydoo.csv', na = c(\"\", \"NA\", \"NULL\"))\r\n\r\n\r\n\r\nWhat does the data look like?\r\n\r\n\r\nglimpse(scooby_raw)\r\n\r\n\r\nRows: 603\r\nColumns: 75\r\n$ index                    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ~\r\n$ series_name              <chr> \"Scooby Doo, Where Are You!\", \"Scoo~\r\n$ network                  <chr> \"CBS\", \"CBS\", \"CBS\", \"CBS\", \"CBS\", ~\r\n$ season                   <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", ~\r\n$ title                    <chr> \"What a Night for a Knight\", \"A Clu~\r\n$ imdb                     <dbl> 8.1, 8.1, 8.0, 7.8, 7.5, 8.4, 7.6, ~\r\n$ engagement               <dbl> 556, 479, 455, 426, 391, 384, 358, ~\r\n$ date_aired               <date> 1969-09-13, 1969-09-20, 1969-09-27~\r\n$ run_time                 <dbl> 21, 22, 21, 21, 21, 21, 21, 21, 21,~\r\n$ format                   <chr> \"TV Series\", \"TV Series\", \"TV Serie~\r\n$ monster_name             <chr> \"Black Knight\", \"Ghost of Cptn. Cut~\r\n$ monster_gender           <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Fe~\r\n$ monster_type             <chr> \"Possessed Object\", \"Ghost\", \"Ghost~\r\n$ monster_subtype          <chr> \"Suit\", \"Suit\", \"Phantom\", \"Miner\",~\r\n$ monster_species          <chr> \"Object\", \"Human\", \"Human\", \"Human\"~\r\n$ monster_real             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ monster_amount           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1,~\r\n$ caught_fred              <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, T~\r\n$ caught_daphnie           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ caught_velma             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ caught_shaggy            <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, FA~\r\n$ caught_scooby            <lgl> TRUE, FALSE, TRUE, FALSE, TRUE, FAL~\r\n$ captured_fred            <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, F~\r\n$ captured_daphnie         <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, F~\r\n$ captured_velma           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, F~\r\n$ captured_shaggy          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ captured_scooby          <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, F~\r\n$ unmask_fred              <lgl> FALSE, TRUE, TRUE, TRUE, FALSE, TRU~\r\n$ unmask_daphnie           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ unmask_velma             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ unmask_shaggy            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ unmask_scooby            <lgl> TRUE, FALSE, FALSE, FALSE, TRUE, FA~\r\n$ snack_fred               <lgl> TRUE, FALSE, TRUE, FALSE, FALSE, TR~\r\n$ snack_daphnie            <lgl> FALSE, FALSE, FALSE, TRUE, TRUE, FA~\r\n$ snack_velma              <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, F~\r\n$ snack_shaggy             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ snack_scooby             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ unmask_other             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ caught_other             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ caught_not               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ trap_work_first          <lgl> NA, FALSE, FALSE, TRUE, NA, TRUE, F~\r\n$ setting_terrain          <chr> \"Urban\", \"Coast\", \"Island\", \"Cave\",~\r\n$ setting_country_state    <chr> \"United States\", \"United States\", \"~\r\n$ suspects_amount          <dbl> 2, 2, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1,~\r\n$ non_suspect              <lgl> FALSE, TRUE, TRUE, FALSE, FALSE, FA~\r\n$ arrested                 <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,~\r\n$ culprit_name             <chr> \"Mr. Wickles\", \"Cptn. Cuttler\", \"Bl~\r\n$ culprit_gender           <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Ma~\r\n$ culprit_amount           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\r\n$ motive                   <chr> \"Theft\", \"Theft\", \"Treasure\", \"Natu~\r\n$ if_it_wasnt_for          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ and_that                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ door_gag                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ number_of_snacks         <chr> \"2\", \"1\", \"3\", \"2\", \"2\", \"4\", \"4\", ~\r\n$ split_up                 <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,~\r\n$ another_mystery          <dbl> 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,~\r\n$ set_a_trap               <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,~\r\n$ jeepers                  <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,~\r\n$ jinkies                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\r\n$ my_glasses               <dbl> 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,~\r\n$ just_about_wrapped_up    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\r\n$ zoinks                   <dbl> 1, 3, 1, 2, 0, 2, 1, 0, 0, 0, 0, 6,~\r\n$ groovy                   <dbl> 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1,~\r\n$ scooby_doo_where_are_you <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0,~\r\n$ rooby_rooby_roo          <dbl> 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 3,~\r\n$ batman                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ scooby_dum               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ scrappy_doo              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ hex_girls                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ blue_falcon              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, ~\r\n$ fred_va                  <chr> \"Frank Welker\", \"Frank Welker\", \"Fr~\r\n$ daphnie_va               <chr> \"Stefanianna Christopherson\", \"Stef~\r\n$ velma_va                 <chr> \"Nicole Jaffe\", \"Nicole Jaffe\", \"Ni~\r\n$ shaggy_va                <chr> \"Casey Kasem\", \"Casey Kasem\", \"Case~\r\n$ scooby_va                <chr> \"Don Messick\", \"Don Messick\", \"Don ~\r\n\r\nWhat’s the range of dates we’re looking at here?\r\n\r\n\r\nrange(scooby_raw$date_aired)\r\n\r\n\r\n[1] \"1969-09-13\" \"2021-02-25\"\r\n\r\nAnd how many episodes are we seeing each year?\r\n\r\n\r\nscooby_raw %>%\r\n  count(year(date_aired)) %>%\r\n  rename(year = 1) %>%\r\n  ggplot(aes(x = year, y = n)) +\r\n  geom_col(fill = herm)\r\n\r\n\r\n\r\n\r\nWhat about episodes by decade?\r\n\r\n\r\nscooby_raw%>%\r\n  count(10*year(date_aired) %/% 10) %>%\r\n  rename(decade = 1) %>%\r\n  ggplot(aes(x = decade, y = n)) +\r\n  geom_col(fill = herm)\r\n\r\n\r\n\r\n\r\nNext, let’s look at what ratings look like over time:\r\n\r\n\r\nscooby_raw %>%\r\n  ggplot(aes(x = index, y = imdb)) +\r\n  geom_point() +\r\n  geom_line() +\r\n  geom_smooth()\r\n\r\n\r\n\r\n\r\nAnd what if we color the points by series – I’d imagine series might have different ratings:\r\n\r\n\r\nscooby_raw %>%\r\n  ggplot(aes(x = index, y = imdb)) +\r\n  geom_point(aes(color = series_name)) +\r\n  geom_line(color = \"grey70\") +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\n\r\nNext, I’m interested in looking at some comparisons across characters for different actions they take, like unmasking baddies, getting caught, etc. There are a bunch of these logical columns (e.g. unmask_fred), and so I’ll write a little helper function to summarize them and then pivot them into a shape that’ll be easier to plot later.\r\n\r\n\r\nsummarize_pivot <- function(df, str) {\r\n  \r\n  df %>%\r\n    summarize(across(starts_with(str), ~sum(.x, na.rm = TRUE))) %>%\r\n    pivot_longer(\r\n      cols = everything(),\r\n      names_to = \"key\",\r\n      values_to = \"value\"\r\n    ) %>%\r\n    extract(col = key, into = c(\"key\", \"char\"), regex = \"^(.*)_(.*)$\") %>%\r\n    arrange(desc(value))\r\n}\r\n\r\n\r\n\r\nAn example of what this does:\r\n\r\n\r\nscooby_raw %>%\r\n  summarize_pivot(\"unmask\")\r\n\r\n\r\n# A tibble: 6 x 3\r\n  key    char    value\r\n  <chr>  <chr>   <int>\r\n1 unmask fred      102\r\n2 unmask velma      94\r\n3 unmask daphnie    37\r\n4 unmask other      35\r\n5 unmask scooby     23\r\n6 unmask shaggy     13\r\n\r\nAaaand another example:\r\n\r\n\r\nscooby_raw %>%\r\n  summarize_pivot(\"caught\")\r\n\r\n\r\n# A tibble: 7 x 3\r\n  key    char    value\r\n  <chr>  <chr>   <int>\r\n1 caught scooby    160\r\n2 caught fred      132\r\n3 caught other      84\r\n4 caught shaggy     77\r\n5 caught velma      41\r\n6 caught not        31\r\n7 caught daphnie    29\r\n\r\nNext, let’s use purrr::map() to do this a few times, combine the results into a df, and then make a plot\r\n\r\n\r\niter_strs <- c(\"caught\", \"captured\", \"unmask\", \"snack\")\r\n\r\nactions_df <- map_dfr(iter_strs, ~summarize_pivot(scooby_raw, .x))\r\n\r\nglimpse(actions_df)\r\n\r\n\r\nRows: 23\r\nColumns: 3\r\n$ key   <chr> \"caught\", \"caught\", \"caught\", \"caught\", \"caught\", \"cau~\r\n$ char  <chr> \"scooby\", \"fred\", \"other\", \"shaggy\", \"velma\", \"not\", \"~\r\n$ value <int> 160, 132, 84, 77, 41, 31, 29, 91, 85, 83, 74, 71, 102,~\r\n\r\n\r\n\r\nactions_df %>%\r\n  ggplot(aes(x = value, y = char, fill = key)) +\r\n  geom_col() +\r\n  facet_wrap(vars(key), scales = \"free_y\") +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nRight, so we see that all of the characters get captured more or less the same amount, Fred and Scooby tend to catch monsters the most, Daphnie and Shaggy eat the most snacks, and Velma and Fred do the most unmasking.\r\nSwitching up a bit, what if we want to look at monster’s motives? First let’s take a look at all of the unique motives.\r\n\r\n\r\nunique(scooby_raw$motive)\r\n\r\n\r\n [1] \"Theft\"            \"Treasure\"         \"Natural Resource\"\r\n [4] \"Competition\"      \"Extortion\"        \"Safety\"          \r\n [7] \"Counterfeit\"      \"Inheritance\"      \"Smuggling\"       \r\n[10] \"Preservation\"     NA                 \"Experimentation\" \r\n[13] \"Food\"             \"Trespassing\"      \"Assistance\"      \r\n[16] \"Abduction\"        \"Haunt\"            \"Anger\"           \r\n[19] \"Imagination\"      \"Bully\"            \"Loneliness\"      \r\n[22] \"Training\"         \"Conquer\"          \"Mistake\"         \r\n[25] \"Automated\"        \"Production\"       \"Entertainment\"   \r\n[28] \"Simulation\"      \r\n\r\nAnd it’s probably useful to count these:\r\n\r\n\r\nscooby_raw %>% \r\n  count(motive, sort = TRUE)\r\n\r\n\r\n# A tibble: 28 x 2\r\n   motive               n\r\n   <chr>            <int>\r\n 1 Competition        168\r\n 2 Theft              125\r\n 3 <NA>                67\r\n 4 Treasure            54\r\n 5 Conquer             42\r\n 6 Natural Resource    26\r\n 7 Smuggling           22\r\n 8 Trespassing         15\r\n 9 Abduction           12\r\n10 Food                11\r\n# ... with 18 more rows\r\n\r\nSo, “Competition” is far and away the most common motive. I’m not sure I really understand what this means? But it’s also been a while since I’ve watched Scooby Doo.\r\nI’m also interested in how often we see “zoinks” in episodes, bc I feel like this is the defining line of the show (along with the meddling kids, which I’ll look at next).\r\n\r\n\r\nscooby_raw %>%\r\n  ggplot(aes(x = zoinks)) +\r\n  geom_histogram(bins = 20, fill = herm)\r\n\r\n\r\n\r\n\r\nThis feels weird to me. Most often, we get 0 or 1, but then there are episodes with more than 10? I’d imagine these are probably movies?\r\n\r\n\r\nscooby_raw %>%\r\n  ggplot(aes(x = zoinks)) +\r\n  geom_histogram(bins = 10, fill = herm) +\r\n  facet_wrap(vars(format), scales = \"free_y\")\r\n\r\n\r\n\r\n\r\nWell, so, there are still some TV shows that have a ton of zoinks’s. But also our biggest outlier is a movie, which makes sense to me since there’s more time for zoinking.\r\nAnd what about our “if it wasn’t for those meddling kids” data?\r\n\r\n\r\nlength(unique(scooby_raw$if_it_wasnt_for))\r\n\r\n\r\n[1] 108\r\n\r\nOk, wow, so that’s a lot of different values for “if it wasn’t for…”\r\nFirst, let’s just see how many episodes have the “if it wasn’t for…” catchphrase\r\n\r\n\r\nscooby_raw %>%\r\n  mutate(has_catchphrase =  if_else(!is.na(if_it_wasnt_for), TRUE, FALSE)) %>%\r\n  count(has_catchphrase)\r\n\r\n\r\n# A tibble: 2 x 2\r\n  has_catchphrase     n\r\n  <lgl>           <int>\r\n1 FALSE             414\r\n2 TRUE              189\r\n\r\nCool, so, 189 of our 603 episodes have the “if it wasn’t for…” catchphrase.\r\nAnd now which of these also use the term “meddling?”\r\n\r\n\r\nscooby_raw %>%\r\n  filter(!is.na(if_it_wasnt_for)) %>%\r\n  mutate(meddling = if_else(str_detect(if_it_wasnt_for, \"meddling\"), TRUE, FALSE)) %>%\r\n  count(meddling) %>%\r\n  ggplot(aes(x = n, y = meddling)) +\r\n  geom_col(fill = herm) +\r\n  geom_text(aes(label = n, x = n - 1), hjust = 1, color = \"white\")\r\n\r\n\r\n\r\n\r\nAlright, so, of the 189 episodes that have the “if it wasn’t for…” catchphrase, most of those also include the word “meddling!”\r\nThe last little bit here – because I’m trying to keep my time to about an hour (again, to test out the feel for if this is a viable approach to streaming or making videos), is going to be to fit a quick linear model predicting the imdb rating of an episode.\r\n\r\n\r\nlibrary(tidymodels)\r\n\r\n\r\n\r\nLet’s just use numeric/logical columns in our model, mostly because preprocessing them is pretty straightforward (although note that this doesn’t mean what I’m doing below is anywhere near the best approach). Then let’s look at how much missing data we have for each of these columns.\r\n\r\n\r\nmod_df <- scooby_raw %>%\r\n  select(where(is.numeric) | where(is.logical)) %>%\r\n  filter(!is.na(imdb))\r\n\r\nmiss_df <- mod_df %>%\r\n  summarize(across(everything(), ~(sum(is.na(.x))/length(.x))))\r\n\r\nmiss_df\r\n\r\n\r\n# A tibble: 1 x 51\r\n  index  imdb engagement run_time monster_amount suspects_amount\r\n  <dbl> <dbl>      <dbl>    <dbl>          <dbl>           <dbl>\r\n1     0     0          0        0              0               0\r\n# ... with 45 more variables: culprit_amount <dbl>, split_up <dbl>,\r\n#   another_mystery <dbl>, set_a_trap <dbl>, jeepers <dbl>,\r\n#   jinkies <dbl>, my_glasses <dbl>, just_about_wrapped_up <dbl>,\r\n#   zoinks <dbl>, groovy <dbl>, scooby_doo_where_are_you <dbl>,\r\n#   rooby_rooby_roo <dbl>, monster_real <dbl>, caught_fred <dbl>,\r\n#   caught_daphnie <dbl>, caught_velma <dbl>, caught_shaggy <dbl>,\r\n#   caught_scooby <dbl>, captured_fred <dbl>, captured_daphnie <dbl>,\r\n#   captured_velma <dbl>, captured_shaggy <dbl>,\r\n#   captured_scooby <dbl>, unmask_fred <dbl>, unmask_daphnie <dbl>,\r\n#   unmask_velma <dbl>, unmask_shaggy <dbl>, unmask_scooby <dbl>,\r\n#   snack_fred <dbl>, snack_daphnie <dbl>, snack_velma <dbl>,\r\n#   snack_shaggy <dbl>, snack_scooby <dbl>, unmask_other <dbl>,\r\n#   caught_other <dbl>, caught_not <dbl>, trap_work_first <dbl>,\r\n#   non_suspect <dbl>, arrested <dbl>, door_gag <dbl>, batman <dbl>,\r\n#   scooby_dum <dbl>, scrappy_doo <dbl>, hex_girls <dbl>,\r\n#   blue_falcon <dbl>\r\n\r\nSo, some of these columns have a ton of missing data. Just to keep moving forward on this, I’m going to chuck any columns with more than 20% missing data, then median impute cases with missing data in the remaining columns (which we’ll do in the recipes step below).\r\n\r\n\r\nkeep_vars <- miss_df %>%\r\n  pivot_longer(cols = everything(),\r\n               names_to = \"nms\",\r\n               values_to = \"vals\") %>%\r\n  filter(vals < .2) %>%\r\n  pull(1)\r\n\r\nmod_df <- mod_df %>%\r\n  select(all_of(keep_vars)) %>%\r\n  mutate(across(where(is.logical), as.numeric))\r\n\r\n\r\n\r\nNow we’ll set up some bootstrap resamples. I’m using bootstrap resamples here rather than k-fold because it’s a relatively small dataset.\r\n\r\n\r\nset.seed(0408)\r\nbooties <- bootstraps(mod_df, times = 10)\r\n\r\n\r\n\r\nAnd then let’s define some very basic preprocessing using a recipe:\r\n\r\n\r\nrec <- recipe(imdb ~ ., data = mod_df) %>%\r\n  step_impute_median(all_numeric_predictors()) %>%\r\n  step_normalize(all_numeric_predictors()) \r\n\r\n\r\n\r\nAnd let’s do a lasso regression, just using a small and kinda of arbitrary penalty value (we could tune this, but I’m not going to).\r\n\r\n\r\nlasso_spec <- linear_reg(mixture = 1, penalty = .001) %>%\r\n  set_engine(\"glmnet\")\r\n\r\n#combining everything into a workflow\r\nlasso_wf <- workflow() %>%\r\n  add_recipe(rec) %>%\r\n  add_model(lasso_spec)\r\n\r\n\r\n\r\nAnd now let’s fit!\r\n\r\n\r\nlasso_res <- fit_resamples(\r\n  lasso_wf,\r\n  resamples = booties\r\n)\r\n\r\n\r\n\r\nThe main reason for fitting on these resamples is to check our model performance, so let’s do that.\r\n\r\n\r\ncollect_metrics(lasso_res)\r\n\r\n\r\n# A tibble: 2 x 6\r\n  .metric .estimator  mean     n std_err .config             \r\n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 rmse    standard   0.626    10  0.0104 Preprocessor1_Model1\r\n2 rsq     standard   0.280    10  0.0165 Preprocessor1_Model1\r\n\r\nOur R-squared is .29, which isn’t great, but it’s also not terrible considering we really didn’t put much effort into our preprocessing here, and we discarded a bunch of data.\r\nLet’s fit one final time on the full dataset to look at the importance of our predictor variables:\r\n\r\n\r\nprepped_df <- rec %>%\r\n  prep() %>%\r\n  bake(new_data = NULL)\r\n\r\nmod_fit <- lasso_spec %>%\r\n  fit(imdb ~ ., data = prepped_df)\r\n\r\n\r\n\r\nAnd then finally we can look at our coefficients.\r\n\r\n\r\nmod_fit %>%\r\n  tidy() %>%\r\n  filter(term != \"(Intercept)\") %>%\r\n  arrange(desc(abs(estimate))) %>%\r\n  ggplot(aes(x = estimate, y = fct_reorder(term, abs(estimate)), fill = estimate >= 0)) +\r\n  geom_col() +\r\n  labs(\r\n    y = NULL\r\n  )\r\n\r\n\r\n\r\n\r\nAnd there we go. That was a bit more than an hour, but it was worth it to get to a reasonable stopping point!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-20-scooby-doo-eda/img/scooby_doo.jpg",
    "last_modified": "2021-07-20T20:27:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-02-robustly-create-parameterized-reports/",
    "title": "Robustly Create Parameterized Reports",
    "description": "Wrapping functions to safeguard against errors",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-07-02",
    "categories": [],
    "contents": "\r\nRecently, I was working on creating parameterized reports for all of the schools in the division where I work. The basic idea was to provide school leadership teams with individualized reports on several (common) key metrics that they could use to both 1) reflect on the previous year(s) and 2) set goals for the upcoming year(s).\r\nThe beauty of parameterized reporting via RMarkdown is that you can build a template report, define some parameters that will vary within each iteration of the report, and then render several reports all from a single template along with a file that will loop (or purrr::walk()) through the parameters. (If you want to learn more about parameterized reporting, the always-incredible Alison Hill has a recent-ish tutorial on them that you can find here). In my case, this meant creating one template for all 65+ schools and then looping through a function that rendered the report for each school. Sounds great, right?\r\nSee, what had happened was…\r\n\r\nThis workflow is great…when it works. Except it doesn’t always. This isn’t to say that {rmarkdown} mysteriously breaks or anything, but rather that when you create these reports using real (read: usually messy) data, and when you’re trying to present a lot of data in a report, the probability that one of your iterations throws an error increases. This is especially true when you work in a school division and the integrity of your data has been absolutely ravaged by COVID during the past ~18 months. When this happens, instead of watching the text zip by on your console as all of your reports render like they’re supposed to, you end up hunting through the data for each individual school wondering why calculating a particular metric threw an error. Which is like, not nearly as much fun.\r\nSo what can we do about this?\r\nFortunately, we can get around this by making “safe” versions of our functions. What exactly that means will vary from function to function and from use case to use case, but generally it means wrapping a function in another function that can facilitate error handling (or prevent errors from occuring). In some cases, it might mean using purrr::safely() or purrr::possibly() to capture errors or provide default values to the functions. In other cases, it might mean writing your own wrapper (which is what I’ll demonstrate below) to deal with errors that pop up. Regardless of the exact route you go, the goal here is to prevent errors that would otherwise stop your document(s) from rendering.\r\nLet’s see this in action.\r\nSetup\r\nI’m not actually going to create “real” parameterized reports here, but I’ll illustrate the principle using data from {palmerpenguins} and some ggplots. First, I’ll load some packages and set some options and whatnot, plus also take a peek at the penguins data we’ll be using.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE)\r\n\r\nlibrary(palmerpenguins) #data on penguins\r\nlibrary(tidyverse) #you all know what this is\r\nlibrary(eemisc) #personal ggplot themes\r\nlibrary(harrypotter) #colors\r\nlibrary(reactable)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\nglimpse(penguins)\r\n\r\n\r\nRows: 344\r\nColumns: 8\r\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Ad~\r\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen~\r\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39~\r\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19~\r\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193~\r\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 46~\r\n$ sex               <fct> male, female, female, NA, female, male, fe~\r\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, ~\r\n\r\nSplit Data\r\nIf you’re not familiar with this dataset, it contains data on a few hundred penguins, and you can learn more here. One feature of this dataset is that it has data on three different species of penguins: Adelie, Gentoo, and Chinstrap. So, let’s imagine we wanted to provide separate reports for each species of penguin. To do this, let’s first divide our data up into separate dataframes to emulate a potential workflow of creating parameterized reports.\r\n\r\n\r\nsplit_df <- split(penguins, penguins$species)\r\n\r\nstr(split_df)\r\n\r\n\r\nList of 3\r\n $ Adelie   : tibble[,8] [152 x 8] (S3: tbl_df/tbl/data.frame)\r\n  ..$ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n  ..$ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\r\n  ..$ bill_length_mm   : num [1:152] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\r\n  ..$ bill_depth_mm    : num [1:152] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\r\n  ..$ flipper_length_mm: int [1:152] 181 186 195 NA 193 190 181 195 193 190 ...\r\n  ..$ body_mass_g      : int [1:152] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\r\n  ..$ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\r\n  ..$ year             : int [1:152] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\r\n $ Chinstrap: tibble[,8] [68 x 8] (S3: tbl_df/tbl/data.frame)\r\n  ..$ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n  ..$ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n  ..$ bill_length_mm   : num [1:68] 46.5 50 51.3 45.4 52.7 45.2 46.1 51.3 46 51.3 ...\r\n  ..$ bill_depth_mm    : num [1:68] 17.9 19.5 19.2 18.7 19.8 17.8 18.2 18.2 18.9 19.9 ...\r\n  ..$ flipper_length_mm: int [1:68] 192 196 193 188 197 198 178 197 195 198 ...\r\n  ..$ body_mass_g      : int [1:68] 3500 3900 3650 3525 3725 3950 3250 3750 4150 3700 ...\r\n  ..$ sex              : Factor w/ 2 levels \"female\",\"male\": 1 2 2 1 2 1 1 2 1 2 ...\r\n  ..$ year             : int [1:68] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\r\n $ Gentoo   : tibble[,8] [124 x 8] (S3: tbl_df/tbl/data.frame)\r\n  ..$ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 3 3 3 3 3 3 3 3 3 3 ...\r\n  ..$ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n  ..$ bill_length_mm   : num [1:124] 46.1 50 48.7 50 47.6 46.5 45.4 46.7 43.3 46.8 ...\r\n  ..$ bill_depth_mm    : num [1:124] 13.2 16.3 14.1 15.2 14.5 13.5 14.6 15.3 13.4 15.4 ...\r\n  ..$ flipper_length_mm: int [1:124] 211 230 210 218 215 210 211 219 209 215 ...\r\n  ..$ body_mass_g      : int [1:124] 4500 5700 4450 5700 5400 4550 4800 5200 4400 5150 ...\r\n  ..$ sex              : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 2 1 1 2 1 2 ...\r\n  ..$ year             : int [1:124] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\r\n\r\nGreat, so now we have a separate dataframe for each penguin species.\r\nCreate a Plot\r\nNow, imagine we want to create a ggplot to include in each of our reports. To illustrate this, let’s just do a scatterplot of flipper length by bill length for Adelie penguins. This is a fairly basic scatterplot, but it works for our purposes.\r\n\r\n\r\nggplot(split_df[[1]], aes(x = flipper_length_mm, y = bill_length_mm)) +\r\n  geom_point(color = herm) +\r\n  labs(\r\n    title = \"Adelie\"\r\n  )\r\n\r\n\r\n\r\n\r\nAnd we can do the same thing for Chinstrap penguins:\r\n\r\n\r\nggplot(split_df[[2]], aes(x = flipper_length_mm, y = bill_length_mm)) +\r\n  geom_point(color = herm) +\r\n  labs(\r\n    title = \"Chinstrap\"\r\n  )\r\n\r\n\r\n\r\n\r\nGood so far, right? Our next step might be to define a function to make this plot. Although this is a fairly basic plot, writing a function still saves us a little bit of typing, and it will prove useful later when we need to wrap it, so let’s go ahead and write our function.\r\n\r\n\r\nmake_penguin_plot <- function(df) {\r\n  \r\n  title <- as.character(unique(df$species))\r\n  \r\n  ggplot(df, aes(x = flipper_length_mm, y = bill_length_mm)) +\r\n    geom_point(color = herm) +\r\n    labs(\r\n      title = title\r\n    )\r\n}\r\n\r\nmake_penguin_plot(split_df[[1]])\r\n\r\n\r\n\r\n\r\nSo what if something goes wrong?\r\nLet’s imagine, now, that we don’t have measurements of Gentoo penguins’ bill lengths. Maybe the bill-length-measuring machine was broken on the one day we were going to take their measurements. Or maybe all of them were especially bite-y and wouldn’t let us measure their bills (I have no clue if penguins actually bite).\r\n\r\n\r\n#dropping the bill_length measurement from the Gentoo data\r\nsplit_df[[3]] <- split_df[[3]] %>%\r\n  select(-bill_length_mm)\r\n\r\n\r\n\r\nNow what happens when we try to make our penguin plot?\r\n\r\n\r\nmake_penguin_plot(split_df[[3]])\r\n#produces message 'Error: Column `bill_length_mm` not found in `.data`'\r\n\r\n\r\n\r\nOh no! An error! In this contrived example here, I’m just showing you the error message produced, but if you’re actually rendering a report, this will stop your report from rendering, which isn’t great.\r\n\r\nIf this happens to you, you have a few options. You might create a separate report template just for Gentoo penguins, although this doesn’t seem ideal, because it defeats the point of having a template if you need to make separate ones every time an exception pops up. You could drop this metric from your main report template if the data seems problematic (which is a good thing to investigate). You could potentially use purrr::possibly() or purrr::safely() if you have a default value you want to use.\r\nAnother option is to write your own little wrapper to make your function “safe”, which I’ll show below.\r\nWrap that function!\r\nThe best part here is that this is, in the very specific case, it’s fairly straightforward. I’m just going to check the names of the variables in the data I’m passing into the function to see if flipper length and bill length are present in the data, then execute make_penguin_plot() if they are and print out “Whoops!” if they’re not.\r\n\r\n\r\nsafe_penguin_plot <- function(df) {\r\n  \r\n nms <- names(df)\r\n \r\n if (all(c(\"flipper_length_mm\", \"bill_length_mm\") %in% nms)) {\r\n   make_penguin_plot(df)\r\n } else print(\"Whoops! Looks like you don't have all of the data you need for this plot!\")\r\n}\r\n\r\nsafe_penguin_plot(split_df[[1]])\r\n\r\n\r\n\r\n\r\n\r\n\r\nsafe_penguin_plot(split_df[[3]])\r\n\r\n\r\n[1] \"Whoops! Looks like you don't have all of the data you need for this plot!\"\r\n\r\nGeneralize your functions using {rlang}\r\nYou can also imagine generalizing this to accept other variables. This requires some quoting/unquoting and diving into {rlang}, which is something I’ve been trying to learn lately:\r\n\r\n\r\ngen_penguin_plot <- function(df, xvar, yvar) {\r\n  \r\n  title <- as.character(unique(df$species))\r\n  \r\n  x <- enexpr(xvar) #captures xvar as an expression\r\n  y <- enexpr(yvar) #captures yvar as an expression\r\n  \r\n  ggplot(df, aes(x = !!x, y = !!y)) +\r\n    geom_point(color = herm) +\r\n    labs(\r\n      title = title\r\n    )\r\n}\r\n\r\ngen_penguin_plot(split_df[[1]], bill_length_mm, bill_depth_mm)\r\n\r\n\r\n\r\n\r\nThere’s a little bit more work in making the more generalized version “safe” that has to do with handling the quoted expressions/environments, especially since we’re passing them into another function:\r\n\r\n\r\nsafe_gen_plot <- function(df, xvar, yvar) {\r\n  \r\n   nms <- names(df)\r\n   \r\n   vec <- c(deparse(enexpr(xvar)), deparse(enexpr(yvar)))\r\n   \r\n   x <- enquo(xvar)\r\n   y <- enquo(yvar)\r\n   \r\n    if (all(vec %in% nms)) {\r\n   gen_penguin_plot(df, xvar = !!x, yvar = !!y)\r\n } else print(\"Whoops! Looks like you don't have all of the data you need for this plot!\")\r\n  \r\n}\r\n\r\nsafe_gen_plot(split_df[[1]], bill_length_mm, bill_depth_mm)\r\n\r\n\r\n\r\n\r\n\r\n\r\nsafe_gen_plot(split_df[[3]], bill_length_mm, bill_depth_mm)\r\n\r\n\r\n[1] \"Whoops! Looks like you don't have all of the data you need for this plot!\"\r\n\r\nConclusion\r\nBy implementing a simple (or less simple, depending on how generalizable you want your function to be) wrapper function, we can replace errors with a message to be displayed when rendering a report. I can’t emphasize how much time this approach has saved me when creating parameterized reports, especially since our data has gotten so wonky due to COVID and this provides a flexible way to handle all of this craziness.\r\nHope this helps others who might be in similar positions!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-02-robustly-create-parameterized-reports/robustly-create-parameterized-reports_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-02T11:21:29-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-30-its-a-me-linear-regression/",
    "title": "It's-a Me, Linear Regression!",
    "description": "Using feature engineering and linear regression to predict Mario Kart world records",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-05-30",
    "categories": [],
    "contents": "\r\nI’ve been sort of out of the #TidyTuesday game for a while, but this week’s dataset on Mario Kart world records called to me. I have tons of fond memories from late elementary school and into middle school playing Mario Kart 64 with kids in my neighborhood. I certainly wasn’t world-record-caliber good (I was like 10ish), but I do remember learning little tricks like how to get the speed boost on the 3-2-1 go! countdown or how to power-slide through turns.\r\nAnyway, when I initially looked at the dataset, I thought I’d approach it by trying to fit a model to predict whether or not a driver took advantage of a shortcut or not when they set a record, but alas, I waited until later in the week and got scooped by Julia Silge (check out her analysis here). Which is probably for the best, because she did a better job than I would have.\r\nThat said, when I dug into the data, I did stumble across some interesting patterns in the progression of records over time, so I want to show how we can model these progressions using some simple feature engineering and a relatively straightforward mixed-effects model.\r\nSetup\r\nFirst, we’ll load our packages, set some global options, and get our data.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc)\r\nlibrary(harrypotter) #colors\r\nlibrary(nlme) #for mixed-effects models\r\nlibrary(broom.mixed) #functions for tidying mixed effects models\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\nherm2 <- harrypotter::hp(n = 2, option = \"HermioneGranger\")[2]\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"HermioneGranger\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\nrecords <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv')\r\n\r\n\r\n\r\nExplore Data\r\nSo, what I’m interested in here is how the world record for each track progresses over time. To make sure all of our comparisons are “apples to apples,” I’m going to limit this to single-lap, no-shortcut records.\r\nLet’s randomly choose 4 tracks and look at these records over time.\r\n\r\n\r\nset.seed(0408)\r\nsamp_tracks <- sample(unique(records$track), size = 4)\r\n\r\n\r\nrecords %>%\r\n  filter(track %in% samp_tracks,\r\n         type == \"Single Lap\",\r\n         shortcut == \"No\") %>%\r\n  ggplot(aes(x = date, y = time, color = track)) +\r\n  geom_point() +\r\n  geom_line() +\r\n  facet_wrap(vars(track))\r\n\r\n\r\n\r\n\r\nIt’s a little bit hard to tell what’s going on here, especially since the tracks are different lengths and seem to have different record asymptotes. Another issue is that record-setting seems to be clustered by dates. A lot of records are set in a cluster, then there’s a drought for several years where the record isn’t broken. In some analyses this may be meaningful, but I care less about the actual date a record was set on and more about where it is in the sequence of records for that track. So, it might be more straightforward to just assign a running count of records for each track:\r\n\r\n\r\nrecords %>%\r\n  filter(track %in% samp_tracks,\r\n         type == \"Single Lap\",\r\n         shortcut == \"No\") %>%\r\n  group_by(track) %>%\r\n  mutate(record_num = row_number()) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = record_num, y = time, color = track)) +\r\n  geom_point() +\r\n  geom_line() +\r\n  facet_wrap(vars(track))\r\n\r\n\r\n\r\n\r\nThis puts all of our X-axes on the same scale, and it also removes a lot of the white space where we had no records being broken (again, this information might be useful for a different analysis).\r\nWe might also want to consider how we’re representing the lap time here. Each track is a different length, and each track has its own unique obstacles. We can see here that Wario Stadium is a much longer track than, say, Sherbet Land. By extension, a 1 second decrease in time on Sherbet Land means a lot more than a 1 second decrease in time on Wario Stadium.\r\nStandardizing our measure of time – and our measure of improvement over time – will help us out here. What I’m going to do is, for each record (and specific to each track), calculate how much better (as a percent) it was than the first world record on that track. This will give us a standard way to compare the progress of each world record across all of the tracks.\r\nLet’s graph this to see what they look like.\r\n\r\n\r\nrecords_scaled <- records %>%\r\n  filter(type == \"Single Lap\",\r\n         shortcut == \"No\") %>%\r\n  group_by(track) %>%\r\n  mutate(init_wr = max(time),\r\n         pct_better = 1 - time/init_wr,\r\n         record_num = row_number()) %>%\r\n  ungroup()\r\n\r\nrecords_scaled %>%\r\n  ggplot(aes(x = record_num, y = pct_better)) +\r\n  geom_point(color = herm) +\r\n  geom_line(color = herm) +\r\n  labs(\r\n    x = \"Record Number\",\r\n    y = \"Pct Improvement over Initial\"\r\n  ) +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  facet_wrap(vars(track)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nThese are a lot easier to compare, and we can see a pretty definitive pattern across all of the tracks. There are some sharp improvements in the early record-setting runs, but then these improvements attenuate over time, and records are getting only a teeny bit faster each time they’re broken.\r\nAnd this is sort of what we’d expect, particularly given a closed system like Mario Kart 64. The game isn’t changing, and people will hit a threshold in terms of how much better they can get, so it makes sense that these records are flattening.\r\nAnother interesting feature of the above graphs is that they (strongly) resemble logarithmic curves. We can plot these below to illustrate the similarity:\r\n\r\n\r\nrecords_scaled %>%\r\n  ggplot(aes(x = record_num, y = pct_better)) +\r\n  geom_point(color = herm) +\r\n  geom_line(color = herm) +\r\n  facet_wrap(vars(track)) +\r\n    labs(\r\n    x = \"Record Number\",\r\n    y = \"Pct Improvement over Initial\"\r\n  ) +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  stat_function(fun = function(x) .1*log(x, base = 10), geom = \"line\", color = herm2) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nWe can see that the general shape of the logarithmic curve matches the general shape of the track records here. I multiplied the curves by an arbitrary constant just to plot them, so of course we don’t expect them to match perfectly. That said, this does give us a clue that, given these feature engineering choices, we can model the data using a logarithmic curve.\r\nBuilding A Model\r\nThere are a few paths we could take moving forward to model the data. Two in particular stand out to me:\r\nWe could fit a model separately for each track, where percent_better is regressed on the log of record_num.\r\nWe could fit a multilevel model where percent_better is regressed on the log of record_num and specify a random intercept and a random slope by track. This is the option I’m going to take.\r\nTo give a very quick and insufficient nod to multilevel models (MLMs), they are useful for modeling clustered data (data where there are known dependencies between observations). The prototypical example of this is looking at people over time. Imagine you have a dataset of 1,000 observations where each of the 50 people in the dataset contributes 20 observations. When modeling this, you’d want to account for the fact that within-person observations are not independent. The one I encounter a lot in education is students clustered within classrooms. Different application, but same principle. For more on MLMs, Raudenbush & Bryk (2002) is a great resource, as is John Fox’s Applied Regression Analysis, which as a chapter on MLMs. My friend and colleague Mike Broda also has made public some content from his multilevel modeling (and multivariate statistics) course in R as well.\r\nAnyway, moving along! We basically have clustered data here: records are clustered within (read: dependent upon) each track. What an MLM allows us to do is fit a single model to the entire dataset while also allowing some of the parameters to vary by track. More specifically, we can allow the intercept to vary (which we don’t actually need here, since we’ve standardized our intercepts, but it’s just as easy to allow it), and we can allow the slope to vary. Varying the slope will let us estimate a different progression of world record runs for each track, which we can see that we need from the plots above.\r\nI’m using the {nlme} package to fit this model. And the model I’m fitting can be read as follows:\r\nI want a “regular” fixed-effects model where pct_better is regressed on the log of record_num.\r\nI also want to allow the coefficient of the log of record_num to vary depending on which track the record was set on.\r\n\r\n\r\nmod <- lme(fixed = pct_better ~ log10(record_num),\r\n           random = ~ log10(record_num) | track,\r\n           data = records_scaled)\r\n\r\n\r\n\r\nAnd let’s take a look at the results (thanks to {broom.mixed}):\r\n\r\n\r\ntidy(mod)\r\n\r\n\r\n# A tibble: 6 x 8\r\n  effect  group term      estimate std.error    df statistic   p.value\r\n  <chr>   <chr> <chr>        <dbl>     <dbl> <dbl>     <dbl>     <dbl>\r\n1 fixed   fixed (Interce~  0.0162    0.00505   608      3.20  1.44e- 3\r\n2 fixed   fixed log10(re~  0.0518    0.00396   608     13.1   1.46e-34\r\n3 ran_pa~ track sd_(Inte~  0.0199   NA          NA     NA    NA       \r\n4 ran_pa~ track cor_log1~  0.768    NA          NA     NA    NA       \r\n5 ran_pa~ track sd_log10~  0.0156   NA          NA     NA    NA       \r\n6 ran_pa~ Resi~ sd_Obser~  0.00651  NA          NA     NA    NA       \r\n\r\nFor what it’s worth, we see that both of the fixed effects are statistically significant. For reasons that I don’t quite remember off the top of my head, there’s some debate about doing hypothesis tests on random effects, so these aren’t included here (I think other packages will run these tests if you really want them). The main thing I focus on here, though, is that there’s a seemingly non-negligible amount of variance in the coefficient for record_num (see the term sd_log10(recordnum)). The mean coefficient is .051, and the standard deviation of the coefficient values is .015, which seems meaningful to me.\r\nPlotting Our Results\r\nTo get a better sense of what this model is doing, as well as to graphically examine how well it does, we can use the augment() function from {broom.mixed}. Let’s plot our fitted values against our actual pct_better values.:\r\n\r\n\r\naug <- mod %>%\r\n  augment()\r\n\r\naug %>%\r\n  filter(pct_better != 0) %>%\r\n  ggplot(aes(x = pct_better, y = .fitted)) +\r\n  geom_point(color = herm) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nWe’re expecting an a-b line here, so this is good.\r\nFinally, though, what if we plot our actual values and our fitted values against record_num to see how well our model predictions compare to the real values, and let’s look at this by track:\r\n\r\n\r\naug %>%\r\n  select(track, .fitted, pct_better, record_num) %>%\r\n  pivot_longer(cols = c(\".fitted\", \"pct_better\"),\r\n               names_to = \"type\",\r\n               values_to = \"val\") %>%\r\n  ggplot(aes(x = record_num, y = val, color = type)) +\r\n  geom_point(alpha = .4) +\r\n  facet_wrap(vars(track)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nThe fitted values pretty much entirely overlap with the actual values (with the exception of Yoshi Valley and maybe sort of DK Jungle Parkway), which means we have a pretty solid model, here!\r\nIf we wanted, it would be fairly straightforward to translate these predictions back into seconds, but I’m going to call it done right here. Hopefully this post illustrates that with a little bit of feature engineering, you can build a really good model without having to load {xgboost} or {keras}. And hopefully it encourages people to dig more into MLMs!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-30-its-a-me-linear-regression/its-a-me-linear-regression_files/figure-html5/pct-better-plot-1.png",
    "last_modified": "2021-05-30T16:04:53-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-26-publishing-rmarkdown-to-google-sites/",
    "title": "Publishing Rmarkdown to Google Sites",
    "description": "A workaround for people in organizations that use Google Sites",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-05-03",
    "categories": [],
    "contents": "\r\nThis post – and the digging behind it – was inspired by Allison Horst and Jacqueline Nolis’s recent-ish blog post about conflicts between data scientists and the teams they’ may be’re joining/supporting. It’s a great post, and I’d recommend it to anyone, whether you’re looking for a job, new to a job, or relatively senior in your organization. When I read it, the post struck a chord with me and led me to do some introspection about how much change I should be initiating/pushing for as a new employee in my organization vs how much I should be adapting to established workflows.\r\nBasically, this was my response:\r\n\r\nSharing HTML: A Sticking Point\r\nOne specific issue that came to mind was sharing html documents. In short, when I joined the organization I work for (Chesterfield County Public Schools; CCPS), I started producing reports/products as html documents – because I like the flexibility and richness of html – which was not common in the organization, and so we went around and around on how best to share these files. We threw around several different ideas, ranging from emailing them as attachments to saving them on the network to exploring RStudio Connect. And, for various reasons, nothing we tried or proposed felt like the right solution.\r\nGoogle Sites to the Rescue (Sort of)\r\nThat said, CCPS widely uses Google Sites. The vast majority of the people who work in CCPS aren’t web developers or analysts; it is, after all, a school division and not a tech company. And Google Sites provides a platform for all sorts of people in the organization to create good-looking sites, so from that perspective, it makes sense that we use Google Sites widely so that teachers, principals, and other non-technical folks can create websites. Given that, I wanted to see if there was a way to take the html reports and products I was making and host them on Google Sites, which would make my work much more integrated into established practices.\r\nAnd so after a lot of Googling and my hopes being dashed by a dramatic change sometime in the past 3-4 years from “classic Google Sites” to “new Google Sites,” I finally arrived at a solution that, although not ideal, seems to do generally what I want it to, which is what I’ll share below.\r\nRendering Rmd Content to Google Sites\r\nThe rest of this post will walk through how to create an html report using Rmarkdown and then “publish” it to Google Sites. To illustrate this, I’m going to use the README for Allison Horst’s {palmerpenguins} package as the example report to publish.\r\nStep 1: Create Your Report\r\nMaybe obviously, the first thing you want to do is write and knit your report as an html file. Again, this can be any content you want, but I’m using the {palmerpenguins} README here. Another note is that, since we’re using html, we can include features like {reactable} tables or {leaflet} maps if we want, although those aren’t included here. We could also knit this as a {distill} article or add whatever styling/css we want.\r\nStep 2: Create A Google Site\r\nOnce you have your report created, you can create the Google Site in Google Drive, like so:\r\n\r\nOr you can navigate to a site you already own/can edit.\r\nn.b. that I’m not really going to get into the weeds of working in Google Sites because that’s not really the point of this post, plus I’m not an expert myself, although basic usage is fairly straightforward.\r\nStep 3: Create a Page in Google Sites to House Your Report\r\nThe details of what this means will depend on the layout of your site, but essentially you want to create a page that can house the report you just knit. You might end up with something that looks like this – a basic banner and then a blank body:\r\n\r\nStep 4: View the Source Code of Your HTML Report\r\nNext, you want to get to the source code of the html file you created. You can do this by either opening the file with a browser and then inspecting the page source (the exact process for doing this will depend on which browser you use), or you can open the html file in RStudio (or another text editor).\r\nOnce you’re there, select and copy all of the source code\r\nStep 5: Embed the Source Code in Your Google Sites Page\r\nReturning to the page where we want this report to live on our Google Site, we want to select the “Embed” option from the “Insert” menu on the right-hand side. You can also double left-click a blank part of your web page to have options pop up (& you can select “Embed” from there).\r\nOnce we click the “Embed” button, a dialogue box pops up:\r\n\r\nAnd we want to select the “Embed code” option. Once we’re here, we want to paste in all of the html source code from our report file. Then, click “Next,” and “Insert”\r\nDon’t stress if you see a box that says “trouble loading embed. Reload to try again.” (see below) The content should load once you publish your site.\r\n\r\nYou also probably want to resize & reposition the embedded content at this point. As far as I can tell, this is only possible via dragging, and there aren’t parameters you can set anywhere in the site to ensure consistent alignment (although there are grid lines to guide you).\r\nStep 6: Publish Your Site\r\nNow we’re ready to publish our site. Click the “Publish” button in the upper-right of the screen and select the options you want in the popup box. If you’re publishing from a Google account that belongs to an organization, your options may be pre-specified. For instance, when I publish from my work account, the default option is to allow only internal users to see the site (which is what I want in most cases).\r\nOnce you’ve published your site, you can navigate to it and see the following:\r\n\r\nIf you want to navigate to the example site I just created, you can find it here.\r\nClosing Thoughts\r\nI came to this process as a way to publish reports & other data tools (e.g. {crosstalk} “dashboards”) I was creating through Google Sites, since that’s what my organization uses. The benefits of this approach are that I’m integrating my work into a tool that others in CCPS are familiar with, and I’m not being a nuisance and asking our technology department to stand up a new tool/platform that only 1 or 2 people would use to create content. And both of these feel like pretty big wins to me from an organizational perspective.\r\nThat said, there are obviously some drawbacks and situations when I wouldn’t do this. Publishing (and updating) anything is a fairly manual process since you’re copy/pasting html code, and as far as I can tell, there’s no way to automate this. So if you have reports/products that need to be updated daily, this might not be the best approach for you. Similarly, if you’re writing a bunch of parameterized reports and need a place to publish all of them, this process could get tedious very quickly. Overall, I think this publishing to Google Sites works best if you’re publishing static reports that don’t need to be updated too often (e.g. annual/semi annual progress reports; one-off project reports/special requests).\r\nAnyway, hopefully this walkthrough/discussion helps someone who’s in a similar position, and thanks again to Allison Horst & Jacqueline Nolis for the blog that inspired me to delve into this more.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-26-publishing-rmarkdown-to-google-sites/img/google_sites_logo.jpg",
    "last_modified": "2021-05-03T21:47:18-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-25-this-code-is-big-time/",
    "title": "This Code is Big Time",
    "description": "Wrap your R code in Your Mom's House audio drops",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-04-25",
    "categories": [],
    "contents": "\r\nWhat’s up, Jeans. This short post is targeting the very niche crowd of R users who also happen to love Tim and Christine, the main mommies of Your Mom’s House. Basically, I discovered the {beepr} package a few days ago, which makes it easier you play short sound files through R, and so naturally my first thought was to “try it out” with a YMH drop. The result was 2 functions added to my personal/miscellaneous package, {eemisc}:\r\nbig_time(), which will play the “THIS SHIT IS BIG TIME” drop from the YMH intro, and\r\nbig_time_operator(), which lets you wrap a function and play the “THIS SHIT IS BIG TIME” drop whenever you call that function.\r\nI’ll demonstrate these below.\r\nSetup\r\nTo use these functions, you’ll need my personal package, {eemisc}, installed. You can install it via Github via the following command:\r\n\r\n\r\nremotes::install_github(\"ekholme/eemisc\")\r\n\r\n\r\n\r\nEventually, I might put these functions into their own package, but for now they live in my odds-and-ends package.\r\nBig Time\r\nTo play the “THIS SHIT IS BIG TIME” drop, you just need to call the function big_time():\r\n\r\n\r\nlibrary(eemisc)\r\n\r\nbig_time()\r\n\r\n\r\n\r\nThat’s it. That will play the drop.\r\nBig Time Operator\r\nBut I took it a step further for all of my kings and queens above 18. The big_time_operator() takes a function of your choosing and produces as its output a new function that wraps the input function with the “big time” drop. Want to let everyone know that taking the mean is big time?\r\n\r\n\r\nbig_time_mean <- big_time_operator(mean)\r\n\r\nx <- 1:10\r\n\r\nbig_time_mean(x)\r\n\r\n\r\n[1] 5.5\r\n\r\nYou can pass any function you want into big_time_operator() to get the same effect.\r\nAnyway, that’s all for now. Keep it high and tight, Jeans, and you bet I’m coming up in May.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-25-this-code-is-big-time/img/ymh_logo.jpg",
    "last_modified": "2021-04-25T15:23:02-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-02-personalizing-the-distill-template/",
    "title": "Personalizing the Distill Template",
    "description": "How to modify the distill template to fit your preferences.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\r\nI think pretty much everyone will agree that {distill} is great. I use it for my personal website. I use it to write reports for work. There are probably other things I could/should be using it for, too. It makes publishing cleanly-formatted articles super easy, and as someone with minimal background in html/css, it’s been invaluable to me.\r\nOne thing that’s been on my to-do list re: {distill}, though, has been to look into how to modify the Rmd template. What I mean by this is not changing the styling (although I’ve played around with that a little bit, too), but rather how to modify to the template to automatically include the packages I use in pretty much every blog post as well as to set knit options. Obviously, it’s not a huge deal to add this stuff in every time I make a new post – it’s only a few lines of code – but it is a little bit tedious.\r\nMaking an Rmd template is fairly straightforward, and the {rmarkdown} bookdown site has some great resources. But the process for creating an Rmd template while simultaneously taking advantage of the features of distill::create_post() felt less obvious to me. I had a little bit of time a couple of days ago to noodle about it, so I wanted to share what I came up with here.\r\nDisclaimer: This solution feels a bit hacky, so I’d love feedback from anyone reading on how to improve this. Regardless, let’s get into it.\r\nThe Default Distill Template\r\nRunning distill::create_post() while you have your website project open will create a new subdirectory in your _posts directory (assuming you’ve stuck with the defaults). The name of this subdirectory will contain the date and a slug related to the title of your post (again, assuming you’ve stuck with the defaults). It will also create an Rmd file in the newly-created subdirectory that looks something like this:\r\n\r\nAgain, this is great, but what if we want the template to come with, say, library(tidyverse) already included? Or what if we wanted to specify some knit options? That is, what if we want our template to look more like this:\r\n\r\nA Personalized Distill Template\r\nThe function below can do just that. I’ll post it in its entirety, then walk through what it does and how you can modify this if you want.\r\nHere’s the function, which I’m calling create_ee_post(), but you might want to use your own initials:\r\n\r\n\r\ncreate_ee_post <- function(..., open = TRUE) {\r\n\r\n  tmp <- distill::create_post(..., edit = FALSE)\r\n\r\n  yaml <- readLines(tmp, n = 12)\r\n\r\n  con <- file(tmp, open = \"w\")\r\n\r\n  on.exit(close(con), add = TRUE)\r\n\r\n  body <- \r\n  '\r\n  \r\n#```{r setup, echo = TRUE, results = \"hide\", warning = FALSE, message = FALSE}\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc)\r\nlibrary(harrypotter)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\n#```\r\n\r\nA message\r\n'\r\n\r\nxfun::write_utf8(yaml, con)\r\nxfun::write_utf8(body, con)\r\n\r\nif (open == TRUE) usethis::edit_file(tmp)\r\n\r\n}\r\n\r\n\r\n\r\nI’ve included the function in my personal R package if you want to take a peek at it there. One note re: the above is that you’ll want to remove the un-comment the ``` within the body. I’ve commented those out here because I’m embedding them in a code chunk.\r\nRight, so, big picture, the above function just wraps distill::create_post() so we still get all of the goodies of that function, then modifies the Rmd file produced by distill::create_post() after it’s already created. Let’s take a look at pieces more closely.\r\nStep by Step\r\n\r\n\r\ntmp <- distill::create_post(..., edit = FALSE)\r\n\r\n\r\n\r\nThis runs distill::create_post() and allows you to pass whatever arguments you typically would (e.g. setting the title) to that function. I’m setting edit = FALSE here because we don’t want to open the file that this function creates since we’re going to modify it after the fact (n.b. that we’ll add in a line later that will open the modified file, if we want). This will also store the path of the file that’s created to a variable called tmp, which is useful later.\r\n\r\n\r\nyaml <- readLines(tmp, n = 12)\r\n\r\n\r\n\r\nThis will read the first 12 lines of the Rmd file we just created and store them in the yaml variable. If you use the defaults of distill::create_post(), then your YAML header should be 12 lines, although I suppose your mileage could vary. It’s probably possible, too, to use some regex to read in lines between the dashes setting off the YAML header, but I didn’t play around with that.\r\n\r\n\r\ncon <- file(tmp, open = \"w\")\r\n\r\non.exit(close(con), add = TRUE)\r\n\r\n\r\n\r\nThese lines will open a connection to the tmp file we specified earlier and then, once the function exits, close this connection. I basically just copied this from the source code of distill::create_post().\r\n\r\n\r\nbody <- \r\n'\r\n  \r\n#```{r setup, echo = TRUE, results = \"hide\", warning = FALSE, message = FALSE}\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc)\r\nlibrary(harrypotter)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n#```\r\n\r\nA message\r\n'\r\n\r\n\r\n\r\nHere’s the heart of this function. In this chunk, we’re specifying what we want the body (as opposed to the YAML header) of the Rmd file to look like, and everything gets passed in as a string. So, here, I’m doing a few things:\r\nSetting the options for the “setup” chunk,\r\nSetting the global chunk options for this file,\r\nLoading some libraries I commonly use (n.b. that {harrypotter} provides HP-themed color palettes and is my go-to option for colors),\r\nSetting ggplot fill/color options (including just grabbing the hex code for the dark red that’s the first color in the “Hermione Granger” palette)\r\nAnd finally setting the {ggplot2} theme to be the custom theme I’ve created in my {eemisc} package.\r\nI’ve also added a message (“A message”) to the file just for funsies.\r\nIf you’re interested in adapting this function, this is the code you’ll want to change to specify whatever options you prefer in your Rmd files/blog posts. You’ll also need to un-comment the ```s.\r\n\r\n\r\nxfun::write_utf8(yaml, con)\r\nxfun::write_utf8(body, con)\r\n\r\n\r\n\r\nThese lines will write the contents of the yaml variable (which we just pulled from the distill::create_post() default – we didn’t change anything here) and the body variable (which we just specified above) to con, which is the file we want to edit.\r\n\r\n\r\nif (open == TRUE) usethis::edit_file(tmp)\r\n\r\n\r\n\r\nFinally, this will open the file we just created in RStudio.\r\nSo that’s that. Again, this may not be the best way to do this, but it does seem to work. I’d love to hear if others have other (and better) ways of accomplishing this.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-02-personalizing-the-distill-template/img/personalized_distill.PNG",
    "last_modified": "2021-04-05T21:20:22-04:00",
    "input_file": {},
    "preview_width": 1009,
    "preview_height": 502
  },
  {
    "path": "posts/2021-03-24-improving-ggplots-with-text-color/",
    "title": "Improving ggplots with Text Color",
    "description": "A walkthrough of how -- and when -- to replace your legend with colored text",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-03-24",
    "categories": [],
    "contents": "\r\nWhen I was working on my submission for #TidyTuesday Week 11, 2021 (Bechdel Test), it hit me that I wanted to write a blog post showcasing a small but impactful trick I’ve learned from #TidyTuesday for improving my plots – using text color to replace a plot legend.\r\nHere’s my Bechdel Test #TidyTuesday plot as an example:\r\n\r\nYou’ll notice that, instead of having a legend off to the right or the bottom or wherever it is you typically place your legends, I’ve changed the font color of the words “Pass” and “Fail” in the title to contain this same information. We can do this using Claus Wilke’s invaluable {ggtext} package, which provides improved rendering for ggplot2.\r\nLet’s walk through how (and when) to do this, then!\r\nDisclaimer: I’m not a data viz expert, and there are tons of people who create visualizations for #TidyTuesday that regularly blow anything I make out of the water (see CedScherer or geokaramanis, for example). That said, what I want to walk through here is a useful, easy-to-implement trick that I use for a lot of plots I make for my job and doesn’t require you to be a ggplot wizard.\r\nSetup\r\nFirst, let’s load the packages we’ll need. I’m going to load:\r\n{tidyverse}, for {ggplot2} and other data wrangling tools;\r\n{ggtext}, for text rendering, and especially the element_markdown() function;\r\n{harrypotter}, for the colors I’m going to use.\r\nI’m also going to use {ggbeeswarm} to make a beeswarm plot, but if the plot you’re interested in making isn’t a beeswarm plot, then obviously you don’t need this. And, finally, I’ll use my personal miscellaneous package, {eemisc} in the very final example, since it has some custom ggplot theming in it that I like better than theme_minimal(), which I’ll use in the previous examples.\r\nIn this setup step, I’m also going to read in the Bechdel Test data from the #TidyTuesday Github repo and do some very light data processing to return the decade each movie was released in (rather than the year).\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\n#load packages\r\nlibrary(tidyverse)\r\nlibrary(ggtext)\r\nlibrary(eemisc)\r\nlibrary(harrypotter)\r\nlibrary(ggbeeswarm)\r\n\r\n#read in data\r\nmovies <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-09/movies.csv')\r\n\r\n#return decade a movie is released in\r\nmovies <- movies %>%\r\n  mutate(decade = year - (year %% 10))\r\n\r\n#getting the hex codes I want to use -- not strictly necessary to store these; I could just look them up\r\nluna <- hp(n = 2, option = \"LunaLovegood\")\r\n\r\n\r\n\r\nCreate the Initial Plot\r\nNow we’re ready to make a plot. First, I’ll make a “typical” plot – one with a legend describing the values that each color represents. I’ll make a beeswarm plot (similar to a violin plot, but with individual points rather than a density curve) using geom_quasirandom() to examine the IMDB rating distributions for movies that pass and fail the Bechdel test, by decade.\r\n\r\n\r\np_legend <- movies %>%\r\n  ggplot(aes(x = as_factor(decade), y = imdb_rating, color = binary, group = as_factor(binary))) +\r\n  geom_quasirandom(dodge.width = .7, width = .15, alpha = .6) +\r\n  scale_color_hp_d(option = \"LunaLovegood\", name = \"Bechdel Status\") +\r\n  theme_minimal() +\r\n  labs(\r\n    y = \"IMDB Rating\",\r\n    x = \"Decade\",\r\n    title = \"Movies that Pass and Fail the Bechdel Test Have Similar IMDB Rating\\nDistributions\",\r\n    caption = \"Data: FiveThirtyEight & IMDB | Viz: Eric Ekholm (@ekholm_e)\"\r\n  ) +\r\n  theme(\r\n    panel.grid.minor = element_blank(),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\"\r\n  )\r\n\r\np_legend\r\n\r\n\r\n\r\n\r\nSo, this isn’t a bad plot. It’s easy to read and the takeaway is fairly obvious. There are several tweaks we could make to improve it, though, and the one that stands out to me is to do something about the legend. It’s eating up a lot of space on the right-hand side of the graph to present a relatively small amount of information (albeit important information).\r\nOne approach might be to put the legend at the bottom of the plot. It will take up less space this way (and give the plot more space to expand horizontally), but it will still take up some space, and there’s another alternative…\r\nCreate a Legend-less Plot\r\nA better approach would be to use {ggtext} to color the text of the words “Pass” and “Fail” in the title to correspond to the colors of the points in the plot. I prefer this approach for a few reasons:\r\nPeople’s eyes will be drawn to the title anyway, so it makes reading the plot more efficient by allowing the title to pull double duty;\r\nIt frees up horizontal (or vertical) space in the plot itself, and;\r\nAs sort of an added bonus, it makes you think a little more carefully about your plot title.\r\nFortunately, {ggtext} lets us change the color text and labels in ggplots via the element_markdown() function, which we can use within the ggplot theme(). We can just pass some HTML directly into the title (or axis, or caption, or w/e) text and it will render appropriately.\r\nMore specifically, we can put whichever word(s) we want to change in the title within a <span> tag and then specify the style we want to apply within the tag. We then tell theme() that the plot title should be considered markdown rather than text (by setting plot.title = element_markdown()), like so:\r\n\r\n\r\np_color <- movies %>%\r\n  ggplot(aes(x = as_factor(decade), y = imdb_rating, color = binary, group = as_factor(binary))) +\r\n  geom_quasirandom(dodge.width = .7, width = .15, alpha = .6) +\r\n  scale_color_hp_d(option = \"LunaLovegood\", name = \"Bechdel Status\") +\r\n  theme_minimal() +\r\n  labs(\r\n    y = \"IMDB Rating\",\r\n    x = \"Decade\",\r\n    title = \"Movies that <span style='color:#830042'>Pass<\/span> and\r\n    <span style='color:#084d49'>Fail<\/span> the Bechdel Test Have Similar IMDB Rating<br>Distributions\",\r\n    caption = \"Data: FiveThirtyEight & IMDB | Viz: Eric Ekholm (@ekholm_e)\"\r\n  ) +\r\n    theme(\r\n    panel.grid.minor = element_blank(),\r\n    plot.title = element_markdown(), #telling ggplot to interpret the title as markdown\r\n    legend.position = \"none\", #remove the legend\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\"\r\n  )\r\n\r\np_color\r\n\r\n\r\n\r\n\r\nThis plot contains the same information, but the actual plot itself has a lot more breathing room now that we’ve gotten rid of the legend. It also requires our eyes to move to fewer places since the title carries now carries the information that was in the legend.\r\nAgain, it’s the styling specified in the <span> tag within the title text (above) coupled with setting plot.title = element_markdown() within the theme() that allows this. You could also specify other styling (bold, italics, sizes, etc) as necessary, but I won’t focus on that here.\r\nSome Caveats\r\nAlthough I think this is a great trick to improve a lot of different plots, it’s not something I use for everything, and there are definitely cases where it’s not appropriate. Most notably, I wouldn’t use this approach if I had more than 3 or 4 groups/classes represented by colors. This isn’t necessarily a hard and fast rule, but I’ve found that if I have a lot of classes, I have a hard time writing a sensible title that incorporates the name of each class (so that I can color-code the text). If you end up making a list and coloring the text of that list, you might as well just use a legend. I’m also not sure I’d use this approach if I had two colors that were fairly similar. It’s easier to distinguish colors that are adjacent to one another, and so if your words containing color aren’t adjacent in your title, it might be hard to tell them apart if they’re similar colors.\r\nSome Extra Styling\r\nLike I mentioned at the outset, I have a custom ggplot theme in my personal R package ({eemisc}) that tweaks the above plot in a few ways to make it look nicer (in my opinion). Additionally, it assumes titles and subtitles are element_markdown() already, so it saves that line of code. Using my theme, we can revise the previous plot to the following:\r\n\r\n\r\np_final <- movies %>%\r\n  ggplot(aes(x = as_factor(decade), y = imdb_rating, color = binary, group = as_factor(binary))) +\r\n  geom_quasirandom(dodge.width = .7, width = .15, alpha = .6) +\r\n  scale_color_hp_d(option = \"LunaLovegood\") +\r\n  labs(\r\n    y = \"IMDB Rating\",\r\n    x = \"Decade\",\r\n    title = \"Movies that <span style='color:#830042'>Pass<\/span> and\r\n    <span style='color:#084d49'>Fail<\/span> the Bechdel Test Have<br>Similar IMDB Rating Distributions\",\r\n    caption = \"Data: FiveThirtyEight & IMDB | Viz: Eric Ekholm (@ekholm_e)\"\r\n  ) +\r\n  theme_ee(size = 10) +\r\n  theme(\r\n    legend.position = \"none\",\r\n    panel.grid.minor = element_blank()\r\n  )\r\n\r\np_final\r\n\r\n\r\n\r\n\r\nHope this is helpful, and happy plotting, all!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-24-improving-ggplots-with-text-color/improving-ggplots-with-text-color_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-24T13:16:52-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-05-dungeons-and-dragons-part-4/",
    "title": "Dungeons and Dragons - Part 4",
    "description": "Predicting monster challenge ratings & examining regression diagnostics.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\r\nIn a few of my earlier posts, I walked through pulling Dungeons and Dragons data from an API, exploring data on D&D monsters, and using latent profile analysis to place these monsters into groups. In this last post of this D&D mini-series, I’m going to try to use monsters’ statistics to predict their challenge rating, i.e. how difficult the monster is to fight (which the name pretty much gives away). I’m going to use this as an opportunity to explore some model diagnostics and talk about tradeoffs in fitting models.\r\nBefore getting into this, I want to give a shoutout to Julia Silge’s recent-ish blog post that gave me the idea (and some of the code) to explore model diagnostics via {tidymodels}. So let’s get going!\r\nSetup\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc) #ggplot theme\r\nlibrary(jsonlite) #work with json data\r\nlibrary(harrypotter) #colors\r\nlibrary(tidymodels)\r\nlibrary(finetune)\r\nlibrary(vip)\r\nlibrary(tidytext)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n\r\n#getting data from api -- see 1st d&d post\r\n#for process explanation\r\n\r\nfetch_monster <- function(monster) {\r\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n  \r\n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\r\n    enframe() %>%\r\n    pivot_wider(names_from = name,\r\n                values_from = value)\r\n  \r\n  return(ret)\r\n}\r\n\r\ncompare_lens <- function(x, size = 1) {\r\n  all(map_lgl(x, ~length(unlist(.x)) == size))\r\n}\r\ncond_unlist <- function(x) {\r\n  if (compare_lens(x) == TRUE) {\r\n    unlist(x)\r\n  } else {\r\n    x\r\n  }\r\n}\r\n\r\nmons <- fromJSON(dnd_base)$results %>%\r\n  pull(index)\r\n\r\nmonster_lists <- purrr::map(mons, fetch_monster)\r\n\r\nmons_bind <- bind_rows(monster_lists)\r\n\r\nmons_df <- mons_bind %>%\r\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\r\n\r\n\r\n\r\nLet’s take a look at our data.\r\n\r\n\r\nglimpse(mons_df)\r\n\r\n\r\nRows: 332\r\nColumns: 31\r\n$ index                  <chr> \"aboleth\", \"acolyte\", \"adult-black...\r\n$ name                   <chr> \"Aboleth\", \"Acolyte\", \"Adult Black...\r\n$ size                   <chr> \"Large\", \"Medium\", \"Huge\", \"Huge\",...\r\n$ type                   <chr> \"aberration\", \"humanoid\", \"dragon\"...\r\n$ subtype                <list> [NULL, \"any race\", NULL, NULL, NU...\r\n$ alignment              <chr> \"lawful evil\", \"any alignment\", \"c...\r\n$ armor_class            <int> 17, 10, 19, 19, 18, 19, 18, 19, 19...\r\n$ hit_points             <int> 135, 9, 195, 225, 172, 212, 184, 2...\r\n$ hit_dice               <chr> \"18d10\", \"2d8\", \"17d12\", \"18d12\", ...\r\n$ speed                  <list> [[\"10 ft.\", \"40 ft.\"], [\"30 ft.\"]...\r\n$ strength               <int> 21, 10, 23, 25, 23, 25, 23, 27, 23...\r\n$ dexterity              <int> 9, 10, 14, 10, 10, 10, 12, 14, 12,...\r\n$ constitution           <int> 15, 10, 21, 23, 21, 23, 21, 25, 21...\r\n$ intelligence           <int> 18, 10, 14, 16, 14, 16, 18, 16, 18...\r\n$ wisdom                 <int> 15, 14, 13, 15, 13, 15, 15, 15, 15...\r\n$ charisma               <int> 18, 11, 17, 19, 17, 19, 17, 24, 17...\r\n$ proficiencies          <list> [<data.frame[5 x 2]>, <data.frame...\r\n$ damage_vulnerabilities <list> [[], [], [], [], [], [], [], [], ...\r\n$ damage_resistances     <list> [[], [], [], [], [], [], [], [], ...\r\n$ damage_immunities      <list> [[], [], \"acid\", \"lightning\", \"fi...\r\n$ condition_immunities   <list> [[], [], [], [], [], [], [], [], ...\r\n$ senses                 <list> [[\"120 ft.\", 20], [12], [\"60 ft.\"...\r\n$ languages              <chr> \"Deep Speech, telepathy 120 ft.\", ...\r\n$ challenge_rating       <dbl> 10.00, 0.25, 14.00, 16.00, 13.00, ...\r\n$ xp                     <int> 5900, 50, 11500, 15000, 10000, 130...\r\n$ special_abilities      <list> [<data.frame[3 x 3]>, <data.frame...\r\n$ actions                <list> [<data.frame[4 x 7]>, <data.frame...\r\n$ legendary_actions      <list> [<data.frame[3 x 4]>, NULL, <data...\r\n$ url                    <chr> \"/api/monsters/aboleth\", \"/api/mon...\r\n$ reactions              <list> [NULL, NULL, NULL, NULL, NULL, NU...\r\n$ forms                  <list> [NULL, NULL, NULL, NULL, NULL, NU...\r\n\r\nThere’s a ton of data here, and a lot of it is still in deep-list-land. If this were a “real” project (i.e. not a blog post and something with stakes tied to it), I’d probably going digging more through these lists to search for useful features. But since this is just a blog post, I’m largely going to focus on the easy-to-use data (i.e. stuff that’s already a good-old atomic vector).\r\nFeature Engineering\r\nThat said, one feature I do want to add is whether or not the monster is a spellcaster. And because I’ve dug around in this data a little bit before (and because I play D&D), I know this is contained within the “special abilities” list-column. So, I’m going to enlist some help from {purrr}'s map_int() and pluck() to dig into this column, identify monsters that have a can cast spells (they’ll have an ability called either “Spellcasting” or “Innate Spellcasting”), and then create a binary yes/no feature.\r\nBeyond that, I’m going to keep a handful of other potentially useful features: - size (a nominal feature ranging from “tiny” to “gargantuan”), - type (a nominal feature indicating whether the monster is a humanoid, beast, dragon, etc), - armor class (a numeric feature indicating how much armor a monster has), and - all of the ability scores (strength through intelligence; all numeric)\r\n\r\n\r\nmons_df_small <- mons_df %>%\r\n  mutate(spellcaster = map_int(seq_along(1:nrow(mons_df)), ~pluck(mons_df$special_abilities, .x, \"name\") %>%\r\n                                 paste(collapse = \", \") %>%\r\n                                 str_detect(\"Spellcast\"))) %>%\r\n  select(index, size, type, armor_class, strength, dexterity, constitution, wisdom, charisma, intelligence, spellcaster, challenge_rating)\r\n\r\n\r\n\r\nSplitting Data\r\nNow I can get into the {tidymodels} flow of splitting data, specifying a recipe, specifying a model, tuning the model, etc. We’ll use bootstrapping here rather than cross validation to split our data because we have a pretty small sample size in the training set (268 obs).\r\n\r\n\r\nset.seed(0408)\r\nmons_split <- initial_split(mons_df_small, strata = challenge_rating, prop = 4/5)\r\ntrn <- training(mons_split)\r\ntst <- testing(mons_split)\r\n\r\n#and also getting our folds\r\nbooties <- bootstraps(trn)\r\n\r\n\r\n\r\nPreprocessing with Recipes\r\nI’m going to do some pretty minimal preprocessing here. There’s more I could do (and I’ll revisit some later, actually), but for now I’m just going to:\r\nTell the model not to use index, which is an id column, in the model,\r\nCreate an “other” category for type (since there are many types, some with low counts),\r\nScale all of the numeric variables, and\r\nDummy-out the size and type variables.\r\n\r\n\r\nmons_rec <- recipe(challenge_rating ~ ., data = trn) %>%\r\n  update_role(index, new_role = \"id_var\") %>%\r\n  step_other(type) %>%\r\n  step_scale(armor_class, strength, dexterity, constitution, wisdom, charisma, intelligence) %>%\r\n  step_dummy(size, type)\r\n\r\n\r\n\r\nSetting Model Specifications\r\nNext, I’ll specify the model I want to fit. Again, there are lots of options here, and if I wanted the best-performing model, I might use xgboost or catboost or something, but I’m just going to stick with a linear model here because I think it will do decently well and they’re faster. More specifically, I’m going to use a lasso model to regularize the regression and potentially do some feature selection for me.\r\n\r\n\r\nlasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\r\n  set_engine(\"glmnet\")\r\n\r\n#and combining into a workflow\r\nlasso_wf <- workflow() %>%\r\n  add_recipe(mons_rec) %>%\r\n  add_model(lasso_spec)\r\n\r\n\r\n\r\nFitting Model\r\nNow we fit the model. Even though this should be quick to fit, I’m going to use tune_race_anova() from the {finetune} package (rather than, say, tune_grid()) to speed up the process a little bit (see Max Kuhn’s video from rstudioconf 2021 for more details about this).\r\n\r\n\r\ndoParallel::registerDoParallel()\r\nset.seed(0408)\r\n\r\nlasso_res <- tune_race_anova(\r\n  lasso_wf,\r\n  resamples = booties\r\n)\r\n\r\n\r\n\r\nModel Selection\r\nNow that the models are fit, I’ll look at the accuracy metrics real quick via autoplot().\r\n\r\n\r\nautoplot(lasso_res)\r\n\r\n\r\n\r\n\r\nIt looks like the best model here has an R-squared of ~.85, which is really good (well, I’m used to modeling education data, where an R-squared of .85 is obscenely high, but I suppose other people’s mileage may vary). I’m going to select the simplest model here that is within 1 standard error of the numerically best model, in the hopes that this will give me some feature selection as well. And once I select that, I’m going to finalize the workflow and use last_fit() to train the model with the selected parameters on the full training set and then evaluate it on the test set.\r\n\r\n\r\nparams <- select_by_one_std_err(lasso_res, metric = \"rmse\", penalty)\r\n\r\nlasso_fin_wf <- finalize_workflow(lasso_wf, params)\r\n\r\n#and doing our last fit\r\nlasso_fin_fit <- last_fit(lasso_fin_wf, mons_split)\r\n\r\n\r\n\r\nFrom there, we can check out the final model’s performance on the test set.\r\n\r\n\r\n#check out final test set performance\r\ncollect_metrics(lasso_fin_fit)\r\n\r\n\r\n# A tibble: 2 x 4\r\n  .metric .estimator .estimate .config             \r\n  <chr>   <chr>          <dbl> <chr>               \r\n1 rmse    standard       1.85  Preprocessor1_Model1\r\n2 rsq     standard       0.871 Preprocessor1_Model1\r\n\r\nOur R-squared on the test set is .871, which is even better than we did on our bootstraps earlier. Not bad!\r\nDiagnosing Model\r\nWe could stop here, but I think it’s worthwhile to dig into our model a bit more to see if anything stands out/if there’s a way we could improve it. To do this, I’m going to look at a few plots:\r\nthe predicted values vs the actual values, and\r\nthe predicted values vs the residuals\r\nFirst, let’s look at predicted values vs the actual challenge ratings:\r\n\r\n\r\ncollect_predictions(lasso_fin_fit) %>%\r\n  ggplot(aes(x = challenge_rating, y = .pred)) +\r\n  geom_abline(lty = 2) +\r\n  geom_point(color = herm, alpha = .4)\r\n\r\n\r\n\r\n\r\nSo, there are a number of things that stand out to me. First, it’s clearly not a bad model, but there are some areas where the model is missing by quite a bit. For instance, there’s quite a bit of variability in predictions for low-CR monsters, and even some negative predictions (which isn’t possible).\r\nLet’s also take a look at the residuals vs the predictions.\r\n\r\n\r\naugment(lasso_fin_fit) %>%\r\n  ggplot(aes(x = .pred, y = .resid)) +\r\n  geom_point(color = herm, alpha = .4) +\r\n  geom_smooth(color = \"black\")\r\n\r\n\r\n\r\n\r\nWhat we’d want to see here is basically no pattern, and a constant variance in the residuals, which isn’t quite what we get here (although this is going to be somewhat harder to see with a small sample, since this is just plotting the 64-observation test data).\r\nAgain – this isn’t a terrible model, but there are a few things we could do to improve it. One would be to drop the Tarrasque observation, since it’s an extreme CR (it has a CR of 30, which is well beyond any other monster). It doesn’t show up in the plot above, but I know from previous data exploration that it’s different from other monsters.\r\nAnother approach is to log-transform challenge_rating (our DV), since I know from previous exploration that it has a strong right skew. This might help with unequal variances in the error terms.\r\n(n.b. that there are other approaches we could take, too, including fitting different type of model or doing some more feature engineering).\r\nRefitting with a Log-Transformed DV\r\nI won’t walk through everything here, but I’m basically redoing all of the previous steps, but adding a log transformation to challenge_rating.\r\n\r\n\r\nmons_rec2 <- recipe(challenge_rating ~ ., data = trn) %>%\r\n  update_role(index, new_role = \"id_var\") %>%\r\n  step_log(all_outcomes(), offset = .1) %>%\r\n  step_other(type) %>%\r\n  step_scale(armor_class, strength, dexterity, constitution, wisdom, charisma, intelligence) %>%\r\n  step_dummy(size, type)\r\n\r\nlasso_wf2 <- workflow() %>%\r\n  add_recipe(mons_rec2) %>%\r\n  add_model(lasso_spec)\r\n\r\n#fitting model\r\ndoParallel::registerDoParallel()\r\nset.seed(0408)\r\n\r\nlasso_res2 <- tune_race_anova(\r\n  lasso_wf2,\r\n  resamples = booties\r\n)\r\n\r\nparams2 <- select_by_one_std_err(lasso_res2, metric = \"rmse\", penalty)\r\n\r\nlasso_fin_wf2 <- finalize_workflow(lasso_wf2, params2)\r\n\r\n#and doing our last fit\r\nlasso_fin_fit2 <- last_fit(lasso_fin_wf2, mons_split)\r\n\r\ncollect_metrics(lasso_fin_fit2)\r\n\r\n\r\n# A tibble: 2 x 4\r\n  .metric .estimator .estimate .config             \r\n  <chr>   <chr>          <dbl> <chr>               \r\n1 rmse    standard       0.590 Preprocessor1_Model1\r\n2 rsq     standard       0.872 Preprocessor1_Model1\r\n\r\nWe see that this model gives us basically the same R-squared. The RMSE isn’t directly comparable since we’ve log-transformed the outcome. But let’s take a look at our predictions. To do that, I first need to recalculate the predictions and residuals to account for the log scale here.\r\n\r\n\r\nlasso_fit2_aug <- augment(lasso_fin_fit2) %>%\r\n  mutate(pred = exp(.pred),\r\n         resid = challenge_rating - pred)\r\n\r\nlasso_fit2_aug %>%\r\n  ggplot(aes(x = challenge_rating, y = pred)) +\r\n  geom_point(color = herm, alpha = .4) +\r\n  geom_abline(lty = 2)\r\n\r\n\r\n\r\n\r\nOk, so, this model does a lot better for very low CR monsters than the previous model did. And recall that most monsters are low CR. However, it seems to perform worse for very high CR monsters – we can see that it’s predicting a CR of over 35 for a monster with an actual CR of 24, which is a pretty big miss.\r\nWe can see something similar when plotting our residuals vs predictions. Through, say, CR 10, the model seems decent, but not so much after that.\r\n\r\n\r\nlasso_fit2_aug %>%\r\n  ggplot(aes(x = pred, y = resid)) +\r\n  geom_point(color = herm, alpha = .4) +\r\n  geom_smooth(color = \"black\")\r\n\r\n\r\n\r\n\r\nSo which of these is the better model? Neither is ideal, obviously, but it depends on what you want to do. The first model seems to be more stable (but not great) across all possible CR values (although let’s not forget that it gave us some negative predictions, which isn’t good). The second model is much better at predicting low CR monsters but much worse at predicting high CR monsters. I sort of like the 2nd one better since low CR monsters are much more common.\r\nInterpreting Coefficients\r\nFinally, let’s interpret the coefficients of this second model. Again, bear in mind that these are the coefficients of a model that does not do a good job at predicting high CR monsters. I’m going to facet these coefficients out so that things in the same facet are roughly comparable.\r\n\r\n\r\nlasso_coefs <- pull_workflow_fit(lasso_fin_fit2$.workflow[[1]]) %>%\r\n  vi()\r\n\r\nlasso_coefs %>%\r\n  mutate(Importance = if_else(Sign == \"NEG\", -1*Importance, Importance)) %>%\r\n  mutate(coef_type = case_when(\r\n    str_detect(Variable, \"type\") ~ \"Monster Type\",\r\n    str_detect(Variable, \"size\") ~ \"Monster Size\",\r\n    TRUE ~ \"Other\"\r\n  )) %>%\r\n  ggplot(aes(y = reorder_within(Variable, Importance, coef_type), x = Importance)) +\r\n  geom_col(aes(fill = Importance > 0)) +\r\n  facet_wrap(vars(coef_type), scales = \"free\") +\r\n  scale_y_reordered() +\r\n  labs(\r\n    y = NULL,\r\n    x = \"Beta\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\",\r\n    plot.margin = margin(t = 14, b = 14, l = 7, r = 7)\r\n  )\r\n\r\n\r\n\r\n\r\nI’d take the “size” stuff here with a grain of salt, since the reference category is “gargantuan” and those are the monsters it does the worst with. The “type” coefficients make sense to me – the reference category is “beast,” and I’d expect those to generally have lower CRs than like fiends (demons & whatnot), monstrosities, etc. And of the coefficients in “other,” it’s no surprise that constitution is the strongest predictor – regardless of how a monster fights or what you expect them to do, harder monsters will have more health. We also see that our spellcaster binary feature (which is not on the same scale as the others in this facet) has a positive effect.\r\nThat’s going to be it for now, and probably the end of this little D&D series of posts. There’s a lot more that could be done with this data – both with the monster data and with other data available through the API – so who knows, I may pick it back up at some point.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-05-dungeons-and-dragons-part-4/dungeons-and-dragons-part-4_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-03-10T06:43:55-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-03-dungeons-and-dragons-part-3/",
    "title": "Dungeons and Dragons - Part 3",
    "description": "Grouping D&D monsters using latent profile analysis.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2021-02-03",
    "categories": [],
    "contents": "\r\nThis post is the next installment in a mini-series I’m doing where I explore Dungeons and Dragons (D&D) data. In the first post, I showed how to wrangle JSON data from a D&D API, and in the second post, I explored monster statistics.\r\nIn this post, I’m going to use latent profile analysis (LPA) and monsters’ ability scores to try to classify monsters into different “classes.” For instance, we might suspect that there is a “class” of up-in-your-face monsters that have high strength and constitution but low intelligence, whereas there may be another class of spell casters that has high intelligence and charisma but low strength and constitution. LPA gives us a framework to estimate how many of these classes exist as well as which monsters fall into which classes.\r\nNote: I’m probably going to use the terms profile, class, group, and cluster somewhat interchangeably throughout this post (because I’m undisciplined), so just as a warning ahead of time – these all mean pretty much the same thing.\r\nBefore getting into the meat of this post, I want to give a shoutout to the excellent {tidyLPA]} package, which provides some functions to make doing LPA easier.\r\nSo let’s get into it, then.\r\nSetup\r\nFirst, I’ll load some packages that are useful here. Notably, we’ll load {tidyLPA} for doing the actual LPA. This calls the {mclust} package to fit the model, and I could use {mclust} directly, but {tidyLPA} makes the process somewhat more convenient. I’m also going to get a tibble of monster data in this setup chunk. I walk through this process in my first D&D blog post, so if you’re curious about that, feel free to check out that post.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc) #ggplot theme\r\nlibrary(jsonlite) #work with json data\r\nlibrary(tidyLPA)\r\nlibrary(harrypotter) #colors\r\nlibrary(tidytext) #for reorder_within function\r\nlibrary(gt) #make tables\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n\r\n#getting data from api -- see 1st d&d post\r\n#for process explanation\r\n\r\nfetch_monster <- function(monster) {\r\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n  \r\n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\r\n    enframe() %>%\r\n    pivot_wider(names_from = name,\r\n                values_from = value)\r\n  \r\n  return(ret)\r\n}\r\n\r\ncompare_lens <- function(x, size = 1) {\r\n  all(map_lgl(x, ~length(unlist(.x)) == size))\r\n}\r\ncond_unlist <- function(x) {\r\n  if (compare_lens(x) == TRUE) {\r\n    unlist(x)\r\n  } else {\r\n    x\r\n  }\r\n}\r\n\r\nmons <- fromJSON(dnd_base)$results %>%\r\n  pull(index)\r\n\r\nmonster_lists <- purrr::map(mons, fetch_monster)\r\n\r\nmons_bind <- bind_rows(monster_lists)\r\n\r\nmons_df <- mons_bind %>%\r\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\r\n\r\n\r\n\r\nWhat is LPA?\r\nBefore getting into actually fitting any models, I want to do a very brief and minimally technical toe-dip into what LPA actually is. Like I mentioned just above, LPA is an approach for estimating groups of people/things/monsters/whatever. You might think of it as akin to principal components analysis (PCA) or exploratory factor analysis (EFA), except instead of grouping variables together, LPA groups observations together. LPA is itself a special type of mixture model, and the basic idea of mixture modeling is that there are k latent (underlying) subpopulations (groups) within a population, where the number of groups k is something the researcher/analyst should determine either empirically or using past research.\r\nIf you’re familiar with k-means clustering, this probably sounds very similar. And it pretty much is. The main difference between LPA and k-means is that, since LPA estimates distributions for each subpopulation in the model, we get the probability that each observation belongs to each class, e.g. Monster 1 might have a 99% probability of belonging to class 1, whereas Monster 2 might have an 80% probability of belonging to class 1. And depending on how you intend to use your model, this information might be useful. Another potentially useful difference is that LPA provides a number of fit statistics that (at least as far as I know) aren’t available for k-means clustering.\r\nAs with EFA (and other latent-variable models), the assumption of LPA is that the latent (unobserved) factor “causes” (I’m using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit.\r\nThere are some other, more practical considerations regarding LPA that I’ll bring up once we get into the modeling, but this is enough background to get started.\r\nBinning Monsters\r\nI’m not really going to do any data exploration here, since I did that in a previous post. But I do want to quickly pull up a distribution of monster challenge rating because it’s relevant to a step we need to take before fitting our LPA.\r\n\r\n\r\nmons_df %>%\r\n  ggplot(aes(x = challenge_rating)) +\r\n  geom_histogram(alpha = .8, fill = herm, color = herm) +\r\n  labs(\r\n    title = \"Distribution of Monster Challenge Rating\"\r\n  )\r\n\r\n\r\n\r\n\r\nFor those not in the know, a monster’s challenge rating in D&D refers to how difficult it is to fight (you probably could have guess that from the name), where higher CR = more difficult. Monsters can be difficult for many reasons, such as how they move and how many/which types of spells they have, but often, higher CR means higher ability scores. Since I’m going to be doing an LPA where ability scores are the profile indicators, I want to account for the fact that monsters with higher CRs will tend to have higher ability scores. Without accounting for this, CR will (likely) dominate our LPA, and we’d (probably) end up with profiles that represent “very low level monster,” “kinda low level monster,” “mid-level monster,” etc, rather than profiles that represent a monster archetype (e.g. spellcaster, beefcake).\r\nThe approach I’m going to take is to create sexile bins using monster CR to get 6 “bins” of monsters. Using this approach, monsters in each bin will have (roughly) the same CR. We can see this in the plot below.\r\n\r\n\r\n#divide monsters into hexiles\r\nmons_bin <- mons_df %>%\r\n  mutate(cr_bin = ntile(x = challenge_rating, n = 6)) \r\n\r\nmons_bin %>%\r\n  ggplot(aes(x = challenge_rating, y = cr_bin, color = cr_bin)) +\r\n  geom_jitter() +\r\n  scale_color_hp() +\r\n  labs(\r\n    y = \"Challenge Bin\",\r\n    x = \"Challenge Rating\",\r\n    title = \"Binned CR by Actual CR\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nSince the distribution we saw above has a long right tail, the higher-CR bins are more spread out than the lower bins, but obviously each bin has less variance than the entire population.\r\nThe next step is to group mean center each monster’s ability scores. To do this, I’ll subtract the bin-average ability score from the monster ability score. So, for example, to group mean center a monster in bin 1’s strength score, I’d subtract the bin-average strength score from the monster’s strength score. The resulting score tells me if the monster’s score is higher than average for it’s CR or lower than average for it’s CR. Importantly, this removes the effect of the bin average on our LPA. I could go a step further and divide by the bin-specific standard deviation to remove the effect of bin-specific variance, but that makes the estimates somewhat harder to interpret, so I’m not going to.\r\n\r\n\r\nab_scores <- c(\"strength\", \"dexterity\", \"constitution\", \"intelligence\", \"wisdom\", \"charisma\")\r\n\r\nmons_bin <- mons_bin %>%\r\n  group_by(cr_bin) %>%\r\n  mutate(across(.cols = ab_scores, .fns = mean, .names = \"{.col}_bin_mean\")) %>%\r\n  ungroup()\r\n\r\n\r\nab_scores_grp <- str_replace_all(ab_scores, \"$\", \"_bin_mean\")\r\n\r\n\r\nmons_centered <- map2_dfc(mons_bin[, ab_scores], mons_bin[, ab_scores_grp],\r\n         ~.x - .y) %>%\r\n  rename_with(.fn = ~str_replace_all(.x, \"$\", \"_centered\")) %>%\r\n  bind_cols(mons_bin, .) %>%\r\n  select(name, ends_with(\"centered\"))\r\n\r\n\r\n\r\nFit LPA & Select Best Model\r\nThe next step is to estimate our LPA. This is very easy using {tidyLPA}’s estimate_profiles() function. Here’s what I’m doing in the step below:\r\nSetting the n_profiles option to 1:5, which will fit models with 1 profile, 2 profiles, …, 5 profiles.\r\nSetting the variances option to equal and varying. This will fit 1-5 profile models where the conditional variances of each indicator are constrained to be equal across profiles as well as 1-5 profile models where the conditional variances are allowed to vary.\r\nSetting covariances equal to 0. The means that indicator variables are conditionally independent.\r\nTelling the model to only use the centered ability score variables to estimate the profiles.\r\nAlso! always remember to set your seed.\r\n\r\n\r\nset.seed(0408)\r\nlpa_fits <- mons_centered %>%\r\n  estimate_profiles(1:5,\r\n                    variances = c(\"equal\", \"varying\"),\r\n                    covariances = c(\"zero\", \"zero\"),\r\n                    select_vars = str_subset(names(mons_centered), \"centered\"))\r\n\r\n\r\n\r\nNow that the model is fit, I want to select the best model. {tidyLPA} makes this pretty easy via the compare_solutions() function, which I’ll use later, but I also want to explore a few fit indices – Bayesian Information Criterion (BIC) and entropy. I’ve forgotten what BIC actually is, but I do know that it’s the fit index that people who know more about statistics than I do suggest we want to minimize in LPA. Entropy is a measure of how distinct classes are from one another and ranges from 0-1, where values closer to 1 represent more separation between classes (which is what we want). I’m going to pull these out and plot them.\r\n\r\n\r\nmods <- names(lpa_fits)\r\n#recall that model 1 corresponds to equal variances; model 2 corresponds to varying variances\r\n\r\n#getting some fit indices\r\nbics <- map_dbl(1:10, ~pluck(lpa_fits, .x, \"fit\", \"BIC\"))\r\nentrops <- map_dbl(1:10, ~pluck(lpa_fits, .x, \"fit\", \"Entropy\"))\r\n\r\nfit_indices <- bind_cols(mods, bics, entrops) %>%\r\n  set_names(c(\"model\", \"bic\", \"entrop\")) %>%\r\n  pivot_longer(cols = c(\"bic\", \"entrop\"),\r\n               names_to = \"metric\",\r\n               values_to = \"val\")\r\n\r\nfit_indices %>%\r\n  ggplot(aes(x = val, y = reorder_within(model, val, metric), fill = metric)) +\r\n  geom_col() +\r\n  geom_text(aes(label = if_else(val > 1, round(val, 0), round(val, 3)), x = val - .01), hjust = 1, color = \"white\") +\r\n  facet_wrap(vars(metric), scales = \"free\") +\r\n  scale_y_reordered() +\r\n  labs(\r\n    y = \"Model\",\r\n    x = \"Value\",\r\n    title = \"Selected Fit Indices\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nA couple things to keep in mind here:\r\nModels with 1 class will always have an entropy of 1 since there’s no overlap between classes.\r\nModel 1, Class 5 (equal variances, 5 profiles) threw a warning indicating that < 1% of observations were classified into one profile, so we probably don’t want to use that one.\r\nWe can see in the above plot that Model 2, Class 5 (varying variances, 5 profiles) has the lowest BIC (other than the model we want to ignore anyway) and the highest entropy (other than the 1-profile models). So that’s the one we likely want to go with, but let’s see what compare_solutions() tells me.\r\n\r\n\r\nlpa_fits %>%\r\n  compare_solutions()\r\n\r\n\r\nCompare tidyLPA solutions:\r\n\r\n Model Classes BIC      Warnings\r\n 1     1       10533.37         \r\n 1     2       10299.52         \r\n 1     3       10178.13         \r\n 1     4       10098.73         \r\n 1     5       10070.50 Warning \r\n 2     1       10533.37         \r\n 2     2       10314.86         \r\n 2     3       10169.60         \r\n 2     4       10110.16         \r\n 2     5       10082.39         \r\n\r\nBest model according to BIC is Model 1 with 5 classes.\r\n\r\nAn analytic hierarchy process, based on the fit indices AIC, AWE, BIC, CLC, and KIC (Akogul & Erisoglu, 2017), suggests the best solution is Model 2 with 5 classes.\r\n\r\nExamine Profiles\r\nNow that we’ve selected a model specification, let’s look at what each profile looks like. The plot below shows the average bin-centered ability scores for each profile.\r\n\r\n\r\nprof_estimates <- get_estimates(lpa_fits) %>%\r\n  filter(Model == 2,\r\n         Classes == 5)\r\n\r\nprof_estimates %>%\r\n  filter(Category == \"Means\") %>%\r\n  mutate(Class = str_replace_all(Class, \"^\", \"Profile \"),\r\n         Parameter = str_remove_all(Parameter, \"_centered\")) %>%\r\n  ggplot(aes(x = Estimate, y = Parameter)) +\r\n  geom_col(aes(fill = if_else(Estimate > 0, TRUE, FALSE))) +\r\n  facet_wrap(vars(Class)) +\r\n  labs(\r\n    y = NULL,\r\n    x = \"CR-Centered Ability Score\",\r\n    title = \"Ability Score Means by Profile\",\r\n    caption = \"5 profiles, varying variances\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nProfiles 2 and 4 aren’t terribly interesting – they represent monsters that have all ability scores either higher or lower than the bin average. This is probably an artifact of the challenge rating variation within each bin (especially the higher ones). Profiles 1, 3, and 5 are interesting, though.\r\nProfile 1 seems to represent spell-casting monsters with lower strength and constitution but higher intelligence, dexterity, and charisma.\r\nLikewise, Profile 3 looks like a different type of spellcaster, with very high intelligence and charisma.\r\nProfile 5 is more of a fighter-type monster, with higher strength and constitution but low intelligence and charisma.\r\nNext, I’ll check how many monsters ended up in each class. I didn’t get a warning for this model suggesting that a class had dangerously low membership, but it’s still good practice to check this.\r\n\r\n\r\nclass_assigns <- get_data(lpa_fits) %>%\r\n  filter(model_number == 2,\r\n         classes_number == 5) %>%\r\n  group_by(id) %>%\r\n  filter(Probability == max(Probability)) %>%\r\n  ungroup() %>%\r\n  select(name, Class, Probability)\r\n\r\nclass_assigns %>%\r\n  count(Class)\r\n\r\n\r\n# A tibble: 5 x 2\r\n  Class     n\r\n  <dbl> <int>\r\n1     1    72\r\n2     2    61\r\n3     3    33\r\n4     4    49\r\n5     5   117\r\n\r\nThis feels reasonable to me. The smallest class (Class 3) still comprises ~10% of the total monsters in the dataset.\r\nFinally, let’s take a quick look at one example monster from each class as well as the estimated probability that the mosnter belongs to that class (note that the assigned/estimated class is the class for each monster with the highest probability.)\r\n\r\n\r\nset.seed(0409)\r\nclass_assigns %>%\r\n  group_by(Class) %>%\r\n  sample_n(size = 1) %>%\r\n  ungroup() %>%\r\n  select(Class, name, Probability) %>%\r\n  gt() %>%\r\n  tab_header(\r\n    title = \"Example Monster for Each Class\"\r\n  ) %>%\r\n  cols_label(\r\n    Class = \"Esimated Class\",\r\n    name = \"Monster\",\r\n    Probability = \"Estimated Class Prob\"\r\n  ) %>%\r\n  fmt_percent(\r\n    columns = vars(Probability)\r\n  )\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#cnbnyptbzc .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#cnbnyptbzc .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#cnbnyptbzc .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#cnbnyptbzc .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#cnbnyptbzc .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#cnbnyptbzc .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#cnbnyptbzc .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#cnbnyptbzc .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#cnbnyptbzc .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#cnbnyptbzc .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#cnbnyptbzc .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#cnbnyptbzc .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#cnbnyptbzc .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#cnbnyptbzc .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#cnbnyptbzc .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#cnbnyptbzc .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#cnbnyptbzc .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#cnbnyptbzc .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#cnbnyptbzc .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#cnbnyptbzc .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#cnbnyptbzc .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#cnbnyptbzc .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#cnbnyptbzc .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#cnbnyptbzc .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#cnbnyptbzc .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#cnbnyptbzc .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#cnbnyptbzc .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nExample Monster for Each Class\r\n    \r\n    Esimated Class\r\n      Monster\r\n      Estimated Class Prob\r\n    1\r\n      Magmin\r\n      99.27&percnt;\r\n    2\r\n      Ancient Silver Dragon\r\n      100.00&percnt;\r\n    3\r\n      Young Black Dragon\r\n      94.79&percnt;\r\n    4\r\n      Swarm of Insects\r\n      96.74&percnt;\r\n    5\r\n      Hippogriff\r\n      99.59&percnt;\r\n    \r\n\r\nThat’s it for now! Hopefully that was interesting to people who like D&D and helpful for anyone interested in LPA. I’ll probably do one more post in this series, and likely one that gets into some predictive modeling.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-03-dungeons-and-dragons-part-3/dungeons-and-dragons-part-3_files/figure-html5/cr_distrib-1.png",
    "last_modified": "2021-03-05T08:24:32-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-11-dungeons-and-dragons-part-2/",
    "title": "Dungeons and Dragons - Part 2",
    "description": "Exploring monster stats in D&D 5e",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-12-28",
    "categories": [],
    "contents": "\r\nIn my last blog post, I walked through how to extract and wrangle/rectangle monster data from the Dungeons and Dragons 5th edition (D&D 5e) API. In this post, I want to explore this data a little bit – looking at some counts, descriptive statistics, etc. In later posts, I’m planning to do some different statistical analyses, potentially include cluster analysis and some predictive modeling.\r\nOne note – the monsters represented in this data aren’t all of the monsters in D&D. The API I’m using has monsters from the systems reference document (SRD), and this doesn’t include all of the monsters introduced in specific campaigns or in books like Xanathar’s Guide to Everything.\r\nWith all of that said, let’s get to it.\r\nSetup\r\nTo start, I’m going to load in some packages and get the data. If you’re interested in the process for getting the data, you might want to check out my previous post in this series.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(eemisc)\r\nlibrary(jsonlite)\r\nlibrary(ggridges)\r\nlibrary(gt)\r\nlibrary(corrr)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 7, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n\r\n### Functions and process for getting data; described in previous post\r\nfetch_monster <- function(monster) {\r\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n  \r\n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\r\n    enframe() %>%\r\n    pivot_wider(names_from = name,\r\n                values_from = value)\r\n  \r\n  return(ret)\r\n}\r\n\r\ncompare_lens <- function(x, size = 1) {\r\n  all(map_lgl(x, ~length(unlist(.x)) == size))\r\n}\r\ncond_unlist <- function(x) {\r\n  if (compare_lens(x) == TRUE) {\r\n    unlist(x)\r\n  } else {\r\n    x\r\n  }\r\n}\r\n\r\nmons <- fromJSON(dnd_base)$results %>%\r\n  pull(index)\r\n\r\nmonster_lists <- map(mons, fetch_monster)\r\n\r\nmons_bind <- bind_rows(monster_lists)\r\n\r\nmons_df <- mons_bind %>%\r\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\r\n\r\n\r\n\r\nChallenge Ratings and Hit Points\r\nFor whatever reason, the first thing that strikes me to look at is the monsters’ challenge ratings (CRs). As the name suggests, CRs are an indication of how difficult a monster is for a group of players to fight, with higher CRs corresponding to a more difficult fight. The general rule of thumb is that a party of players can fight monsters about equal to their own level (or lower), and that higher CR monsters could be pretty tough.\r\n\r\n\r\nmons_df %>%\r\n  ggplot(aes(x = challenge_rating)) +\r\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\r\n  labs(\r\n    title = \"Distribution of Challenge Ratings\"\r\n  )\r\n\r\n\r\n\r\n\r\nThe general rule makes sense in conjunction with this distribution of CRs. Most of the action in D&D campaigns tends to occur in lower levels, so it makes sense that the majority of the monsters here have CRs < 10. We do see one dude hanging out at CR 30, though, so let’s see who that is.\r\n\r\n\r\nmons_df %>%\r\n  slice_max(order_by = challenge_rating, n = 1)\r\n\r\n\r\n# A tibble: 1 x 31\r\n  index name  size  type  subtype alignment armor_class hit_points\r\n  <chr> <chr> <chr> <chr> <list>  <chr>           <int>      <int>\r\n1 tarr~ Tarr~ Garg~ mons~ <chr [~ unaligned          25        676\r\n# ... with 23 more variables: hit_dice <chr>, speed <list>,\r\n#   strength <int>, dexterity <int>, constitution <int>,\r\n#   intelligence <int>, wisdom <int>, charisma <int>,\r\n#   proficiencies <list>, damage_vulnerabilities <list>,\r\n#   damage_resistances <list>, damage_immunities <list>,\r\n#   condition_immunities <list>, senses <list>, languages <chr>,\r\n#   challenge_rating <dbl>, xp <int>, special_abilities <list>,\r\n#   actions <list>, legendary_actions <list>, url <chr>,\r\n#   reactions <list>, forms <list>\r\n\r\nRight, so we can see the monster with the highest challenge rating is the Tarrasque. That’s this guy.\r\n\r\nNext, let’s take a look at the distribution of monster hit points. For those unfamiliar with D&D/video games more broadly, hit points represent the amount of health a character/monster has, and reducing someone to 0 hit points will defeat them. Typically (and I’ll explore this relationship more momentarily), hit points will increase as CR increases.\r\n\r\n\r\nmons_df %>%\r\n  ggplot(aes(x = hit_points)) +\r\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\r\n  labs(\r\n    title = \"Distribution of Monster Hit Points\"\r\n  )\r\n\r\n\r\n\r\n\r\nLet’s look a little bit more at descriptives for these previous two stats:\r\n\r\n\r\nsummary(mons_df[c(\"challenge_rating\", \"hit_points\")])\r\n\r\n\r\n challenge_rating   hit_points    \r\n Min.   : 0.000   Min.   :  1.00  \r\n 1st Qu.: 0.500   1st Qu.: 19.00  \r\n Median : 2.000   Median : 45.00  \r\n Mean   : 4.515   Mean   : 81.49  \r\n 3rd Qu.: 6.000   3rd Qu.:114.00  \r\n Max.   :30.000   Max.   :676.00  \r\n\r\nAnd, again, presumably there’s a strong correlation between them, but let’s check that as well.\r\n\r\n\r\ncor(mons_df$hit_points, mons_df$challenge_rating)\r\n\r\n\r\n[1] 0.9414071\r\n\r\nSo, we see a very strong correlation between hit points and challenge rating. Let’s plot this.\r\n\r\n\r\nmons_df %>%\r\n  ggplot(aes(x = hit_points, y = challenge_rating)) +\r\n  geom_point(color = herm) +\r\n  labs(\r\n    title = \"Monster Hit Points vs Challenge Rating\"\r\n  )\r\n\r\n\r\n\r\n\r\nYeah…that’s what we’d expect a strong correlation to look like. One thing to note is that, although it looks like the Tarrasque fits on the general trend line here, outliers can have a strong influence on the correlation coefficient, so I’ll do a quick check to see what the value would be if we didn’t include the Tarrasque.\r\n\r\n\r\nmons_df %>%\r\n  filter(name != \"Tarrasque\") %>%\r\n  select(hit_points, challenge_rating) %>%\r\n  cor() %>%\r\n  .[[1,2]]\r\n\r\n\r\n[1] 0.9409642\r\n\r\nOur correlation coefficient is pretty much identical to the previous, Tarrasque-included calculation, which makes sense given what we see in the scatterplot, but still a reasonable check to include.\r\nAbility Scores\r\nAbility scores are central to D&D and are the thing I’m most interested in looking at here. A quick Google search will tell you all you want to know (& more) about ability scores, but as a quick tl;dr – ability scores represent a character’s (player, monster, or non-player-character) abilities in several different areas, and each does different things. Furthermore, different character classes will value different ability scores. Fighters, for example, will tend to value strength and constitution; rangers and rogues will value dexterity; wizards will value intelligence, etc. Characters’ ability scores affect how well they do in combat, how they cast spells, how well they can persuade others, whether or not they can successfully climb a trees, etc – they affect pretty much anything you want to do in the game. And, due to how ability scores are allocated, characters will not have high scores on every ability, and the differential prioritizations and limited availability makes the distributions (and relationships among the scores) interesting to me.\r\nSo, let’s first check out the distributions.\r\n\r\n\r\nabs <- c(\"strength\", \"charisma\", \"dexterity\", \"intelligence\", \"wisdom\", \"constitution\")\r\n\r\nab_scores <- mons_df %>%\r\n  select(name, all_of(abs)) %>%\r\n  pivot_longer(cols = 2:ncol(.),\r\n               names_to = \"ability\",\r\n               values_to = \"score\")\r\n\r\nab_scores %>%\r\n  ggplot(aes(x = score, y = ability, fill = ability)) +\r\n  geom_density_ridges(alpha = .7) +\r\n  labs(\r\n    title = \"Monster Ability Score Distributions\",\r\n    y = NULL\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt’s interesting to me that the distributions all have different shapes, and especially that wisdom appears to the be only ability score that has a normal distribution for monsters.\r\nAnother interesting question to me is to what extent are these correlated. To check this out, we could use base R’s cor() function, but I want to try the {corrr} package, which provides some helpers for doing correlation analyses.\r\n\r\n\r\nabs_corrs <- mons_df %>%\r\n  select(all_of(abs)) %>%\r\n  correlate() %>%\r\n  rearrange() %>%\r\n  shave()\r\n\r\nabs_corrs %>%\r\n  fashion() %>%\r\n  gt() %>%\r\n  tab_header(\r\n    title = \"Ability Score Correlations\",\r\n    subtitle = \"D&D 5e Monsters\"\r\n  )\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#nyidrkwezl .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#nyidrkwezl .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#nyidrkwezl .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#nyidrkwezl .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#nyidrkwezl .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#nyidrkwezl .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#nyidrkwezl .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#nyidrkwezl .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#nyidrkwezl .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#nyidrkwezl .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#nyidrkwezl .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#nyidrkwezl .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#nyidrkwezl .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#nyidrkwezl .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#nyidrkwezl .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#nyidrkwezl .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#nyidrkwezl .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#nyidrkwezl .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#nyidrkwezl .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#nyidrkwezl .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#nyidrkwezl .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#nyidrkwezl .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#nyidrkwezl .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#nyidrkwezl .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#nyidrkwezl .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#nyidrkwezl .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#nyidrkwezl .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nAbility Score Correlations\r\n    D&D 5e Monsters\r\n    term\r\n      constitution\r\n      strength\r\n      charisma\r\n      intelligence\r\n      wisdom\r\n      dexterity\r\n    constitution\r\n      \r\n      \r\n      \r\n      \r\n      \r\n      \r\n    strength\r\n       .86\r\n      \r\n      \r\n      \r\n      \r\n      \r\n    charisma\r\n       .60\r\n       .52\r\n      \r\n      \r\n      \r\n      \r\n    intelligence\r\n       .51\r\n       .42\r\n       .90\r\n      \r\n      \r\n      \r\n    wisdom\r\n       .45\r\n       .42\r\n       .74\r\n       .65\r\n      \r\n      \r\n    dexterity\r\n      -.19\r\n      -.23\r\n       .21\r\n       .21\r\n       .36\r\n      \r\n    \r\n\r\nWe see some interesting stuff here:\r\nDexterity has weak to moderate correlations with everything.\r\nExcept for dexterity, constitution has moderate to large correlations with everything. This makes sense, since constitution relates to how many hit points creatures have, and so we’d expect cons to increase with level, and so regardless of what a monster’s primary ability is, higher level monsters will likely have high cons scores.\r\nThe strongest correlations are cons:strength and intelligence:charisma, which both seem reasonable. Int/charisma are useful for spellcasting and spell saving throws (and so are likely to travel together in monster stat blocks), and strength/cons are likely going to travel together in the form of big beefy melee combat type monsters.\r\nWe can also look at the same data using the rplot() function in {corrr}, although it’s not super easy to see:\r\n\r\n\r\nabs_corrs %>%\r\n  rplot()\r\n\r\n\r\n\r\n\r\nBeyond looking at the distributions and correlations of these ability scores, I think it’s also worth it to just look at the means. This will give us a sense of what the average monster in D&D is like, stats-wise.\r\n\r\n\r\nab_scores %>%\r\n  group_by(ability) %>%\r\n  summarize(avg = mean(score)) %>%\r\n  mutate(ability = fct_reorder(ability, avg)) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = avg, y = ability)) +\r\n  geom_col(fill = herm) +\r\n  geom_text(aes(label = round(avg, 1), x = avg - .2), hjust = 1, color = \"white\") +\r\n  labs(\r\n    x = \"Average Ability Score\",\r\n    y = NULL,\r\n    title = \"Average Monster Ability Scores in D&D 5e\"\r\n  )\r\n\r\n\r\n\r\n\r\nSo, on average, monsters tend to be stronger and have higher constitutions, but have lower intelligence. Which makes sense if we look at the distributions again.\r\nFinally, we might want to look at which monster scores the highest on each ability score.\r\n\r\n\r\nab_scores %>%\r\n  select(ability, name, score) %>%\r\n  mutate(ability = str_to_title(ability)) %>%\r\n  group_by(ability) %>%\r\n  slice_max(order_by = score, n = 1) %>%\r\n  ungroup() %>%\r\n  gt() %>%\r\n  tab_header(\r\n    title = \"Highest Ability Scores\",\r\n    subtitle = \"...and the monsters that own them\"\r\n  ) %>%\r\n  cols_label(\r\n    name = \"Monster\",\r\n    ability = \"Ability\",\r\n    score = \"Score\"\r\n  ) %>%\r\n  tab_style(\r\n    style = cell_text(weight = \"bold\"),\r\n    locations = cells_column_labels(columns = everything())\r\n  )\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#caajeoqkgq .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#caajeoqkgq .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#caajeoqkgq .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#caajeoqkgq .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#caajeoqkgq .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#caajeoqkgq .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#caajeoqkgq .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#caajeoqkgq .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#caajeoqkgq .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#caajeoqkgq .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#caajeoqkgq .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#caajeoqkgq .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#caajeoqkgq .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#caajeoqkgq .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#caajeoqkgq .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#caajeoqkgq .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#caajeoqkgq .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#caajeoqkgq .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#caajeoqkgq .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#caajeoqkgq .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#caajeoqkgq .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#caajeoqkgq .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#caajeoqkgq .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#caajeoqkgq .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#caajeoqkgq .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#caajeoqkgq .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#caajeoqkgq .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nHighest Ability Scores\r\n    ...and the monsters that own them\r\n    Ability\r\n      Monster\r\n      Score\r\n    Charisma\r\n      Solar\r\n      30\r\n    Constitution\r\n      Tarrasque\r\n      30\r\n    Dexterity\r\n      Will-o'-Wisp\r\n      28\r\n    Intelligence\r\n      Solar\r\n      25\r\n    Strength\r\n      Ancient Gold Dragon\r\n      30\r\n    Strength\r\n      Ancient Red Dragon\r\n      30\r\n    Strength\r\n      Ancient Silver Dragon\r\n      30\r\n    Strength\r\n      Kraken\r\n      30\r\n    Strength\r\n      Tarrasque\r\n      30\r\n    Wisdom\r\n      Solar\r\n      25\r\n    \r\n\r\nOne thing to keep in mind is that ability scores are capped at 30, so we see a handful of abilities that, according to the game rules, cannot be any higher (cons, strength, and charisma). We also see multiple monsters hitting the strength cap, including our old friend the Tarrasque. And we see the Solar represented in 3 categories (charisma, int, wisdom). I wasn’t familiar with the Solar before making this table, so I looked it up, and it seems like a sword-welding angel, which is pretty cool.\r\n\r\nOther Questions\r\nI’ll cap this post off by looking at a few odds and ends. This one might be a little silly, but since the game is called Dungeons and Dragons, it might be relevant to see how many monsters are actually dragons.\r\n\r\n\r\nmons_df %>%\r\n  mutate(is_dragon = if_else(type == \"dragon\", \"Dragon\", \"Not Dragon\")) %>%\r\n  count(is_dragon) %>%\r\n  ggplot(aes(x = n, y = is_dragon)) +\r\n  geom_col(fill = herm) +\r\n  geom_text(aes(label = n, x = n - 3), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n  labs(\r\n    title = \"More Dungeons than Dragons\",\r\n    subtitle = \"Most monsters in D&D 5e are not dragons\",\r\n    y = NULL,\r\n    x = \"Monster Count\"\r\n  )\r\n\r\n\r\n\r\n\r\nGiven the above, we might be interested in seeing which monster types are the most common:\r\n\r\n\r\nmons_df %>%\r\n  count(type) %>%\r\n  ggplot(aes(x = n, y = fct_reorder(type, n))) +\r\n  geom_col(fill = herm) +\r\n  geom_text(aes(label = n, x = n - 1), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n  labs(\r\n    title = \"Most Common Monster Types\",\r\n    subtitle = \"D&D 5e\",\r\n    y = NULL,\r\n    x = \"Monster Count\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see here that beasts are the most common monster type, followed by humanoids and then dragons. I’m not really sure what’s going on with the “swarm of tiny beasts,” so let’s take a peek at those.\r\n\r\n\r\nmons_df %>%\r\n  filter(str_detect(type, \"swarm\")) %>%\r\n  pull(name)\r\n\r\n\r\n [1] \"Swarm of Bats\"             \"Swarm of Beetles\"         \r\n [3] \"Swarm of Centipedes\"       \"Swarm of Insects\"         \r\n [5] \"Swarm of Poisonous Snakes\" \"Swarm of Quippers\"        \r\n [7] \"Swarm of Rats\"             \"Swarm of Ravens\"          \r\n [9] \"Swarm of Spiders\"          \"Swarm of Wasps\"           \r\n\r\nI suppose that makes sense – there are stat blocks for swarms of bats, spiders, etc.\r\nThe last thing I’m going to check out here is the experience points (xp) distributions. My sense is that it’ll look similar to the distributions for challenge rating and hit points, since tougher monsters will award more experience for beating them. But still worth checking out.\r\n\r\n\r\nmons_df %>%\r\n  ggplot(aes(x = xp)) +\r\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\r\n  labs(\r\n    title = \"Distribution of XP from Monsters\"\r\n  )\r\n\r\n\r\n\r\n\r\nSo this plot isn’t great since the Tarrasque is worth over 150k xp, which makes it harder to see the distribution at the lower end. There are also a few monsters that award ~50k xp, which again also makes it difficult to distinguish values at the lower end. I’ll filter put this on a log scale to make it easier to see the lower end.\r\n\r\n\r\nmons_df %>%\r\n  ggplot(aes(x = xp)) +\r\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\r\n  labs(\r\n    title = \"Distribution of XP from Monsters\"\r\n  ) +\r\n  scale_x_log10()\r\n\r\n\r\n\r\n\r\nThis is a lot easier to read – we can see that the modal value for xp is maybe 500ish, although it’s also quite common for monsters to award 100xp or less. Monsters that award over 10k xp are pretty rare.\r\nAnd that’ll be it for this one. In the next blog post in this series, I’ll likely do some sort of clustering – probably latent profile analysis because I want to brush back up on it – to examine different groups of monsters based on their ability scores.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-11-dungeons-and-dragons-part-2/dungeons-and-dragons-part-2_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T08:24:31-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-11-dungeons-and-dragons-part-1/",
    "title": "Dungeons and Dragons - Part 1",
    "description": "Wrangling JSON data from an API.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-12-18",
    "categories": [],
    "contents": "\r\nI’ve been playing Dungeons and Dragons 5th edition (D&D 5e) for a few years now and really enjoy it, although COVID has really hindered my opportunity to play. That said, I recently discovered a D&D 5e API, so I figured I’d do a series of blog posts analyzing D&D data from this API. In this first post, I wanted to do a quick walkthrough of how to get data from this API using R and wrangling it into a structure that’s more or less conducive to later analysis. In later posts, I’ll explore the data and then get into some modeling.\r\nAs something of an aside – the API has data for character classes, spells, races, monsters, etc. I’m mostly going to focus on the monsters data, but might use some of the other data later on.\r\nSetup\r\nFirst, I’ll load the packages I need to get and wrangle the data, which is really just {tidyverse}, {jsonlite} and good old base R. I’m also adding in the base URL of the API.\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(jsonlite)\r\n\r\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n\r\n\r\n\r\nFetching Data\r\nSo, the first step here is to actually get the data from the API. Let’s walk through the process here, illustrating this with a single monster (the aboleth) and then applying the process to all of the monsters.\r\nWe’ll use the fromJSON() function to get JSON data from the API. We’ll see that this gives us a pretty gnarly nested list.\r\n\r\n\r\nexample <- fromJSON(paste0(dnd_base, \"aboleth\"))\r\n\r\nglimpse(example)\r\n\r\n\r\nList of 29\r\n $ index                 : chr \"aboleth\"\r\n $ name                  : chr \"Aboleth\"\r\n $ size                  : chr \"Large\"\r\n $ type                  : chr \"aberration\"\r\n $ subtype               : NULL\r\n $ alignment             : chr \"lawful evil\"\r\n $ armor_class           : int 17\r\n $ hit_points            : int 135\r\n $ hit_dice              : chr \"18d10\"\r\n $ speed                 :List of 2\r\n  ..$ walk: chr \"10 ft.\"\r\n  ..$ swim: chr \"40 ft.\"\r\n $ strength              : int 21\r\n $ dexterity             : int 9\r\n $ constitution          : int 15\r\n $ intelligence          : int 18\r\n $ wisdom                : int 15\r\n $ charisma              : int 18\r\n $ proficiencies         :'data.frame': 5 obs. of  2 variables:\r\n  ..$ value      : int [1:5] 6 8 6 12 10\r\n  ..$ proficiency:'data.frame': 5 obs. of  3 variables:\r\n  .. ..$ index: chr [1:5] \"saving-throw-con\" \"saving-throw-int\" \"saving-throw-wis\" \"skill-history\" ...\r\n  .. ..$ name : chr [1:5] \"Saving Throw: CON\" \"Saving Throw: INT\" \"Saving Throw: WIS\" \"Skill: History\" ...\r\n  .. ..$ url  : chr [1:5] \"/api/proficiencies/saving-throw-con\" \"/api/proficiencies/saving-throw-int\" \"/api/proficiencies/saving-throw-wis\" \"/api/proficiencies/skill-history\" ...\r\n $ damage_vulnerabilities: list()\r\n $ damage_resistances    : list()\r\n $ damage_immunities     : list()\r\n $ condition_immunities  : list()\r\n $ senses                :List of 2\r\n  ..$ darkvision        : chr \"120 ft.\"\r\n  ..$ passive_perception: int 20\r\n $ languages             : chr \"Deep Speech, telepathy 120 ft.\"\r\n $ challenge_rating      : int 10\r\n $ xp                    : int 5900\r\n $ special_abilities     :'data.frame': 3 obs. of  3 variables:\r\n  ..$ name: chr [1:3] \"Amphibious\" \"Mucous Cloud\" \"Probing Telepathy\"\r\n  ..$ desc: chr [1:3] \"The aboleth can breathe air and water.\" \"While underwater, the aboleth is surrounded by transformative mucus. A creature that touches the aboleth or tha\"| __truncated__ \"If a creature communicates telepathically with the aboleth, the aboleth learns the creature's greatest desires \"| __truncated__\r\n  ..$ dc  :'data.frame':    3 obs. of  3 variables:\r\n  .. ..$ dc_type     :'data.frame': 3 obs. of  3 variables:\r\n  .. ..$ dc_value    : int [1:3] NA 14 NA\r\n  .. ..$ success_type: chr [1:3] NA \"none\" NA\r\n $ actions               :'data.frame': 4 obs. of  7 variables:\r\n  ..$ name        : chr [1:4] \"Multiattack\" \"Tentacle\" \"Tail\" \"Enslave\"\r\n  ..$ desc        : chr [1:4] \"The aboleth makes three tentacle attacks.\" \"Melee Weapon Attack: +9 to hit, reach 10 ft., one target. Hit: 12 (2d6 + 5) bludgeoning damage. If the target i\"| __truncated__ \"Melee Weapon Attack: +9 to hit, reach 10 ft. one target. Hit: 15 (3d6 + 5) bludgeoning damage.\" \"The aboleth targets one creature it can see within 30 ft. of it. The target must succeed on a DC 14 Wisdom savi\"| __truncated__\r\n  ..$ options     :'data.frame':    4 obs. of  2 variables:\r\n  .. ..$ choose: int [1:4] 1 NA NA NA\r\n  .. ..$ from  :List of 4\r\n  ..$ damage      :List of 4\r\n  .. ..$ :'data.frame': 0 obs. of  0 variables\r\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\r\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\r\n  .. ..$ :'data.frame': 0 obs. of  0 variables\r\n  ..$ attack_bonus: int [1:4] NA 9 9 NA\r\n  ..$ dc          :'data.frame':    4 obs. of  3 variables:\r\n  .. ..$ dc_type     :'data.frame': 4 obs. of  3 variables:\r\n  .. ..$ dc_value    : int [1:4] NA 14 NA 14\r\n  .. ..$ success_type: chr [1:4] NA \"none\" NA \"none\"\r\n  ..$ usage       :'data.frame':    4 obs. of  2 variables:\r\n  .. ..$ type : chr [1:4] NA NA NA \"per day\"\r\n  .. ..$ times: int [1:4] NA NA NA 3\r\n $ legendary_actions     :'data.frame': 3 obs. of  4 variables:\r\n  ..$ name        : chr [1:3] \"Detect\" \"Tail Swipe\" \"Psychic Drain (Costs 2 Actions)\"\r\n  ..$ desc        : chr [1:3] \"The aboleth makes a Wisdom (Perception) check.\" \"The aboleth makes one tail attack.\" \"One creature charmed by the aboleth takes 10 (3d6) psychic damage, and the aboleth regains hit points equal to \"| __truncated__\r\n  ..$ attack_bonus: int [1:3] NA NA 0\r\n  ..$ damage      :List of 3\r\n  .. ..$ : NULL\r\n  .. ..$ : NULL\r\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\r\n $ url                   : chr \"/api/monsters/aboleth\"\r\n\r\nTo clean this list up a bit, we’ll use the enframe() function (from {tibble}) to convert the lists into a dataframe and then the pivot_wider() function to reshape this into a single-row tibble.\r\n\r\n\r\nexample %>%\r\n  enframe() %>%\r\n  pivot_wider(names_from = name,\r\n              values_from = value) %>%\r\n  glimpse()\r\n\r\n\r\nRows: 1\r\nColumns: 29\r\n$ index                  <list> [\"aboleth\"]\r\n$ name                   <list> [\"Aboleth\"]\r\n$ size                   <list> [\"Large\"]\r\n$ type                   <list> [\"aberration\"]\r\n$ subtype                <list> [NULL]\r\n$ alignment              <list> [\"lawful evil\"]\r\n$ armor_class            <list> [17]\r\n$ hit_points             <list> [135]\r\n$ hit_dice               <list> [\"18d10\"]\r\n$ speed                  <list> [[\"10 ft.\", \"40 ft.\"]]\r\n$ strength               <list> [21]\r\n$ dexterity              <list> [9]\r\n$ constitution           <list> [15]\r\n$ intelligence           <list> [18]\r\n$ wisdom                 <list> [15]\r\n$ charisma               <list> [18]\r\n$ proficiencies          <list> [<data.frame[5 x 2]>]\r\n$ damage_vulnerabilities <list> [[]]\r\n$ damage_resistances     <list> [[]]\r\n$ damage_immunities      <list> [[]]\r\n$ condition_immunities   <list> [[]]\r\n$ senses                 <list> [[\"120 ft.\", 20]]\r\n$ languages              <list> [\"Deep Speech, telepathy 120 ft.\"]\r\n$ challenge_rating       <list> [10]\r\n$ xp                     <list> [5900]\r\n$ special_abilities      <list> [<data.frame[3 x 3]>]\r\n$ actions                <list> [<data.frame[4 x 7]>]\r\n$ legendary_actions      <list> [<data.frame[3 x 4]>]\r\n$ url                    <list> [\"/api/monsters/aboleth\"]\r\n\r\nGreat. This is more or less the structure we want. You might notice that all of our columns are lists rather than atomic vectors – we’ll deal with that later once we get all of the data.\r\nNow that we know the basic process, we’ll just apply this to all of the monsters with data available through the API. To do that, I’ll write a function that executes the previous steps, get a list of all of the monsters available in the API, use map() to iterate the “fetch” function for each monster, and then bind all of the resulting rows together.\r\n\r\n\r\nfetch_monster <- function(monster) {\r\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\r\n  \r\n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\r\n    enframe() %>%\r\n    pivot_wider(names_from = name,\r\n                values_from = value)\r\n  \r\n  return(ret)\r\n}\r\n\r\n#this gets all of the monster indices to plug into the fetch function\r\nmons <- fromJSON(dnd_base)$results %>%\r\n  pull(index)\r\n\r\nmonster_lists <- map(mons, fetch_monster)\r\n\r\nmons_bind <- bind_rows(monster_lists)\r\n\r\nglimpse(mons_bind)\r\n\r\n\r\nRows: 332\r\nColumns: 31\r\n$ index                  <list> [\"aboleth\", \"acolyte\", \"adult-bla...\r\n$ name                   <list> [\"Aboleth\", \"Acolyte\", \"Adult Bla...\r\n$ size                   <list> [\"Large\", \"Medium\", \"Huge\", \"Huge...\r\n$ type                   <list> [\"aberration\", \"humanoid\", \"drago...\r\n$ subtype                <list> [NULL, \"any race\", NULL, NULL, NU...\r\n$ alignment              <list> [\"lawful evil\", \"any alignment\", ...\r\n$ armor_class            <list> [17, 10, 19, 19, 18, 19, 18, 19, ...\r\n$ hit_points             <list> [135, 9, 195, 225, 172, 212, 184,...\r\n$ hit_dice               <list> [\"18d10\", \"2d8\", \"17d12\", \"18d12\"...\r\n$ speed                  <list> [[\"10 ft.\", \"40 ft.\"], [\"30 ft.\"]...\r\n$ strength               <list> [21, 10, 23, 25, 23, 25, 23, 27, ...\r\n$ dexterity              <list> [9, 10, 14, 10, 10, 10, 12, 14, 1...\r\n$ constitution           <list> [15, 10, 21, 23, 21, 23, 21, 25, ...\r\n$ intelligence           <list> [18, 10, 14, 16, 14, 16, 18, 16, ...\r\n$ wisdom                 <list> [15, 14, 13, 15, 13, 15, 15, 15, ...\r\n$ charisma               <list> [18, 11, 17, 19, 17, 19, 17, 24, ...\r\n$ proficiencies          <list> [<data.frame[5 x 2]>, <data.frame...\r\n$ damage_vulnerabilities <list> [[], [], [], [], [], [], [], [], ...\r\n$ damage_resistances     <list> [[], [], [], [], [], [], [], [], ...\r\n$ damage_immunities      <list> [[], [], \"acid\", \"lightning\", \"fi...\r\n$ condition_immunities   <list> [[], [], [], [], [], [], [], [], ...\r\n$ senses                 <list> [[\"120 ft.\", 20], [12], [\"60 ft.\"...\r\n$ languages              <list> [\"Deep Speech, telepathy 120 ft.\"...\r\n$ challenge_rating       <list> [10, 0.25, 14, 16, 13, 15, 14, 17...\r\n$ xp                     <list> [5900, 50, 11500, 15000, 10000, 1...\r\n$ special_abilities      <list> [<data.frame[3 x 3]>, <data.frame...\r\n$ actions                <list> [<data.frame[4 x 7]>, <data.frame...\r\n$ legendary_actions      <list> [<data.frame[3 x 4]>, NULL, <data...\r\n$ url                    <list> [\"/api/monsters/aboleth\", \"/api/m...\r\n$ reactions              <list> [NULL, NULL, NULL, NULL, NULL, NU...\r\n$ forms                  <list> [NULL, NULL, NULL, NULL, NULL, NU...\r\n\r\nNotice that we have the same structure as in the previous example, but now with 322 rows instead of 1. Now we can take care of coercing some of these list columns into atomic vectors.\r\nRestructuring Data\r\nOne problem here, though, is that the possible variable values for each column differ depending on the monster (for some variables). Variables like strength, hit points, challenge rating, and xp will always be a single integer value, but variables like legendary_actions can differ greatly. People who play D&D will know that normal monsters don’t have any legendary actions, and so this will be NULL for those monsters. But some monsters might have 1 or 2 legendary actions, whereas big baddies like ancient dragons can have several. This same varying structure applies to columns like proficiencies, special abilities, reactions, etc. Ultimately, this means that a list column is probably the best way to represent this type of data, since lists are more flexible, whereas some columns can be represented as an atomic vector, and so we need to figure out how to address this.\r\nTo do this, we can write a couple of functions. The first, compare_lens() (below), will determine if the length of each element of a list is equal to whatever size we want to compare against (I’ve set the default to 1, which is what we want to use in this case). It then uses the all() function to determine if all of these comparisons are equal to TRUE, and will return a single value of TRUE if this is the case (and a single FALSE if not).\r\n\r\n\r\ncompare_lens <- function(x, size = 1) {\r\n  all(map_lgl(x, ~length(unlist(.x)) == size))\r\n}\r\n\r\n\r\n\r\nNext, we’ll use the compare_lens() function as the test expression in another function, cond_unlist (or conditionally unlist), below. The idea here is if compare_lens() is TRUE, then we will unlist the list (simplify it to a vector) passed to the function; otherwise, we’ll leave it as is (as a list). Putting these functions together, the logic is:\r\nDetermine if all elements of a list have a length equal to 1.\r\nIf so, turn that list into a vector.\r\nIf not, leave it as a list.\r\n\r\n\r\ncond_unlist <- function(x) {\r\n  if (compare_lens(x) == TRUE) {\r\n    unlist(x)\r\n  } else {\r\n    x\r\n  }\r\n}\r\n\r\n\r\n\r\nThe final step is to apply this function to all of the columns (which, recall, are lists) in our mons_bind tibble. We can do this using a combination of mutate() and across(). After doing this, we’ll see that some of the columns in our data frame have been simplified to character, integer, and double vectors, whereas others remain lists (lists of lists, lists of data frames).\r\n\r\n\r\nmons_df <- mons_bind %>%\r\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\r\n\r\nglimpse(mons_df)\r\n\r\n\r\nRows: 332\r\nColumns: 31\r\n$ index                  <chr> \"aboleth\", \"acolyte\", \"adult-black...\r\n$ name                   <chr> \"Aboleth\", \"Acolyte\", \"Adult Black...\r\n$ size                   <chr> \"Large\", \"Medium\", \"Huge\", \"Huge\",...\r\n$ type                   <chr> \"aberration\", \"humanoid\", \"dragon\"...\r\n$ subtype                <list> [NULL, \"any race\", NULL, NULL, NU...\r\n$ alignment              <chr> \"lawful evil\", \"any alignment\", \"c...\r\n$ armor_class            <int> 17, 10, 19, 19, 18, 19, 18, 19, 19...\r\n$ hit_points             <int> 135, 9, 195, 225, 172, 212, 184, 2...\r\n$ hit_dice               <chr> \"18d10\", \"2d8\", \"17d12\", \"18d12\", ...\r\n$ speed                  <list> [[\"10 ft.\", \"40 ft.\"], [\"30 ft.\"]...\r\n$ strength               <int> 21, 10, 23, 25, 23, 25, 23, 27, 23...\r\n$ dexterity              <int> 9, 10, 14, 10, 10, 10, 12, 14, 12,...\r\n$ constitution           <int> 15, 10, 21, 23, 21, 23, 21, 25, 21...\r\n$ intelligence           <int> 18, 10, 14, 16, 14, 16, 18, 16, 18...\r\n$ wisdom                 <int> 15, 14, 13, 15, 13, 15, 15, 15, 15...\r\n$ charisma               <int> 18, 11, 17, 19, 17, 19, 17, 24, 17...\r\n$ proficiencies          <list> [<data.frame[5 x 2]>, <data.frame...\r\n$ damage_vulnerabilities <list> [[], [], [], [], [], [], [], [], ...\r\n$ damage_resistances     <list> [[], [], [], [], [], [], [], [], ...\r\n$ damage_immunities      <list> [[], [], \"acid\", \"lightning\", \"fi...\r\n$ condition_immunities   <list> [[], [], [], [], [], [], [], [], ...\r\n$ senses                 <list> [[\"120 ft.\", 20], [12], [\"60 ft.\"...\r\n$ languages              <chr> \"Deep Speech, telepathy 120 ft.\", ...\r\n$ challenge_rating       <dbl> 10.00, 0.25, 14.00, 16.00, 13.00, ...\r\n$ xp                     <int> 5900, 50, 11500, 15000, 10000, 130...\r\n$ special_abilities      <list> [<data.frame[3 x 3]>, <data.frame...\r\n$ actions                <list> [<data.frame[4 x 7]>, <data.frame...\r\n$ legendary_actions      <list> [<data.frame[3 x 4]>, NULL, <data...\r\n$ url                    <chr> \"/api/monsters/aboleth\", \"/api/mon...\r\n$ reactions              <list> [NULL, NULL, NULL, NULL, NULL, NU...\r\n$ forms                  <list> [NULL, NULL, NULL, NULL, NULL, NU...\r\n\r\nAnd there we have it. Our data is now in a pretty good state for some analysis. Depending on what we’re interested in doing, we could also do some additional feature engineering on the list columns, but the choices there will be contingent on the analyses we want to do.\r\nFor my next blog in this series, I’ll use this data to do some exploratory analysis, which I hope to get to in the next week or so.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T08:24:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-predicting-mobile-phone-subscription-growth/",
    "title": "Predicting Mobile Phone Subscription Growth",
    "description": "Using splines and iteration via map() to fit and interrogate models.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-11-18",
    "categories": [],
    "contents": "\r\nI’ll keep the intro short and sweet for this one. A few weeks ago, I watched this screencast from Julia Silge in which she used splines to model the relationship between wins and seed in the Women’s NCAA Tournament. I don’t have a ton of experience using splines, and so that screencast made me want to learn a bit more and practice using them myself. Lo and behold, the phone subscription data from week 46 (2020) of #TidyTuesday seemed like a pretty good opportunity, so that’s what I’m doing here. More specifically, I’m using splines to model the relationship between year and the number of mobile phone subscriptions per 100 people across different continents and then investigating which countries these models perform best and worst for.\r\nLet’s get right into it, then.\r\nSetup\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\r\nlibrary(tidyverse)\r\nlibrary(eemisc)\r\nlibrary(harrypotter)\r\nlibrary(splines)\r\nlibrary(tidymodels)\r\nlibrary(patchwork)\r\nlibrary(kableExtra)\r\n\r\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\r\n\r\nopts <- options(\r\n  ggplot2.discrete.fill = list(\r\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\r\n    harrypotter::hp(n = 5, option = \"Always\")\r\n  )\r\n)\r\n\r\ntheme_set(theme_ee())\r\n\r\nmobile <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-10/mobile.csv')\r\n\r\n\r\n\r\nVery Brief Exploration\r\nSo, I kinda already know what I want to do, so I’m going to keep the exploration pretty minimal here. I will plot the number of mobile phone subscriptions for each country over time, though, just so I can get some feel for the relationships.\r\n\r\n\r\nmobile %>%\r\n  ggplot(aes(x = year, y = mobile_subs, group = entity)) +\r\n  geom_line(alpha = .7, color = herm) +\r\n  facet_wrap(~ continent)\r\n\r\n\r\n\r\n\r\nThe most apparent takeaway here to me is that the relationship between year and mobile phone subscriptions is not linear – it looks sigmoidal to me. It also seems to differ by continent (although that may just be an artifact of faceting the continents).\r\nEither way, I got into this to do some splines, so that’s what I’m going to do. First, let’s do a very brief and not statistically rigorous overview of what a spline is (n.b. for a better summary, go here). At a very high level, splines allow us to estimate flexible models to data. They do this by allowing us to include “knots” in our regression models and fitting smooth functions that model the data between consecutive knots. The person building the model can specify the number of knots (or degrees of freedom) they want to include in the model. Including more knots makes the function more flexible (but also, maybe obviously, increases model complexity), whereas including fewer knots makes the model simpler but less flexible.\r\nLet’s try plotting a few different splines to the full dataset here to illustrate this.\r\n\r\n\r\nplot_spline <- function(df) {\r\n  ggplot(mobile, aes(x = year, y = mobile_subs)) +\r\n    geom_smooth(\r\n      method = lm,\r\n      se = FALSE,\r\n      formula = y ~ ns(x, df = df),\r\n      color = herm\r\n    ) +\r\n    labs(\r\n      title = glue::glue(\"{ df } degrees of freedom\")\r\n    ) +\r\n    theme_minimal()\r\n}\r\n\r\nplots_list <- map(c(2, 3, 4, 6, 8, 10), plot_spline)\r\n\r\nwrap_plots(plots_list)\r\n\r\n\r\n\r\n\r\nNote that the number of knots is equal to 1 - the degrees of freedom. Anyway – looking at this, we can see noticeable differences between the models wtih 2, 3, and 4 degrees of freedom, but very little difference once we get beyond that, which makes me think that a 4 df spline is the way to go. I could more rigorously tune the degrees of freedom by fitting models with each and comparing the accuracy on holdout data, but visually examining it feels good enough here.\r\nFitting Models\r\nNow, let’s fit a spline model for each continent. To do this, I’m going to first filter down to only the countries that have mobile phone data for every year in the dataset (1990 - 2017). Next, I’m going to create a nested tibble for each continent using the nest() function, fit a spline for each continent using a combination of map(), lm(), and ns() (which is a function for natural splines). Finally, I’m going to use glance() from the {broom} package to get the R-squared for each continent’s model. I like this workflow for fitting multiple models because it keeps everything together in a tibble and really facilitates iterating with map().\r\n\r\n\r\ncomplete_countries <- mobile %>%\r\n  group_by(entity) %>%\r\n  summarize(miss = sum(is.na(mobile_subs))) %>%\r\n  filter(miss == 0) %>%\r\n  pull(entity)\r\n\r\ncontinent_df <- mobile %>%\r\n  select(continent, entity, year, mobile_subs) %>%\r\n  filter(entity %in% complete_countries) %>%\r\n  group_by(continent) %>%\r\n  nest() %>%\r\n  mutate(model = map(data, ~lm(mobile_subs ~ ns(year, df = 4), data = .x)),\r\n         rsq = map_dbl(model, ~glance(.x) %>% pull(1)))\r\n\r\ncontinent_df\r\n\r\n\r\n# A tibble: 5 x 4\r\n# Groups:   continent [5]\r\n  continent data                 model    rsq\r\n  <chr>     <list>               <list> <dbl>\r\n1 Asia      <tibble [1,202 x 3]> <lm>   0.689\r\n2 Europe    <tibble [1,119 x 3]> <lm>   0.869\r\n3 Africa    <tibble [1,146 x 3]> <lm>   0.684\r\n4 Americas  <tibble [846 x 3]>   <lm>   0.822\r\n5 Oceania   <tibble [221 x 3]>   <lm>   0.628\r\n\r\nSo, the R-squared values here seem pretty good considering we’re only using year as a predictor. In Europe we’re getting .86, which seems very high, and suggests that most countries follow similar trajectories (which we can see in the very first plot above). What could be interesting, though, is to see which country in each continent this model performs best on and which it performs worst on. This will give us a sense of what the most “typical” country is (the country that most closely follows the overall continent model) and what the most atypical country is (the country that least closely follows the overall continent model) in each continent.\r\nExamining Predictions & Accuracy\r\nTo do this, I’m first going to predict values for each observation (each year for each country) using augment(), again from {broom}. I’m then going to do a little bit of binding and straightening up, ending by unnesting the data.\r\n\r\n\r\npreds_data <- continent_df %>%\r\n  ungroup() %>%\r\n  mutate(preds = map(model, ~augment(.x) %>% select(.fitted)),\r\n         joined_data = map2(data, preds, bind_cols)) %>%\r\n    select(joined_data, \r\n         continent_rsq = rsq,\r\n         continent) %>%\r\n    unnest(joined_data)\r\n\r\n\r\n\r\nNext, I’m going to calculate the R-squared for each country. There’s probably another way to do this, but it’s a pretty easy calculation to do by hand, so I’m just going to do that.\r\n\r\n\r\nrsq_df <- preds_data %>%\r\n  group_by(entity) %>%\r\n  mutate(avg = mean(mobile_subs),\r\n         res = (mobile_subs - .fitted)^2,\r\n         tot = (mobile_subs - .fitted)^2 + (.fitted - avg)^2) %>%\r\n  summarize(country_rsq = 1 - (sum(res)/sum(tot))) %>%\r\n  ungroup() %>%\r\n  left_join(x = preds_data %>% distinct(entity, continent), y = ., by = \"entity\")\r\n\r\n\r\n\r\nNow, I’m going to pick filter down to the countries in each continent that have the highest R-squared (the country the model performs best on) and the lowest R-squared (the country the model performs worst on). One thing to note is that a low R-squared doesn’t mean the country has few mobile phone subscriptions, it just means that the model does a relatively bad job predicting the mobile phone subscriptions for that country. This could be for a number of reasons, only one of which is that the country has considerably fewer subscriptions each year.\r\n\r\n\r\nselected_rsq <- rsq_df %>%\r\n  group_by(continent) %>%\r\n  filter(country_rsq == max(country_rsq) | country_rsq == min(country_rsq)) %>%\r\n  mutate(type = if_else(country_rsq == max(country_rsq), \"best fit\", \"worst fit\")) %>%\r\n  select(continent, entity, country_rsq, type) %>%\r\n  arrange(continent, country_rsq)\r\n\r\nselected_rsq %>%\r\n  select(-type) %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\"))\r\n\r\n\r\n\r\ncontinent\r\n\r\n\r\nentity\r\n\r\n\r\ncountry_rsq\r\n\r\n\r\nAfrica\r\n\r\n\r\nSeychelles\r\n\r\n\r\n0.5109688\r\n\r\n\r\nAfrica\r\n\r\n\r\nKenya\r\n\r\n\r\n0.9895082\r\n\r\n\r\nAmericas\r\n\r\n\r\nCuracao\r\n\r\n\r\n0.4431118\r\n\r\n\r\nAmericas\r\n\r\n\r\nParaguay\r\n\r\n\r\n0.9864706\r\n\r\n\r\nAsia\r\n\r\n\r\nHong Kong\r\n\r\n\r\n0.5080236\r\n\r\n\r\nAsia\r\n\r\n\r\nPhilippines\r\n\r\n\r\n0.9916221\r\n\r\n\r\nEurope\r\n\r\n\r\nMoldova\r\n\r\n\r\n0.6943629\r\n\r\n\r\nEurope\r\n\r\n\r\nFaeroe Islands\r\n\r\n\r\n0.9868045\r\n\r\n\r\nOceania\r\n\r\n\r\nKiribati\r\n\r\n\r\n0.5775901\r\n\r\n\r\nOceania\r\n\r\n\r\nFrench Polynesia\r\n\r\n\r\n0.9451227\r\n\r\n\r\nRight, so, for example, we can see that the model fit using Europe’s data does the worst job predicting for Moldova and the best job for the Faeroe Islands.\r\nFinally, let’s take a look at these best- and worst-fitting countries graphically\r\n\r\n\r\nuse_countries <- pull(selected_rsq, entity)\r\n\r\nmobile_small_joined <- mobile %>%\r\n  filter(entity %in% use_countries) %>%\r\n  left_join(selected_rsq, by = c(\"entity\", \"continent\"))\r\n\r\nlabel_df <- mobile_small_joined %>%\r\n  group_by(entity) %>%\r\n  filter(mobile_subs == max(mobile_subs)) %>%\r\n  ungroup()\r\n  \r\nmobile %>%\r\n  filter(entity %in% use_countries) %>%\r\n  left_join(selected_rsq, by = c(\"entity\", \"continent\")) %>%\r\n  ggplot(aes(x = year, y = mobile_subs, group = entity, color = type)) +\r\n  geom_line() +\r\n  geom_text(data = label_df, aes(label = entity), x = max(mobile$year), hjust = 1, fontface = \"bold\", show.legend = FALSE) +\r\n  facet_wrap(~ continent) +\r\n  scale_color_hp_d(option = \"HermioneGranger\",\r\n                   name = \"Model Fit\") +\r\n  labs(\r\n    title = \"Best and Worst Fitting Models\"\r\n  )\r\n\r\n\r\n\r\n\r\nLet’s take a look at Hong Kong. It has the worst fit for the Asian model, but that’s because Hong Kong has way more mobile phones per person than other countries in Asia. On the other hand, we see that Kiribati (in Oceania) has way fewer than the model would predict. Both have (relatively) poor accuracy.\r\nOkie dokie, I think that’s it for now. Hopefully this is helpful for others in dipping your toes into spline models but also for demonstrating a workflow for nesting data and using map() along with some other functions to fit and interrogate multiple models.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-11-predicting-mobile-phone-subscription-growth/predicting-mobile-phone-subscription-growth_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T08:24:32-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-11-rstudio-table-contest-submission/",
    "title": "Rstudio Table Contest Submission",
    "description": "And impressions of the {gt} package.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-10-26",
    "categories": [],
    "contents": "\r\nis is going to be a pretty short post. After seeing the Rstudio Table Contest announced a few weeks ago, I decided that I wanted to submit something to give myself more of a reason to practice with the {gt} package. I (somewhat arbitrarily) chose to use some longitudinal Broadway data posted earlier in the year as part of #TidyTuesday that I thought would lend itself well to a table.\r\nAnyway, below is my submission to the contest, and below that are some initial impressions of the {gt} package (tl;dr – it’s pretty awesome).\r\nTable\r\n\r\n@import url(\"https://fonts.googleapis.com/css2?family=Lobster:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\r\n@import url(\"https://fonts.googleapis.com/css2?family=Lobster:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\r\n@import url(\"https://fonts.googleapis.com/css2?family=Rubik:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\r\nhtml {\r\n  font-family: Rubik, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#ilhvcxipab .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #FFFFFF;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #474747;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_heading {\r\n  background-color: #474747;\r\n  text-align: center;\r\n  border-bottom-color: #474747;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_title {\r\n  color: #FFFFFF;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #474747;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#ilhvcxipab .gt_subtitle {\r\n  color: #FFFFFF;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #474747;\r\n  border-top-width: 0;\r\n}\r\n\r\n#ilhvcxipab .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_col_heading {\r\n  color: #FFFFFF;\r\n  background-color: #373737;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#ilhvcxipab .gt_column_spanner_outer {\r\n  color: #FFFFFF;\r\n  background-color: #373737;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#ilhvcxipab .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#ilhvcxipab .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#ilhvcxipab .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#ilhvcxipab .gt_group_heading {\r\n  padding: 8px;\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#ilhvcxipab .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#ilhvcxipab .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#ilhvcxipab .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#ilhvcxipab .gt_row {\r\n  padding-top: -30px;\r\n  padding-bottom: -30px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#ilhvcxipab .gt_stub {\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#ilhvcxipab .gt_summary_row {\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#ilhvcxipab .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_grand_summary_row {\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#ilhvcxipab .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_striped {\r\n  background-color: #515151;\r\n}\r\n\r\n#ilhvcxipab .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_footnotes {\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#ilhvcxipab .gt_sourcenotes {\r\n  color: #FFFFFF;\r\n  background-color: #474747;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#ilhvcxipab .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#ilhvcxipab .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#ilhvcxipab .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#ilhvcxipab .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#ilhvcxipab .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#ilhvcxipab .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#ilhvcxipab .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#ilhvcxipab .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#ilhvcxipab .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nTop Earning Broadway Shows\r\n    through 2019\r\n    Rank\r\n      Show\r\n      \r\n      Premiere Year\r\n      Total Earnings\r\n      Avg TheaterCapacity Filled\r\n      \r\n        Tickets Sold Per Year\r\n      \r\n    Average\r\n      1987 - 2019\r\n    1\r\n      The Lion King\r\n      \r\n      1997\r\n      $1.66B\r\n      97.7&percnt;\r\n      676,795\r\n      \r\n    2\r\n      Wicked\r\n      \r\n      2003\r\n      $1.35B\r\n      97.2&percnt;\r\n      708,412\r\n      \r\n    3\r\n      The Phantom of the Opera\r\n      \r\n      1988\r\n      $1.24B\r\n      89.8&percnt;\r\n      600,508\r\n      \r\n    4\r\n      Chicago\r\n      \r\n      1996\r\n      $673.91M\r\n      82.7&percnt;\r\n      392,534\r\n      \r\n    5\r\n      The Book of Mormon\r\n      \r\n      2011\r\n      $647.07M\r\n      102.4&percnt;\r\n      445,435\r\n      \r\n    6\r\n      Mamma Mia!\r\n      \r\n      2001\r\n      $624.39M\r\n      89.7&percnt;\r\n      504,408\r\n      \r\n    7\r\n      Hamilton\r\n      \r\n      2015\r\n      $620.60M\r\n      101.7&percnt;\r\n      498,052\r\n      \r\n    8\r\n      Jersey Boys\r\n      \r\n      2005\r\n      $557.51M\r\n      89.5&percnt;\r\n      395,571\r\n      \r\n    9\r\n      Les Miserables\r\n      \r\n      1987\r\n      $548.80M\r\n      88.5&percnt;\r\n      353,014\r\n      \r\n    10\r\n      Aladdin\r\n      \r\n      2014\r\n      $447.72M\r\n      98.0&percnt;\r\n      686,539\r\n      \r\n    Data: Playbill | Table: Eric Ekholm (@ekholm_e)\r\n    \r\n\r\nImpressions\r\nRight, so, my overall impression of {gt} is that it’s an amazing package for building stylized static tables that fills a gap in R’s table ecosystem (to the extent that’s a thing). In my day job, I often find myself having to build tables either 1) as part of documents I’m creating on my own or 2) as stand-alone pieces that end up getting dropped into Powerpoints other people are putting together, and I’m excited about incorporating {gt} into my workflow for both of those types of tasks. Some more specific impressions of {gt}:\r\nIt feels a lot like {ggtplot2}. This isn’t surprising, given that the intent of the package is to provide a “grammar of tables.” But the flow & general process felt very familiar to me even though I haven’t used it extensively before, and I imagine anyone else who’s reasonably proficient using ggplot will feel the same when picking up gt. Which is a big plus, because it mitigates a lot of that difficulty of learning a new package.\r\nIt’s refreshingly easy to work with fonts. My biggest sore spot with ggplot is incorporating different fonts, which I always seem to struggle with (and my understanding is that this is a common struggle for Windows users). The {ragg} package seems to have made using fonts in ggplot easier, though. That said, using any Google font in gt is as easy as dropping in the google_font() function and voila, it works! Such a nice change of pace after my typical long troubleshooting sessions with fonts in ggplot.\r\nThe ability to include ggplot images and web images in a table is pretty cool. You’ll see that I added both to my table above, and each felt very easy to include. Honestly, the most time-consuming part was finding the urls for the playbill images.\r\nThe helper functions to format numbers, percents, and currency are great. I work with a lot of large $ amounts – as well as percents – in my job, and I’m super stoked about not having to manually format these anymore.\r\nI may have run into a small bug passing where font types (e.g. Lobster, Rubik) weren’t being recognized when called from within a list in the tab_style() function, which I’ll open an issue for. I found a workaround, but it involved essentially stylizing the same element multiple times, which doesn’t feel ideal.\r\nOverall, {gt} is a really awesome package – huge thanks to the team at Rstudio for putting it together and maintaining it!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T08:24:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-simulating-triangles/",
    "title": "Simulating Triangles?",
    "description": "Determining whether random numbers can form a triangle using simulation.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-10-18",
    "categories": [],
    "contents": "\r\nI saw this question on Reddit last night, and it seemed interesting to me & like a pretty quick blog post (which feels like all I have time for lately), so I wanted to run with it.\r\nThe question is: Three numbers between 0 and 1 are randomly chosen. What is the probability that the 3 numbers are the lengths of the sides of a triangle?\r\nThere’s probably a mathematical way to answer this question, but I’m going to approach it using simulation. First, let’s load packages and do some setup (note that we can do all of this first part in base R, but I’m loading some packages to help out with the bonus section at the end).\r\n\r\n\r\n\r\nDefining the Problem\r\nOur problem is fairly straightforward. My approach is to think about it like this: - If we choose 3 random numbers between 0 and 1, could they correspond to the lengths of a triangle? - If we repeated the previous step thousands (or millions) of times, what percent of the trials would form triangles. This will approximate the actual probability.\r\nThe other piece of information that’s relevant here is the requirement that, to form a triangle, the sum of the lengths of the two shorter segments must be longer than the length of the longest segment. Which makes sense intuitively if you think about what a triangle actually looks like.\r\nSimulating & Evaluating a Single Case\r\nSimulating a single trial is very straightforward and only requires the runif() function, which randomly samples from a uniform distribution (note that I used to think this function meant “run if…” and was very confused by it). We can do this as follows:\r\n\r\n\r\nset.seed(0408)\r\nx <- runif(n = 3, min = 0, max = 1)\r\nx\r\n\r\n\r\n[1] 0.009171368 0.084638904 0.618047172\r\n\r\nNow that we have these random numbers, the task is to figure out whether they can serve as the lengths of a triangle. Thinking back to our previous rule, we need the sum of the two shorter numbers to be greater than the largest number. We can evaluate this by:\r\n\r\n\r\nmin(x) + median(x) > max(x)\r\n\r\n\r\n[1] FALSE\r\n\r\nFor our first example, we see that this is false\r\nSimulating & Evaluating Many Cases\r\nOk, great. Now that we’ve managed to do this once, we just need to repeat the process several times. After evaluating many repeated trials, we’ll have our (approximate) probability. Here, I’m going to write a function to do simulate some numbers and then evaluate whether they could be the lengths of the sides of a triangle.\r\n\r\n\r\nsim_func <- function() {\r\n  x <- runif(n = 3, min = 0, max = 1)\r\n  \r\n  min(x) + median(x) > max(x)\r\n}\r\nsim_func()\r\n\r\n\r\n[1] TRUE\r\n\r\nAnd then the next step is to run that function several times (I’m going to go with 100,000) and then calculate the % that evaluate to TRUE.\r\n\r\n\r\ncalc_prob <- function(sims) {\r\n  tmp <- replicate(sims, sim_func())\r\n  \r\n  sum(tmp)/sims\r\n}\r\nset.seed(0408)\r\ncalc_prob(1e5)\r\n\r\n\r\n[1] 0.50059\r\n\r\nWe arrive at an answer of about 50%.\r\nBonus – Calculating Triangle Angles\r\nNow that we’ve determined how to simulate a bunch of triangles by evaluating the lengths of their sides, we can go a step further and calculate all the interior angles of these triangles. To do this, I’ll need to tweak my earlier function a little bit – rather than returning TRUE/FALSE, I need it to now return each of the sides. After simulating 10,000 (potential) triangles, I’ll filter for just those that actually can be triangles.\r\n\r\n\r\nsim_sides <- function() {\r\n  x <- runif(n = 3)\r\n  \r\n  tmp <- tibble(a = min(x), b = median(x), c = max(x))\r\n  \r\n}\r\nset.seed(0408)\r\nsimmed_triangles <- replicate(10000, sim_sides(), simplify = FALSE) %>%\r\n  bind_rows() %>%\r\n  filter(a + b > c)\r\n\r\n\r\n\r\nThe next step is to throw it back to high school geometry and the law of cosines. Using the formula below, we can calculate any angle of a triangle if we know the lengths of the three sides:\r\n\\(\\gamma = cos^{-1}(\\frac{a^2 + b^2 - c^2}{2ab})\\)\r\nwhere \\(\\gamma\\) is the angle we’re solving for, a and b are adjacent angles, and c is the angle opposite \\(\\gamma\\). One other thing to keep in mind is that R will calculate these angles in radians, so I’ll need to convert them to degrees (well, I don’t need to, but I’d prefer to).\r\n\r\n\r\ncalc_angle <- function(opp, adj1, adj2) {\r\n  180*(acos((adj1^2 + adj2^2 - opp^2)/(2*adj1*adj2)))/pi\r\n}\r\nsimmed_angles <- simmed_triangles %>%\r\n  mutate(ang_a = calc_angle(a, b, c),\r\n         ang_b = calc_angle(b, a, c),\r\n         ang_c = calc_angle(c, a, b),\r\n         id = row_number())\r\n\r\n\r\n\r\nI’m going to check to make sure that the sum of the angles equals 180 (which much be true for triangles).\r\n\r\n\r\nsimmed_angles %>%\r\n  mutate(is_180 = between((ang_a + ang_b + ang_c), 179.9, 180.1)) %>%\r\n  distinct(is_180)\r\n\r\n\r\n# A tibble: 1 x 1\r\n  is_180\r\n  <lgl> \r\n1 TRUE  \r\n\r\nCool, so the sum of all of the angles is 180.\r\nAngle C should always be the largest angle, which means it must be >= 60. Let’s do a quick check on that here.\r\n\r\n\r\nrange(simmed_angles$ang_c)\r\n\r\n\r\n[1]  60.36463 179.27215\r\n\r\nGreat. And let’s finally plot the distribution of these angles\r\n\r\n\r\nsimmed_angles %>%\r\n  select(-c(\"a\", \"b\", \"c\")) %>%\r\n  pivot_longer(cols = -id,\r\n               names_to = \"nms\",\r\n               values_to = \"value\") %>%\r\n  ggplot(aes(y = nms, x = value)) +\r\n  geom_density_ridges(aes(fill = nms), alpha = .8, stat = \"binline\") +\r\n  scale_y_discrete(labels = c(\"Angle A\", \"Angle B\", \"Angle C\")) +\r\n  labs(\r\n    x = \"Angle (in degrees)\",\r\n    y = NULL,\r\n    title = \"Distribution of Angles\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\",\r\n  )\r\n\r\n\r\n\r\n\r\nGreat, so we learned how to determine whether 3 random numbers could form the sides of a triangle and how to calculate the angles for those that did form a triangle.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-11-simulating-triangles/simulating-triangles_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-03-05T08:24:32-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-11-your-moms-house-analysis/",
    "title": "Your Mom's House Analysis",
    "description": "An exploratory analysis of transcripts from the YMH podcast.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-09-20",
    "categories": [],
    "contents": "\r\nWhat’s going on, mommies? I’ve been a big fan of Your Mom’s House for a long time now, and find myself constantly refreshing my podcast feed on Wednesdays waiting for those sweet sweet updates. I’ve also been getting more and more into text analysis, and so I figured why not combine the two and analyze the transcripts from YMH.\r\nA few months ago, I wrote a blog post describing how to pull transcripts from each video in a Youtube playlist. I’m not going to go over that part again, so if you’re curious about how to get the data, check out that post.\r\nA couple notes before getting into the analysis:\r\nThe episode transcripts are automatically created by Youtube’s speech-to-text model. It’s usually really good, but it does make some mistakes & obviously has trouble with proper nouns (e.g. TikTok).\r\nAnother feature of the fact that the transcripts are generated automatically is that the speaker of each phrase isn’t tagged. That is, there are no indications whether it’s Tom, Christina, or someone else speaking any given line.\r\nI’m only looking at the episodes available on Youtube, and the most recent episode I’m including is the one with Fortune Feimster.\r\nI’m going to include my code, but I’m not going to explain step-by-step what each code chunk does here.\r\nWith that out of the way, let’s move on and make sure to follow proto.\r\nSetup\r\n\r\n\r\n\r\nGeneral Exploration\r\nThere are a total of 169 episodes, beginning with 394 and ending with 567\r\nNext, let’s take a look at the length of the episodes here.\r\n\r\n\r\nep_lengths <- ymh %>%\r\n  group_by(ep_num) %>%\r\n  summarize(mins = (max(start) + max(duration))/60) %>%\r\n  ungroup()\r\n\r\nep_lengths %>%\r\n  ggplot(aes(x = ep_num, y = mins)) +\r\n  geom_line(color = herm) +\r\n  geom_point(color = herm) +\r\n  labs(\r\n    title = \"Length of YMH Episodes\",\r\n    x = \"Episode Number\",\r\n    y = \"Length (mins)\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt looks like episodes have tended to get longer up until about episode ~510 or so, and then the length dropped a bit. This might have been due to COVID and the fact that there was a stretch there where they didn’t have guests (which made the episodes shorter).\r\nNext, let’s take a look at the most common guests on the show.\r\n\r\n\r\nymh %>%\r\n  distinct(title) %>%\r\n  mutate(guest = str_remove_all(title, \"^.* w/ \") %>%\r\n           str_remove_all(\" - Ep. \\\\d+| - REUPLOADED\")) %>%\r\n  filter(str_detect(guest, \"Your Mom\", negate = TRUE)) %>%\r\n  mutate(guest = str_replace_all(guest, \"\\\\&\", \",\")) %>%\r\n  separate_rows(guest, sep = \",\") %>%\r\n  mutate(guest = str_trim(guest)) %>%\r\n  count(guest, name = \"num_appearances\", sort = TRUE) %>% \r\n  reactable()\r\n\r\n\r\n\r\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"guest\":[\"Bert Kreischer\",\"Dr. Drew Pinsky\",\"Brendan Schaub\",\"Johnny Pemberton\",\"Nikki Glaser\",\"Adam Ray\",\"Andrew Santino\",\"Ari Shaffir\",\"Brian Simpson\",\"Chris D'Elia\",\"H3H3\",\"Jo Koy\",\"Matt Fulchiron\",\"Moshe Kasher\",\"Natasha Leggero\",\"Ryan Sickler\",\"Sam Tripoli\",\"Tom Papa\",\"Tony Hinchcliffe\",\"Wheeler Walker Jr.\",\"Alison Rosen\",\"Alyssa Milano\",\"Amanda Cerny\",\"Andrew Collin\",\"Annie Lederman\",\"Anthony Jeselnik\",\"Big Daddy Kane\",\"Big Jay Oakerson\",\"Bill Burr\",\"Bobby Lee\",\"Brad Williams\",\"Brendon Urie\",\"Brent Weinbach\",\"Brittany Furlan\",\"Cesar Millan\",\"Chad Daniels\",\"Craig Ferguson\",\"Creators of HBO's McMillions\",\"Dane Cook\",\"Danny Brown\",\"Daymond John\",\"DMC\",\"Donnell Rawlings\",\"Doug Mellard\",\"Dr. Drew\",\"Earl Skakel\",\"Elizabeth Lail\",\"Fahim Anwar\",\"Felipe Esparza\",\"Finesse Mitchell\",\"Fortune Feimster\",\"Gabriel Iglesias\",\"Geoff Tate\",\"George Perez\",\"Grant Cardone\",\"Greg Fitzsimmons\",\"Greg Fitzsimmons and Kyle Kinane\",\"Heath Evans\",\"Ian Edwards\",\"J. Elvis Weinstein\",\"Jake Weisman and Matt Ingebretson\",\"Jamie-Lynn Sigler\",\"Jeff Ross\",\"Jessica Kirson\",\"Jim Gaffigan\",\"Joe Rogan\",\"Joey \\\"Coco\\\" Diaz\",\"Joey Diaz\",\"Josh Wolf\",\"Judd Apatow\",\"Kate Kennedy\",\"Kevin Blatt\",\"Kevin Christy\",\"Kevin Nealon\",\"Khalyla\",\"Kreayshawn\",\"Kyle Dunnigan\",\"Link\",\"Liza Treyger\",\"Marc Maron\",\"Martin Riese\",\"Matt Braunger\",\"Maz Jobrani\",\"Melissa Villaseñor\",\"Michael Rapaport\",\"Ms. Pat\",\"Nicole Byer\",\"Paul Gilmartin\",\"Pauly Shore\",\"Redban\",\"Rhett\",\"Robert Hines\",\"Robert Iler\",\"Ron Funches\",\"Russell Peters\",\"Ryan Sickler and Steven Randolph\",\"Ryan Stout\",\"Sarah Tiana\",\"Scott Thompson\",\"Sean Anders\",\"Sean Evans\",\"Sera Gamble\",\"Steve-O\",\"Steve Byrne\",\"Steve Simeone\",\"Taylor Tomlinson\",\"The Sklar Brothers\",\"Theo Von\",\"Tigerbelly\",\"Tim Dillon\",\"Tom Green\",\"Tom Rhodes\",\"Tommy Lee\",\"Too Short\",\"Try It Out Guy\",\"Water Sommelier\",\"Wheeler Walker Jr\",\"Whitney Cummings\",\"Yakov Smirnoff\",\"Yoshi\"],\"num_appearances\":[4,4,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"columns\":[{\"accessor\":\"guest\",\"name\":\"guest\",\"type\":\"character\"},{\"accessor\":\"num_appearances\",\"name\":\"num_appearances\",\"type\":\"numeric\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"0e3ec624c6368792cb6590377f18c7aa\",\"key\":\"0e3ec624c6368792cb6590377f18c7aa\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\r\nNote that this only counts guests named in the title of the episode and won’t capture people like Josh Potter, Top Dog & Charo, etc. Dr. Drew is also probably under-counted here, since he makes a lot of cameos via phone call and whatnot (like when Tim and Christine called him up to ask about corn cob sanitation recently). The big takeaway here, though, is that there really aren’t that many repeat guests.\r\nMost Common Words\r\nCool. Let’s shift gears here, now, and start to dig into the actual transcripts. Once again, these don’t delineate who is speaking, but there is still a lot of data here. I’m going to start by looking at the most commonly-used words across all of the episodes. In doing this, I’m also going to filter out stop words – things like “the,” “at,” “I’m”, etc.\r\n\r\n\r\nymh_words <- ymh_one_obs %>%\r\n  unnest_tokens(word, text)\r\n\r\nymh_words <- ymh_words %>%\r\n  anti_join(stop_words) %>%\r\n  filter(!(word %in% c(\"yeah\", \"gonna\", \"uh\", \"hey\", \"cuz\")))\r\n\r\nymh_words %>%\r\n  count(word) %>%\r\n  slice_max(order_by = n, n = 20) %>%\r\n  ggplot(aes(x = n, y = fct_reorder(word, n))) +\r\n  geom_col(fill = herm) +\r\n  labs(\r\n    title = \"Most Used Words in YMH\",\r\n    y = NULL,\r\n    x = \"Count\"\r\n  )\r\n\r\n\r\n\r\n\r\nOk, so, there’s a lot we could unpack here. But what stands out to me is that “fucking” is the 3rd most commonly used word in the podcast, right after “people” and “guy.” This isn’t surprising to me, but I want to dig into it a little more on an episode-by-episode basis.\r\nI’m going to take a look at how many times the word “fucking” appears per episode. Actually, I’m going to look at “fucking”s per minute as a way to control for episode length.\r\n\r\n\r\nymh_words %>%\r\n  filter(word == \"fucking\") %>%\r\n  count(ep_num, word) %>%\r\n  left_join(ep_lengths, by = \"ep_num\") %>%\r\n  mutate(fpm = n/mins) %>%\r\n  ggplot(aes(x = ep_num, y = fpm)) +\r\n  geom_text(aes(size = fpm), label = \"fucking\", show.legend = FALSE, color = herm) +\r\n  annotate(\"text\", x =  510, y = 1.6, label = \"Ep 494 with Joey Diaz\", hjust = 0, size = 3.5) +\r\n  annotate(\"curve\", x = 494, xend = 508, y = 1.68, yend = 1.58, curvature = .4) +\r\n  labs(\r\n    x = \"Episode Number\",\r\n    y = \"'Fucking's Per Minute\",\r\n    title = \"Fucking's per Minute in YMH Episodes\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt’s maybe no surprise that Episode 494 with Joey Diaz had far and away the highest rate of fucking’s-per-minute at 1.76, which is about .76 higher than the next highest (1.008 in Ep 433 with Bill Burr).\r\nYMH Vocabulary Over Time\r\nNext, I want to look at the evolution of some of the more popular mommy-isms over time – the memes and vocabulary that define the show. I’m going to start by tracking the use of the word “Julia” – as in “good morning, julia!” – over time, since that’s still my favorite clip from the show.\r\n\r\n\r\nymh_words %>%\r\n  filter(word == \"julia\") %>%\r\n  count(ep_num, word) %>%\r\n  left_join(x = tibble(ep_num = unique(ep_lengths$ep_num), word = \"julia\"),\r\n            y = ., by = c(\"ep_num\", \"word\")) %>%\r\n  mutate(n = replace_na(n, 0)) %>%\r\n  ggplot(aes(x = ep_num, y = n)) +\r\n  geom_line(color = herm, size = 1.25) +\r\n  labs(\r\n    y = \"Count\",\r\n    x = \"Episode Number\",\r\n    title = \"Good Morning, Julia!\",\r\n    subtitle = \"Counts of the word 'Julia' by episode\"\r\n  )\r\n\r\n\r\n\r\n\r\nSo, we see a pretty clear trend here – the popularity of “julia” starts up right around episode ~470ish, stays high for a few episodes, and then peaks at episode ~477 (give or take), which I assume is the episode where Tony and Catherine actually interview Julia.\r\nThere are tons of other words and phrases I could make this same graph for – “four strokes,” “let me eat ya,” etc. – but I’m going to pick a handful and write a little function that’ll extract those from the transcript and then make a faceted graph similar to the above.\r\n\r\n\r\nisolate_phrases <- function(phrase) {\r\n  nwords <- str_count(phrase, \" \") + 1\r\n  \r\n  token <- if (nwords == 1) {\r\n    \"words\"\r\n  } else {\r\n      \"ngrams\"\r\n  }\r\n  \r\n  tmp <- if (token == \"ngrams\") {\r\n    ymh_one_obs %>%\r\n      unnest_tokens(output = words, input = text, token = token, n = nwords)\r\n  } else {\r\n    ymh_one_obs %>%\r\n      unnest_tokens(output = words, input = text, token = token)\r\n  }\r\n  \r\n  tmp %>%\r\n    filter(words == phrase)\r\n}\r\n\r\nphrases <- c(\"julia\", \"charles\", \"robert paul\", \"retarded\", \"garth\", \"cool guy\", \"fed smoker\", \"feathering\", \"scrum\", \"mystic rick\", \"don't be stingy\", \"piss on me\")\r\n\r\nymh_phrase_list <- map(phrases, isolate_phrases)\r\n\r\nphrase_grid <- expand_grid(unique(ep_lengths$ep_num), phrases) %>%\r\n  rename(ep_num = 1)\r\n\r\nymh_phrase_df <- ymh_phrase_list %>%\r\n  bind_rows() %>%\r\n  count(ep_num, words) %>%\r\n  left_join(x = phrase_grid, y = ., by = c(\"ep_num\", \"phrases\" = \"words\")) %>%\r\n  mutate(n = replace_na(n, 0))\r\n\r\nymh_phrase_df %>%\r\n  mutate(phrases = str_to_title(phrases)) %>%\r\n  ggplot(aes(x = ep_num, y = n)) +\r\n  geom_line(color = herm) +\r\n  facet_wrap(~phrases, scales = \"free_y\") +\r\n  labs(\r\n    y = \"Count\",\r\n    x = \"Episode Number\",\r\n    title = \"Now Here Are Some Cool Phrases\",\r\n    subtitle = \"Counts of Key YMH Words/Phrases by Episode\"\r\n  ) +\r\n  ggthemes::theme_tufte() +\r\n  theme(\r\n    plot.title.position = \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nOne thing to note here is that the y-axis scale is different for each facet, so it’s not really easy to make comparisons across plots. But if I keep them on the same scale, it makes it hard to see movement in anything other than the “Julia” plot. Overall, though, I think this does a pretty nice job at tracking the lifespan of some of the common memes/jokes that run through the show.\r\nPopular Words Across Groups of Episodes\r\nOne limitation of the plots above is that I just chose 12 words/phrases that I think exemplify the show and tracked their usage over time. Like I said, there are tons of other phrases I could have chosen instead, and so the analysis is kinda limited by my personal choice.\r\nOne way to let the data drive the analysis is to look at “definitive” words over time. The way I’ll do this is – I’ll classify every 5 episodes as a group (yes this is arbitrary, but it seems reasonable enough). Then, I’ll calculate the weighted (log) odds of a specific word showing up in that group versus any other group. Then I’ll choose the words with the highest odds per group. These will be the “definitive” words for that group of episodes.\r\n\r\n\r\nymh_groups <- tibble(\r\n  ep_num = unique(ep_lengths$ep_num),\r\n  group = 1:length(unique(ep_lengths$ep_num)) %/% 5\r\n) %>%\r\n  group_by(group) %>%\r\n  mutate(max = max(ep_num),\r\n         min = min(ep_num),\r\n         group_name = glue::glue(\"Ep { min } - Ep { max }\")) %>%\r\n  ungroup() %>%\r\n  select(-c(group, min, max))\r\n\r\nset.seed(0408)\r\n\r\nymh_lo <- ymh_words %>%\r\n  left_join(x = .,\r\n            y = ymh_groups,\r\n            by = \"ep_num\") %>%\r\n  count(group_name, word) %>%\r\n  filter(n > 6 & (str_length(word) > 2) & !(word %in% c(\"xiy\", \"pql\", \"8ww\", \"hmh\", \"music\"))) %>%\r\n  bind_log_odds(set = group_name, feature = word, n = n) %>%\r\n  group_by(group_name) %>%\r\n  slice_max(order_by = log_odds_weighted, n = 3, with_ties = FALSE)\r\n\r\nymh_lo %>%\r\n  ungroup() %>%\r\n  mutate(header = rep(c(\"Most Distinctive\", \"2nd Most Distinctive\", \"3rd Most Distinctive\"),\r\n            length(unique(ymh_lo$group_name)))) %>%\r\n  select(-c(n, log_odds_weighted)) %>%\r\n  rename(`Ep Group` = group_name) %>%\r\n  pivot_wider(names_from = header, values_from = word) %>%\r\n  reactable()\r\n\r\n\r\n\r\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"Ep Group\":[\"Ep 394 - Ep 397\",\"Ep 398 - Ep 402\",\"Ep 404 - Ep 409\",\"Ep 410 - Ep 414\",\"Ep 415 - Ep 419\",\"Ep 420 - Ep 424\",\"Ep 426 - Ep 430\",\"Ep 431 - Ep 435\",\"Ep 436 - Ep 440\",\"Ep 441 - Ep 445\",\"Ep 446 - Ep 451\",\"Ep 452 - Ep 456\",\"Ep 457 - Ep 461\",\"Ep 462 - Ep 466\",\"Ep 467 - Ep 471\",\"Ep 472 - Ep 476\",\"Ep 477 - Ep 481\",\"Ep 482 - Ep 487\",\"Ep 488 - Ep 492\",\"Ep 493 - Ep 497\",\"Ep 498 - Ep 502\",\"Ep 503 - Ep 507\",\"Ep 508 - Ep 512\",\"Ep 513 - Ep 517\",\"Ep 518 - Ep 522\",\"Ep 523 - Ep 527\",\"Ep 528 - Ep 532\",\"Ep 533 - Ep 537\",\"Ep 538 - Ep 542\",\"Ep 543 - Ep 547\",\"Ep 548 - Ep 552\",\"Ep 553 - Ep 557\",\"Ep 558 - Ep 562\",\"Ep 563 - Ep 567\"],\"Most Distinctive\":[\"waters\",\"marry\",\"soup\",\"soup\",\"machines\",\"machines\",\"machines\",\"wrestling\",\"mcdonald's\",\"tree\",\"stroke\",\"burp\",\"queef\",\"forrest\",\"julia\",\"julia\",\"julia\",\"julia\",\"pimp\",\"homie\",\"persian\",\"panties\",\"bukkake\",\"september\",\"julia\",\"wreck\",\"wolf\",\"charles\",\"charles\",\"quarantine\",\"wrestling\",\"wrestling\",\"jamie\",\"stingy\"],\"2nd Most Distinctive\":[\"personality\",\"blonde\",\"queef\",\"rub\",\"egg\",\"claus\",\"mtv\",\"hog\",\"breast\",\"scat\",\"taco\",\"stroke\",\"marijuana\",\"mcdonald's\",\"throttle\",\"mtv\",\"studio\",\"mcdonald's\",\"bus\",\"prison\",\"500\",\"dmc\",\"intro\",\"jet\",\"wreck\",\"bus\",\"wreck\",\"robert\",\"fed\",\"elementary\",\"poutine\",\"quarantine\",\"jewish\",\"quarantine\"],\"3rd Most Distinctive\":[\"spring\",\"blind\",\"blind\",\"blind\",\"bus\",\"waters\",\"wolf\",\"march\",\"sneeze\",\"defecation\",\"meth\",\"massage\",\"rub\",\"chicago\",\"diaper\",\"canada\",\"machines\",\"terry\",\"tape\",\"aids\",\"coach\",\"intro\",\"tock\",\"intro\",\"jewish\",\"rec\",\"mcdonald's\",\"march\",\"sparkling\",\"virus\",\"quarantine\",\"poutine\",\"quarantine\",\"fed\"]},\"columns\":[{\"accessor\":\"Ep Group\",\"name\":\"Ep Group\",\"type\":[\"glue\",\"character\"]},{\"accessor\":\"Most Distinctive\",\"name\":\"Most Distinctive\",\"type\":\"character\"},{\"accessor\":\"2nd Most Distinctive\",\"name\":\"2nd Most Distinctive\",\"type\":\"character\"},{\"accessor\":\"3rd Most Distinctive\",\"name\":\"3rd Most Distinctive\",\"type\":\"character\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"efc171a4a2b00b240b05a9826f59cd87\",\"key\":\"efc171a4a2b00b240b05a9826f59cd87\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\r\nLet’s unpack this a little bit. I’ll start at the end. The three most definitive words of Eps 563-567 are “stingy” (as in “don’t be stingy, Mark”), “quarantine” (as in the thing everyone is talking about), and “fed” (as in “fed smoker” – presumably this is driven by the interview with fed smoker’s nemesis). Before that, in Eps 558-562, we have “jamie” (as in Jamie-Lynn Sigler), “jewish” (the dude with the cool mouse pads), and, again, “quarantine.” This analysis gives us a loose idea about what some of the key themes of these episodes are.\r\nConnections between Words\r\nThe last thing I’m going to do is look at connections between words. To do this, I’m going to make a network diagram. The basic idea here is that this analysis will visualize words that are commonly used together by drawing links between them. I’m also going to change the size of the word to indicate how often the word gets used (with bigger words being used more).\r\nOne other note – like in previous steps here, I’m filtering out stop words (e.g. “the”, “it”, etc). I also noticed when I first tried this that I got a lot of phrases from Saatva reads – lots of pairings with the word “mattress” and “delivery,” so I’m filtering those out as well.\r\n\r\n\r\nfilter_words <- c(\"music\", \"yeah\", \"uh\", \"huh\", \"hmm\", \"mattress\", \"delivery\")\r\n\r\nymh_bigrams <- ymh_one_obs %>%\r\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\r\n  separate(bigram, c(\"word1\", \"word2\")) %>%\r\n  filter(!is.element(word1, stop_words$word) &\r\n           !is.element(word2, stop_words$word) &\r\n           !is.na(word1) &\r\n           !is.na(word2) &\r\n           !is.element(word1, filter_words) &\r\n           !is.element(word2, filter_words) &\r\n           word1 != word2) %>%\r\n  count(word1, word2, sort = TRUE)\r\n\r\nymh_bigrams_small <- ymh_bigrams %>%\r\n  slice_max(order_by = n, n = 100)\r\n\r\nymh_verts <- ymh_one_obs %>%\r\n  unnest_tokens(word, text) %>%\r\n  count(word) %>%\r\n  mutate(n = log(n)) %>%\r\n  filter(is.element(word, ymh_bigrams_small$word1) |\r\n           is.element(word, ymh_bigrams_small$word2))\r\n\r\nymh_net <- graph_from_data_frame(ymh_bigrams_small, vertices = ymh_verts)\r\n\r\nset.seed(0409)\r\n\r\nymh_net %>%\r\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\r\n  geom_edge_link(aes(size = n), color = herm) +\r\n  geom_node_point(color = herm) +\r\n  geom_node_text(aes(label = name, size = n), repel = TRUE, segment.color = herm, label.padding = .1, box.padding = .1) +\r\n  theme_void() +\r\n  labs(title = \"Most Commonly Connected Words in YMH\") +\r\n  theme(legend.position = \"none\",\r\n        plot.title = element_text(hjust = .5, size = 16))\r\n\r\n\r\n\r\n\r\nThe way to read this graph is to look for words that are connected (note that the direction doesn’t mean anything here – it’s just random). So, the words “tick” and “tock” (TikTok) show up a lot together, as do “toilet” and “paper” (from Tushy reads). Over on the left side, we see more of a wheel-shaped pattern with “gonna” in the middle, which is linked to a bunch of other words – “gonna die,” “gonna throw” (up), “gonna call,” etc. There’s also a little network toward the bottom left where we see connections between guy, guys, black, white, cool, and nice (black guys, cool guys, white guy, etc). A few other standouts to me are “water champ,” “mentall ill”, “robert paul,” and, of course “morning julia.”\r\nThat’s it for now – keep it high and tight, Jeans.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-11-your-moms-house-analysis/your-moms-house-analysis_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T08:24:32-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-11-chopped-episode-simulator/",
    "title": "Chopped Episode Simulator",
    "description": "Simple app to simulate an episode of Chopped.\"",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-09-05",
    "categories": [],
    "contents": "\r\nAbout a week ago, I used a dataset containing ingredient lists (among other things) across the entire history of the show Chopped to create a simple Shiny app that will simulate an episode. I’ve embedded the app below, and you can find the code used to create it here\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T08:24:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-scrantonicity-part-3/",
    "title": "Scrantonicity - Part 3",
    "description": "Predicting the speaker of dialogue from The Office.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-08-29",
    "categories": [],
    "contents": "\r\nTL;DR:: In this blog, I use LASSO logistic regression and multilevel logistic regression to predict the speaker of lines of dialogue from The Office.\r\nWhat feels like forever ago, I wrote two blog posts analyzing transcripts from The Office. The first was a basic EDA of the dialogue, and the second used k-means clustering to determine types of Office episodes based on who speaks to whom. At the end of that second blog, I mentioned that I might do some predictive analysis with that data in the future. Well, it’s four months later, and I’m declaring that the future is now!\r\n\r\nBasically, the goal here is going to be, for a given line of dialogue from the show, to predict whether it’s Michael talking or someone else. At first blush, this seems like it shouldn’t be too hard. Many of Michael’s lines are iconic (e.g. see the above gif), but I feel like this might be more a function of the delivery than the actual words themselves, and I’m curious to see how well a model (or multiple models) could predict this just from the text.\r\nIn doing this, there are a couple of things I’m interested in doing here:\r\nGenerally getting more practice with {tidymodels}\r\nLearning to use the {textrecipes} package\r\nTrying the {glmmTMB} package (not part of the {tidymodels} ecosystem)\r\nAlso, before getting too much further, I want to acknowledge that I looked at this blog by Julia Silge and this blog by Emil Hvitfeldt for some background on {textrecipes}. Both are really great for people interested in text analysis.\r\nAnyway, without much further ado, let’s get into it. As has been the case in all of my “Scrantonicity” posts, the data I’m using here comes from the {schrute} package. First, I’ll load in libraries and set some defaults/options. I’m also going to read in the data, limiting the dialogue to the first seven seasons of the show (the Michael Scott era).\r\nSetup\r\n\r\n\r\n\r\nBrief EDA and Data Preprocessing\r\nBefore modeling data, I would typically do a more thorough EDA. But I’ve already explored this data pretty closely (albeit months ago) in two previous blog posts, so rather than re-doing that EDA, I’m just going to look at those posts. One thing I will include here, though, is a quick look at the number of lines spoken by Michael Scott vs other characters, since this is the outcome I’m interested in predicting here.\r\n\r\n\r\noffice %>%\r\n  count(character) %>%\r\n  top_n(10) %>%\r\n  ggplot(aes(x = n, y = fct_reorder(character, n))) +\r\n  geom_col(fill = herm) +\r\n  labs(\r\n    title = \"Lines by Character\",\r\n    subtitle = \"First seven seasons\",\r\n    y = NULL,\r\n    x = \"Number of Lines\"\r\n  )\r\n\r\n\r\n\r\n\r\nSo, Michael has far and away the most lines of any character. But it’ll also be useful to look at Michael vs all of the others lumped together (since this is what I’m actually predicting).\r\n\r\n\r\noffice %>%\r\n  count(is_mike) %>%\r\n  ggplot(aes(x = n, y = fct_reorder(is_mike, n))) +\r\n  geom_col(fill = herm) +\r\n  labs(\r\n    title = \"Mike vs Not Mike\",\r\n    y = \"Is Michael?\",\r\n    x = \"Number of Lines\"\r\n  )\r\n\r\n\r\n\r\n\r\nEven though Michael speaks more than any other given character, he speaks about a third as many lines as all of the other characters combined. This is relevant here because it means I’ll want to downsample when I train my model to ensure the number of observations in each class are similar, which will help the model fit.\r\nData Splitting & Preprocessing\r\nNext, I’m going to split my data into a training a testing set.\r\n\r\n\r\nset.seed(0408)\r\noffice_split <- initial_split(office, strata = is_mike)\r\ntr <- training(office_split)\r\nte <- testing(office_split)\r\n\r\n\r\n\r\nNow that I’ve split my data, I’m going to preprocess the data using {recipes}, {textrecipes}, and {themis} (to handle class imbalance). One thing to clarify here: I’m building a model to predict whether the speaker of a given line of dialogue is Michael. In this analysis, I want to build this model using only the text data, although there are plenty of other text-based features I could include. More specifically, I am going to handle the preprocessing such that the model I end up fitting is a bag-of-words model. This means that I want my data to include a variable for each word* (not really each word, but I’ll show later) in the transcript, each row to represent a line of dialogue, and the value in each cell to represent the tf-idf of that word. From this data structure, I can build a model where each word has an individual effect on the odds that the line is spoken by Michael, although note that this model will have no sense of word order.\r\nI’ll specify this recipe and then walk through each step afterward.\r\n\r\n\r\noffice_recipe <- recipe(is_mike ~ text + episode_name, data = tr) %>%\r\n  themis::step_downsample(is_mike) %>%\r\n  step_tokenize(text) %>%\r\n  step_stopwords(text) %>%\r\n  step_tokenfilter(text, max_tokens = 200) %>%\r\n  step_tfidf(text) %>%\r\n  prep()\r\ntr_prepped <- juice(office_recipe)\r\ntr_prepped_noep <- tr_prepped %>%\r\n  select(-episode_name)\r\nte_prepped <- bake(office_recipe, te)\r\nte_prepped_noep <- te_prepped %>%\r\n  select(-episode_name)\r\n\r\n\r\n\r\nLet’s unpack this step-by-step:\r\nstep_downsample() will balance the data so that the number of cases where Michael is the speaker is equal to the number of cases where Michael is not the speaker. This is done by randomly dropping rows.\r\nstep_tokenize() will take the text column in the data and create a isolate each word per line.\r\nstep_stopwords() will remove stop words (e.g. “the”, “it”, “a”) that likely won’t contain much useful information.\r\nstep_tokenfilter(), as I’m using it here, will retain only the 200 most frequently used words. This is a pretty large number, but I’m going to fit a LASSO regression later, which can select out some of these if necessary.\r\nstep_tfidf() calculates the term frequency-inverse document frequency, which provides a metric for how important a word is to a given document (e.g. a line in this case).\r\nAnother thing to note here is that I’m creating two versions of this preprocessed data for the training and test sets. The differences between “tr_prepped” and “tr_prepped_noep” (as well as their “te” counterparts) is that the “noep” versions do not have a variable identifying which line the episode came from (but are otherwise identical). This is because I don’t want to include the episode identifier in my single-level LASSO model but do want to include it in the multilevel model. I could also accomplish this by specifying the formula and having it not include the episode_number variable rather than creating two datasets.\r\nMoving along! Next, I’m going to specify my model. Since I have a binary outcomes (yes/no if the speaker is Michael), I’m going to run a logistic regression. I’m going to run this as a LASSO model, which will provide some feature selection and generally shrink coefficients. I’m going to tune the model to choose the best amount of penalty as well.\r\n\r\n\r\nreg_spec <- logistic_reg(mixture = 1, penalty = tune()) %>%\r\n  set_engine(\"glmnet\")\r\nreg_spec\r\n\r\n\r\nLogistic Regression Model Specification (classification)\r\n\r\nMain Arguments:\r\n  penalty = tune()\r\n  mixture = 1\r\n\r\nComputational engine: glmnet \r\n\r\nHere, I’m creating some resamples of my training data to help with the tuning. I’m creating 10 bootstrap samples here.\r\n\r\n\r\nset.seed(0408)\r\nbooties <- bootstraps(tr_prepped_noep, strata = is_mike, times = 10)\r\n\r\n\r\n\r\nLASSO Model Fitting & Examination\r\nNow it’s time to fit the LASSO model. I’m going to add the logistic regression specification that I just created to a workflow. Along with that model specification, I’m also going to add a formula where is_mike is regressed on all of the word features I just created. Then, I’m going to tune the model across 10 candidate values of the penalty parameter (i.e. how much regularization I’m adding).\r\n\r\n\r\noffice_wf <- workflow() %>%\r\n  add_model(reg_spec) %>%\r\n  add_formula(is_mike ~ .)\r\nset.seed(0408)\r\nlogreg_fit <- tune_grid(\r\n  office_wf,\r\n  resamples = booties,\r\n  grid = 10\r\n)\r\n\r\n\r\n\r\nGreat. Now that the models have been fit with various penalty values across the bootstrap resamples, I can check to see what the best penalty value is to move forward with & finalize a model. I’m going to choose the best by one standard error (which, in this case, happens also to be the best model). The one standard error rule will let me choose the most parsimonious model (in this case, the one with the most penalty) that is within one standard error of the best model. And once I choose the best penalty value, I’ll go ahead and finalize the model and refit on the training set.\r\n\r\n\r\nlogreg_fit %>%\r\n  show_best(\"accuracy\")\r\n\r\n\r\n# A tibble: 5 x 7\r\n   penalty .metric  .estimator  mean     n std_err .config            \r\n     <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>              \r\n1 2.27e- 3 accuracy binary     0.580    10 0.00181 Preprocessor1_Mode~\r\n2 1.02e-10 accuracy binary     0.577    10 0.00216 Preprocessor1_Mode~\r\n3 1.27e- 9 accuracy binary     0.577    10 0.00216 Preprocessor1_Mode~\r\n4 7.94e- 8 accuracy binary     0.577    10 0.00216 Preprocessor1_Mode~\r\n5 4.46e- 7 accuracy binary     0.577    10 0.00216 Preprocessor1_Mode~\r\n\r\nbest_params <- logreg_fit %>%\r\n  select_by_one_std_err(metric = \"accuracy\", desc(penalty))\r\nfinal_logreg <- office_wf %>%\r\n  finalize_workflow(best_params) %>%\r\n  fit(data = tr_prepped_noep)\r\n\r\n\r\n\r\nSo, the best model here has an accuracy of ~58%. Not great, but better than just straight-up guessing. Remember that this is on the training set. Now, I’ll take a look at what the accuracy is on the test set.\r\n\r\n\r\nbind_cols(\r\n  predict(final_logreg, te_prepped_noep), te_prepped_noep\r\n) %>%\r\n  accuracy(is_mike, .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.614\r\n\r\n61% – not bad! It’s actually better than the training set accuracy, which means our training process didn’t lead to overfitting, which is great.\r\nNow, I’m going to take a look at which words are the most important to predicting whether the speaker of a line of dialogue is Michael or not.\r\n\r\n\r\nfinal_logreg %>%\r\n  pull_workflow_fit() %>%\r\n  vi() %>%\r\n  slice_max(order_by = abs(Importance), n = 10) %>%\r\n  ggplot(aes(x = abs(Importance), y = fct_reorder(Variable %>% str_remove(\"tfidf_text_\"), abs(Importance)), fill = Sign)) +\r\n  geom_col() +\r\n  labs(\r\n    title = \"Most Important Words Identifying Michael Scott\",\r\n    subtitle = \"Positive values more representative of MS, negative values more representative of others\",\r\n    y = NULL\r\n  )\r\n\r\n\r\n\r\n\r\nNot surprisingly, the word “Michael” is the strongest predictor, and has a negative effect – if a line has the word “Michael” in it, it is less likely to be spoken by Michael. Intuitively, this makes sense. Other people use Michael’s name when speaking to or about him. The rest of the effects in this chart make sense to me as well (except for “mifflin” and “dunder,” which I don’t really get). But Michael is certainly more likely to talk about Jan and David than are other characters, and “everybody” feels right to me as well…\r\n\r\nAnd the final thing I’m going to do with this logistic regression is to pull out names of the non-zero coefficients. Recall that the lasso penalty can (but doesn’t always) shrink coefficients to zero. These variables will have no effect on the outcome. The reason I’m doing this is because I want to fit a multilevel model next, but I’m not going to regularize that model. Instead, I’ll just specify a formula that doesn’t include the variables that got shrunk to zero in this model.\r\n\r\n\r\nkeep_vars <- final_logreg %>%\r\n  pull_workflow_fit() %>%\r\n  vi() %>%\r\n  filter(Importance != 0) %>%\r\n  pull(Variable)\r\n\r\n\r\n\r\nMultilevel Model Fitting\r\nNow, I’m going to dive into fitting a multilevel model. To give a very brief overview of multilevel models, they are models that can take into account dependencies (nesting) within data. Recall that one of the assumptions of a linear regression is that each observation is independent. We often violate that assumption in the real world. In my work, for instance, students are often nested within classrooms (i.e. a common effect – their teacher – influences them & introduces a dependency). Another common case of nesting is when you have multiple observations over time from the same set of people. In the case of this current data, we can consider that each line is nested within an episode (terminology note: episode would be the “clustering variable” or “grouping variable” here). We could also go a step further and nest episodes within seasons to get a 3-level model rather than a 2-level model, but I’m not going to do that here.\r\nFitting multilevel models allows for random effects, where the coefficient of a given term differs based on the clustering variable. Any term in the model can have a random effect, but the simplest form of a multilevel model – and the one I’m going to fit here – is a random intercept model, where the value of the intercept changes depending on the clustering variable. In the current dataset, this would mean that Michael might be more (or less) likely to speak overall in a given episode (when compared to all other episodes), and so the intercept value will change to reflect that. It’s also possible to fit random slopes, where the effect of a given non-intercept term differs from episode to episode. Contextualizing that in the current data, it might mean that the word “Jan” is more (or less) associated with being spoken by Michael depending on the episode. Usually, you want a pretty clear theoretical rationale for specifying random slopes, and I don’t really have that here. Plus, it would be unreasonable to try to estimate random slopes for all of the words in the dataset (even though I only have a subset of ~190).\r\nIf you’re interested in learning more about multilevel models, Raudenbush & Bryk (2002) is a classic, and John Fox’s Applied Regression Analysis is just generally a really good book that has a chapter on MLMs.\r\nAnyway – onward and upward. First, I want to specify the formula of the model. I’m going to include all of the variables that had non-zero coefficients in the lasso model earlier, and I’m also going to add a term at the end to specify the random intercept for each episode – (1 | episode_name).\r\n\r\n\r\nglmm_formula <- as.formula(paste(\"is_mike ~ \", paste(keep_vars, collapse = \" + \"), \" + (1 | episode_name)\"))\r\n\r\n\r\n\r\nI’m going to fit this model using the {glmmTMB} package, which provides an interface for fitting all sort of generalized linear mixed models. I haven’t used this specific package before, but I have used {lme4}, which has similar syntax and is essentially the same thing for fitting linear models. I’m going to fit the model using the training data – note that I’m not tuning anything here – and I’m specifying the binomial family because this is a logistic regression.\r\n\r\n\r\nglmm_fit <- glmmTMB(glmm_formula, data = tr_prepped, family = binomial)\r\n\r\n\r\n\r\nI’m going to show the summary of the model here, but it’s going to be a biiig printout since we have so many terms in the model, so feel free to scroll on by. One thing you might want to check out, though, is the summary of the variance of the intercept, which summarizes the amount of randomness in that effect.\r\n\r\n\r\nsummary(glmm_fit)\r\n\r\n\r\n Family: binomial  ( logit )\r\nFormula:          \r\nis_mike ~ tfidf_text_michael + tfidf_text_scott + tfidf_text_friend +  \r\n    tfidf_text_stanley + tfidf_text_jan + tfidf_text_everybody +  \r\n    tfidf_text_business + tfidf_text_may + tfidf_text_bad + tfidf_text_pretty +  \r\n    tfidf_text_say + tfidf_text_dunder + tfidf_text_god + tfidf_text_angela +  \r\n    tfidf_text_scranton + tfidf_text_somebody + tfidf_text_pam +  \r\n    tfidf_text_ah + tfidf_text_best + tfidf_text_going + tfidf_text_show +  \r\n    tfidf_text_life + tfidf_text_guy + tfidf_text_ok + tfidf_text_thing +  \r\n    tfidf_text_well + tfidf_text_alright + tfidf_text_holly +  \r\n    tfidf_text_mean + tfidf_text_know + tfidf_text_okay + tfidf_text_toby +  \r\n    tfidf_text_cause + tfidf_text_coming + tfidf_text_good +  \r\n    tfidf_text_right + tfidf_text_thinking + tfidf_text_dwight +  \r\n    tfidf_text_come + tfidf_text_yes + tfidf_text_people + tfidf_text_andy +  \r\n    tfidf_text_give + tfidf_text_party + tfidf_text_ryan + tfidf_text_today +  \r\n    tfidf_text_oscar + tfidf_text_fun + tfidf_text_guys + tfidf_text_everyone +  \r\n    tfidf_text_see + tfidf_text_away + tfidf_text_go + tfidf_text_sure +  \r\n    tfidf_text_please + tfidf_text_phyllis + tfidf_text_listen +  \r\n    tfidf_text_believe + tfidf_text_told + tfidf_text_name +  \r\n    tfidf_text_whole + tfidf_text_ever + tfidf_text_just + tfidf_text_money +  \r\n    tfidf_text_boss + tfidf_text_ask + tfidf_text_feel + tfidf_text_find +  \r\n    tfidf_text_three + tfidf_text_need + tfidf_text_made + tfidf_text_long +  \r\n    tfidf_text_gonna + tfidf_text_hear + tfidf_text_friends +  \r\n    tfidf_text_wow + tfidf_text_old + tfidf_text_check + tfidf_text_wait +  \r\n    tfidf_text_head + tfidf_text_hold + tfidf_text_look + tfidf_text_talk +  \r\n    tfidf_text_want + tfidf_text_company + tfidf_text_room +  \r\n    tfidf_text_got + tfidf_text_five + tfidf_text_new + tfidf_text_mifflin +  \r\n    tfidf_text_get + tfidf_text_work + tfidf_text_time + tfidf_text_every +  \r\n    tfidf_text_thanks + tfidf_text_one + tfidf_text_lot + tfidf_text_mr +  \r\n    tfidf_text_kevin + tfidf_text_hello + tfidf_text_thought +  \r\n    tfidf_text_stop + tfidf_text_things + tfidf_text_said + tfidf_text_two +  \r\n    tfidf_text_sorry + tfidf_text_never + tfidf_text_called +  \r\n    tfidf_text_oh + tfidf_text_back + tfidf_text_better + tfidf_text_jim +  \r\n    tfidf_text_much + tfidf_text_hi + tfidf_text_guess + tfidf_text_corporate +  \r\n    tfidf_text_care + tfidf_text_day + tfidf_text_kind + tfidf_text_little +  \r\n    tfidf_text_great + tfidf_text_part + tfidf_text_night + tfidf_text_fine +  \r\n    tfidf_text_take + tfidf_text_put + tfidf_text_saying + tfidf_text_office +  \r\n    tfidf_text_actually + tfidf_text_morning + tfidf_text_job +  \r\n    tfidf_text_um + tfidf_text_last + tfidf_text_getting + tfidf_text_around +  \r\n    tfidf_text_trying + tfidf_text_leave + tfidf_text_whoa +  \r\n    tfidf_text_idea + tfidf_text_nothing + tfidf_text_wrong +  \r\n    tfidf_text_went + tfidf_text_help + tfidf_text_first + tfidf_text_love +  \r\n    tfidf_text_us + tfidf_text_even + tfidf_text_cool + tfidf_text_wanna +  \r\n    tfidf_text_home + tfidf_text_anything + tfidf_text_might +  \r\n    tfidf_text_everything + tfidf_text_like + tfidf_text_man +  \r\n    tfidf_text_car + tfidf_text_now + tfidf_text_real + tfidf_text_paper +  \r\n    tfidf_text_still + tfidf_text_second + tfidf_text_done +  \r\n    tfidf_text_happy + tfidf_text_talking + tfidf_text_meet +  \r\n    tfidf_text_really + tfidf_text_place + tfidf_text_something +  \r\n    tfidf_text_call + tfidf_text_sales + tfidf_text_thank + tfidf_text_hot +  \r\n    tfidf_text_yeah + tfidf_text_next + tfidf_text_make + tfidf_text_big +  \r\n    tfidf_text_together + tfidf_text_can + tfidf_text_many +  \r\n    tfidf_text_years + tfidf_text_uh + tfidf_text_think + tfidf_text_ready +  \r\n    tfidf_text_manager + tfidf_text_year + tfidf_text_let + tfidf_text_else +  \r\n    tfidf_text_way + tfidf_text_maybe + tfidf_text_baby + tfidf_text_probably +  \r\n    tfidf_text_huh + tfidf_text_tell + tfidf_text_hey + tfidf_text_wanted +  \r\n    (1 | episode_name)\r\nData: tr_prepped\r\n\r\n     AIC      BIC   logLik deviance df.resid \r\n 21739.8  23257.5 -10672.9  21345.8    16183 \r\n\r\nRandom effects:\r\n\r\nConditional model:\r\n Groups       Name        Variance Std.Dev.\r\n episode_name (Intercept) 0.2519   0.5019  \r\nNumber of obs: 16380, groups:  episode_name, 139\r\n\r\nConditional model:\r\n                       Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)           -0.284909   0.056210  -5.069 4.01e-07 ***\r\ntfidf_text_michael    -1.365494   0.120733 -11.310  < 2e-16 ***\r\ntfidf_text_scott       0.817744   0.174819   4.678 2.90e-06 ***\r\ntfidf_text_friend      0.706557   0.205575   3.437 0.000588 ***\r\ntfidf_text_stanley     0.537424   0.139759   3.845 0.000120 ***\r\ntfidf_text_jan         0.504584   0.119162   4.234 2.29e-05 ***\r\ntfidf_text_everybody   0.520978   0.132091   3.944 8.01e-05 ***\r\ntfidf_text_business    0.563248   0.143005   3.939 8.19e-05 ***\r\ntfidf_text_may         0.503039   0.173855   2.893 0.003810 ** \r\ntfidf_text_bad         0.471181   0.146416   3.218 0.001290 ** \r\ntfidf_text_pretty     -0.412679   0.163791  -2.520 0.011751 *  \r\ntfidf_text_say         0.404528   0.085141   4.751 2.02e-06 ***\r\ntfidf_text_dunder      0.556549   0.615429   0.904 0.365822    \r\ntfidf_text_god         0.352119   0.081974   4.295 1.74e-05 ***\r\ntfidf_text_angela     -0.338799   0.125293  -2.704 0.006850 ** \r\ntfidf_text_scranton    0.323568   0.130686   2.476 0.013289 *  \r\ntfidf_text_somebody    0.282980   0.121483   2.329 0.019839 *  \r\ntfidf_text_pam         0.319905   0.072147   4.434 9.25e-06 ***\r\ntfidf_text_ah          0.309050   0.086264   3.583 0.000340 ***\r\ntfidf_text_best        0.332800   0.139941   2.378 0.017400 *  \r\ntfidf_text_going       0.297253   0.067610   4.397 1.10e-05 ***\r\ntfidf_text_show        0.350289   0.150457   2.328 0.019903 *  \r\ntfidf_text_life        0.309884   0.136512   2.270 0.023207 *  \r\ntfidf_text_guy         0.311064   0.110614   2.812 0.004921 ** \r\ntfidf_text_ok          0.272147   0.054382   5.004 5.60e-07 ***\r\ntfidf_text_thing       0.270642   0.103021   2.627 0.008613 ** \r\ntfidf_text_well        0.284100   0.056668   5.013 5.35e-07 ***\r\ntfidf_text_alright     0.281422   0.062810   4.481 7.45e-06 ***\r\ntfidf_text_holly       0.303695   0.109919   2.763 0.005729 ** \r\ntfidf_text_mean       -0.300037   0.088790  -3.379 0.000727 ***\r\ntfidf_text_know        0.273930   0.056596   4.840 1.30e-06 ***\r\ntfidf_text_okay        0.307069   0.044521   6.897 5.30e-12 ***\r\ntfidf_text_toby        0.249094   0.108059   2.305 0.021157 *  \r\ntfidf_text_cause      -0.248942   0.149764  -1.662 0.096469 .  \r\ntfidf_text_coming     -0.290930   0.145258  -2.003 0.045193 *  \r\ntfidf_text_good        0.268924   0.057343   4.690 2.74e-06 ***\r\ntfidf_text_right       0.259742   0.055691   4.664 3.10e-06 ***\r\ntfidf_text_thinking    0.232674   0.121078   1.922 0.054645 .  \r\ntfidf_text_dwight      0.253907   0.063727   3.984 6.77e-05 ***\r\ntfidf_text_come        0.203893   0.071037   2.870 0.004102 ** \r\ntfidf_text_yes         0.248846   0.040821   6.096 1.09e-09 ***\r\ntfidf_text_people      0.222956   0.105130   2.121 0.033942 *  \r\ntfidf_text_andy       -0.157914   0.089205  -1.770 0.076687 .  \r\ntfidf_text_give        0.205885   0.083517   2.465 0.013694 *  \r\ntfidf_text_party       0.194342   0.093019   2.089 0.036683 *  \r\ntfidf_text_ryan        0.188932   0.096564   1.957 0.050401 .  \r\ntfidf_text_today       0.214202   0.112770   1.899 0.057505 .  \r\ntfidf_text_oscar       0.222130   0.100434   2.212 0.026988 *  \r\ntfidf_text_fun         0.189363   0.091348   2.073 0.038174 *  \r\ntfidf_text_guys        0.165921   0.080568   2.059 0.039456 *  \r\ntfidf_text_everyone   -0.168222   0.125141  -1.344 0.178862    \r\ntfidf_text_see         0.187325   0.073920   2.534 0.011271 *  \r\ntfidf_text_away        0.161108   0.143229   1.125 0.260664    \r\ntfidf_text_go          0.220001   0.061381   3.584 0.000338 ***\r\ntfidf_text_sure       -0.159186   0.074734  -2.130 0.033169 *  \r\ntfidf_text_please      0.161400   0.072772   2.218 0.026563 *  \r\ntfidf_text_phyllis     0.141596   0.095280   1.486 0.137251    \r\ntfidf_text_listen     -0.134684   0.153642  -0.877 0.380700    \r\ntfidf_text_believe     0.194606   0.115799   1.681 0.092850 .  \r\ntfidf_text_told        0.122873   0.104977   1.170 0.241808    \r\ntfidf_text_name        0.186708   0.094160   1.983 0.047382 *  \r\ntfidf_text_whole      -0.130371   0.136622  -0.954 0.339958    \r\ntfidf_text_ever        0.171535   0.102233   1.678 0.093369 .  \r\ntfidf_text_just        0.127548   0.064389   1.981 0.047604 *  \r\ntfidf_text_money      -0.161886   0.134932  -1.200 0.230233    \r\ntfidf_text_boss        0.153626   0.151328   1.015 0.310019    \r\ntfidf_text_ask         0.123247   0.141323   0.872 0.383154    \r\ntfidf_text_feel        0.128474   0.129301   0.994 0.320418    \r\ntfidf_text_find        0.164066   0.101733   1.613 0.106806    \r\ntfidf_text_three      -0.140787   0.108584  -1.297 0.194780    \r\ntfidf_text_need        0.162706   0.083187   1.956 0.050476 .  \r\ntfidf_text_made        0.123781   0.126288   0.980 0.327016    \r\ntfidf_text_long       -0.117803   0.138038  -0.853 0.393434    \r\ntfidf_text_gonna      -0.143375   0.076708  -1.869 0.061610 .  \r\ntfidf_text_hear        0.104088   0.102352   1.017 0.309171    \r\ntfidf_text_friends     0.084551   0.143475   0.589 0.555655    \r\ntfidf_text_wow         0.150225   0.064220   2.339 0.019324 *  \r\ntfidf_text_old         0.106273   0.116189   0.915 0.360374    \r\ntfidf_text_check       0.114413   0.098850   1.157 0.247094    \r\ntfidf_text_wait       -0.126842   0.081461  -1.557 0.119448    \r\ntfidf_text_head        0.126518   0.126729   0.998 0.318118    \r\ntfidf_text_hold        0.141848   0.116016   1.223 0.221459    \r\ntfidf_text_look        0.128788   0.071782   1.794 0.072789 .  \r\ntfidf_text_talk        0.121851   0.087935   1.386 0.165843    \r\ntfidf_text_want        0.116503   0.065304   1.784 0.074422 .  \r\ntfidf_text_company     0.147109   0.130804   1.125 0.260739    \r\ntfidf_text_room        0.107632   0.107204   1.004 0.315382    \r\ntfidf_text_got        -0.122130   0.067140  -1.819 0.068907 .  \r\ntfidf_text_five       -0.096512   0.094074  -1.026 0.304935    \r\ntfidf_text_new        -0.065516   0.123041  -0.532 0.594396    \r\ntfidf_text_mifflin    -0.294953   0.601955  -0.490 0.624139    \r\ntfidf_text_get         0.126518   0.064409   1.964 0.049496 *  \r\ntfidf_text_work        0.110293   0.100668   1.096 0.273247    \r\ntfidf_text_time        0.164705   0.082474   1.997 0.045820 *  \r\ntfidf_text_every       0.140575   0.158257   0.888 0.374395    \r\ntfidf_text_thanks     -0.101296   0.074035  -1.368 0.171245    \r\ntfidf_text_one        -0.079519   0.073817  -1.077 0.281369    \r\ntfidf_text_lot         0.089214   0.105700   0.844 0.398651    \r\ntfidf_text_mr         -0.114169   0.106592  -1.071 0.284131    \r\ntfidf_text_kevin       0.114483   0.083866   1.365 0.172233    \r\ntfidf_text_hello       0.107170   0.066524   1.611 0.107179    \r\ntfidf_text_thought    -0.115780   0.096173  -1.204 0.228640    \r\ntfidf_text_stop        0.112427   0.067616   1.663 0.096364 .  \r\ntfidf_text_things      0.092322   0.123791   0.746 0.455795    \r\ntfidf_text_said        0.111146   0.068770   1.616 0.106052    \r\ntfidf_text_two        -0.064680   0.094019  -0.688 0.491488    \r\ntfidf_text_sorry      -0.101457   0.077747  -1.305 0.191906    \r\ntfidf_text_never       0.101368   0.087415   1.160 0.246206    \r\ntfidf_text_called     -0.091985   0.127479  -0.722 0.470560    \r\ntfidf_text_oh          0.089262   0.047454   1.881 0.059966 .  \r\ntfidf_text_back       -0.078854   0.087660  -0.900 0.368366    \r\ntfidf_text_better     -0.060033   0.108630  -0.553 0.580509    \r\ntfidf_text_jim        -0.095686   0.065332  -1.465 0.143029    \r\ntfidf_text_much        0.076737   0.087948   0.873 0.382923    \r\ntfidf_text_hi         -0.088520   0.073867  -1.198 0.230775    \r\ntfidf_text_guess      -0.107766   0.127102  -0.848 0.396510    \r\ntfidf_text_corporate  -0.050761   0.132030  -0.384 0.700633    \r\ntfidf_text_care       -0.038296   0.138086  -0.277 0.781525    \r\ntfidf_text_day         0.096423   0.104991   0.918 0.358413    \r\ntfidf_text_kind        0.071155   0.102769   0.692 0.488699    \r\ntfidf_text_little      0.124122   0.100206   1.239 0.215468    \r\ntfidf_text_great      -0.073854   0.069929  -1.056 0.290912    \r\ntfidf_text_part        0.030885   0.103786   0.298 0.766023    \r\ntfidf_text_night      -0.086267   0.119819  -0.720 0.471540    \r\ntfidf_text_fine       -0.076178   0.084942  -0.897 0.369812    \r\ntfidf_text_take       -0.055703   0.083438  -0.668 0.504392    \r\ntfidf_text_put        -0.099941   0.093066  -1.074 0.282877    \r\ntfidf_text_saying     -0.071405   0.108057  -0.661 0.508738    \r\ntfidf_text_office      0.066885   0.097381   0.687 0.492187    \r\ntfidf_text_actually    0.069965   0.105800   0.661 0.508421    \r\ntfidf_text_morning     0.136949   0.115285   1.188 0.234866    \r\ntfidf_text_job        -0.020755   0.145005  -0.143 0.886184    \r\ntfidf_text_um          0.049639   0.073370   0.677 0.498685    \r\ntfidf_text_last       -0.050689   0.122785  -0.413 0.679732    \r\ntfidf_text_getting     0.089356   0.104741   0.853 0.393598    \r\ntfidf_text_around      0.082025   0.147866   0.555 0.579081    \r\ntfidf_text_trying      0.073053   0.123007   0.594 0.552583    \r\ntfidf_text_leave      -0.078307   0.105481  -0.742 0.457857    \r\ntfidf_text_whoa        0.081381   0.088434   0.920 0.357445    \r\ntfidf_text_idea       -0.089929   0.091961  -0.978 0.328122    \r\ntfidf_text_nothing     0.063688   0.086331   0.738 0.460687    \r\ntfidf_text_wrong       0.086905   0.092908   0.935 0.349586    \r\ntfidf_text_went       -0.028418   0.125718  -0.226 0.821166    \r\ntfidf_text_help        0.040718   0.100003   0.407 0.683888    \r\ntfidf_text_first       0.086366   0.122036   0.708 0.479127    \r\ntfidf_text_love        0.071734   0.068545   1.047 0.295317    \r\ntfidf_text_us         -0.040849   0.097550  -0.419 0.675399    \r\ntfidf_text_even       -0.038807   0.113579  -0.342 0.732596    \r\ntfidf_text_cool       -0.049036   0.083179  -0.590 0.555514    \r\ntfidf_text_wanna      -0.048928   0.105429  -0.464 0.642585    \r\ntfidf_text_home       -0.048600   0.144852  -0.336 0.737238    \r\ntfidf_text_anything   -0.016118   0.103091  -0.156 0.875756    \r\ntfidf_text_might      -0.046693   0.133043  -0.351 0.725615    \r\ntfidf_text_everything  0.091705   0.119419   0.768 0.442529    \r\ntfidf_text_like        0.051476   0.060225   0.855 0.392698    \r\ntfidf_text_man        -0.048323   0.088421  -0.547 0.584717    \r\ntfidf_text_car        -0.021845   0.099341  -0.220 0.825948    \r\ntfidf_text_now        -0.040680   0.084568  -0.481 0.630495    \r\ntfidf_text_real        0.052797   0.153645   0.344 0.731124    \r\ntfidf_text_paper       0.023861   0.107711   0.222 0.824683    \r\ntfidf_text_still       0.074397   0.094324   0.789 0.430265    \r\ntfidf_text_second     -0.053789   0.125801  -0.428 0.668965    \r\ntfidf_text_done        0.035836   0.087322   0.410 0.681526    \r\ntfidf_text_happy       0.038974   0.088629   0.440 0.660128    \r\ntfidf_text_talking    -0.024875   0.072337  -0.344 0.730941    \r\ntfidf_text_meet        0.064215   0.119743   0.536 0.591768    \r\ntfidf_text_really     -0.024577   0.055026  -0.447 0.655128    \r\ntfidf_text_place       0.038420   0.114313   0.336 0.736798    \r\ntfidf_text_something   0.017752   0.099054   0.179 0.857767    \r\ntfidf_text_call       -0.035360   0.080138  -0.441 0.659038    \r\ntfidf_text_sales      -0.013491   0.094396  -0.143 0.886355    \r\ntfidf_text_thank       0.042518   0.053183   0.799 0.424022    \r\ntfidf_text_hot        -0.003221   0.107750  -0.030 0.976151    \r\ntfidf_text_yeah       -0.023379   0.039966  -0.585 0.558559    \r\ntfidf_text_next        0.009379   0.138893   0.068 0.946165    \r\ntfidf_text_make       -0.009311   0.089531  -0.104 0.917168    \r\ntfidf_text_big         0.046667   0.091438   0.510 0.609791    \r\ntfidf_text_together   -0.021054   0.150126  -0.140 0.888470    \r\ntfidf_text_can         0.048398   0.067522   0.717 0.473510    \r\ntfidf_text_many        0.036402   0.128211   0.284 0.776468    \r\ntfidf_text_years      -0.003345   0.126027  -0.027 0.978824    \r\ntfidf_text_uh          0.033145   0.065356   0.507 0.612051    \r\ntfidf_text_think       0.039276   0.061806   0.635 0.525120    \r\ntfidf_text_ready       0.004309   0.082615   0.052 0.958406    \r\ntfidf_text_manager    -0.005744   0.104443  -0.055 0.956138    \r\ntfidf_text_year       -0.004601   0.122379  -0.038 0.970012    \r\ntfidf_text_let        -0.010076   0.105709  -0.095 0.924062    \r\ntfidf_text_else        0.036421   0.120693   0.302 0.762831    \r\ntfidf_text_way        -0.015943   0.086084  -0.185 0.853072    \r\ntfidf_text_maybe       0.014926   0.078838   0.189 0.849838    \r\ntfidf_text_baby        0.038613   0.100086   0.386 0.699647    \r\ntfidf_text_probably    0.012967   0.141661   0.092 0.927068    \r\ntfidf_text_huh         0.011606   0.088695   0.131 0.895889    \r\ntfidf_text_tell        0.025608   0.073891   0.347 0.728918    \r\ntfidf_text_hey         0.016844   0.043492   0.387 0.698544    \r\ntfidf_text_wanted     -0.003270   0.119500  -0.027 0.978169    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nRight, so, the next logical step in my mind is to take a closer look at the random intercepts. We see some variance in the intercept (.23), which suggests that there are meaningful between-episode differences in the number of times Michael Scott speaks. Rather than looking at all of these, let’s take a look at the largest 10 effects (as a benchmark, recall that the mean intercept is -.3)\r\n\r\n\r\nranef(glmm_fit) %>%\r\n  as.data.frame() %>%\r\n  select(grp, condval) %>%\r\n  slice_max(order_by = abs(condval), n = 10) %>%\r\n  ggplot(aes(x = abs(condval), y = fct_reorder(grp, abs(condval)), fill = if_else(condval > 0, \"Pos\", \"Neg\"))) +\r\n  geom_col() +\r\n  scale_fill_discrete(name = \"Sign\") +\r\n  labs(\r\n    y = NULL,\r\n    title = \"Top Random Intercepts\"\r\n  )\r\n\r\n\r\n\r\n\r\nThis plot shows the largest (in absolute value) intercepts. The way to interpret this is that, in these episodes, Michael is more or less likely to speak. The effects of each of the words remains the same across episodes (since I didn’t specify random slopes), but these change the assumed “base rate” that Michael speaks. What we see here makes sense, because Michael actually isn’t in the three episodes that have the highest values here (I should have addressed this in data cleaning – whoops!).\r\nFinally, I’ll take a look at the accuracy of the predictions from the multilevel model.\r\n\r\n\r\nglmm_preds_response <- predict(glmm_fit, te_prepped, type = \"response\")\r\nglmm_preds <- ifelse(glmm_preds_response < .5, \"No\", \"Yes\") %>% as_factor() %>%\r\n  fct_relevel(\"No\", \"Yes\")\r\nbind_cols(te_prepped$is_mike, glmm_preds) %>%\r\n  repair_names() %>%\r\n  accuracy(truth = ...1, estimate = ...2)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.596\r\n\r\nIt’s a little bit disappointing that the multilevel model isn’t more accurate than the single-level model I ran previously, but one thing to keep in mind is that the single level model was regularized, whereas the multilevel model wasn’t (beyond omitting the variables that got completely omitted from the single level model). So, even though our intercept seems to have a decent amount of variance – meaning random effects are probably warranted – the gains in predictive accuracy we’d get from that are more than offset by the regularization in the first model. There’s probably a way to regularize a multilevel model, but I might save that one for another day. I could also play around with changing the probability threshold for classifying a line as Michael by setting it to something higher than 50% (e.g. a line needs to have a 70% probability before being classified as spoken by Michael), but I’m also not going to go down that rabbit hole here.\r\nSo, I’m going to wrap it up for now. And who knows, maybe I’ll revisit this dataset in another 4 months.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-11-scrantonicity-part-3/scrantonicity-part-3_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T08:24:32-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-11-intro-to-rvest/",
    "title": "Intro to {rvest}",
    "description": "Using {rvest} to find child care in Virginia.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-07-24",
    "categories": [],
    "contents": "\r\nTL;DR – {rvest} is awesome. Before yesterday, I had 0 experience web scraping and very little experience with HTML/CSS in general, and in a couple of hours, I managed to submit a form and scrape the resulting tables. And it would have been even less time if I weren’t a dummy a remembered that NULL is a thing…more on that later.\r\nMotivation\r\nYesterday, for work, I needed to get a list of all of the Family Day Homes (FDH) in Virginia to check against another dataset. For those not in the early childhood game, FDHs are childcare programs run out of the provider’s home. During the COVID epidemic, there’s been even more attention on them because they typically have smaller group sizes than schools or childcare centers, and so may be a more durable option for many families. But that’s not really the point of this post.\r\nAnyway, I needed to get this list of FDHs. Normally, I’d reach out to someone at the Department of Social Services (DSS), who is responsible for licensing childcare in Virginia, to ask for this. But I needed it last night, outside of normal working hours. I knew their website has a search function, and so I decided to take a stab at scraping the webpage to get the info I needed. Since it worked out pretty well, I also figured I’d write a short blog post about it in case that can help others navigate webscraping for the first time.\r\nDisclaimer\r\nI am not a web developer. I know a handful of the more common HTML tags, and I know enough CSS to make Rmd reports look a little prettier, but that’s basically it. I’m not going to pretend I understand all of what {rvest} does under the hood, but I also think that’s kinda the point of the package. With that out of the way, onward and upward!\r\nSetup\r\n\r\n\r\n\r\nFirst, let’s go visit the web page itself. This page is where families can search for child care centers in the DSS database. Here’s the relevant part of the page, including the form we’ll be using to search:\r\n\r\nI’m going to capture this using the read_html() function from {rvest}.\r\n\r\n\r\ndss <- read_html(\"https://www.dss.virginia.gov/facility/search/cc2.cgi\")\r\n\r\n\r\n\r\nNext, I need to isolate this form in the page. I’m going to do this with a combination of the html_nodes() and html_form() functions, plus extract2() from {magrittr}.\r\n\r\n\r\ndss_form <- dss %>% \r\n  html_nodes(\"form\") %>% \r\n  magrittr::extract2(2) %>% \r\n  html_form()\r\n\r\n\r\n\r\nLet’s walk through this. First, we pipe the dss object into html_nodes(), which will – in this case – extract all of the “form” elements from the page. Note that I’m using html_nodes() rather than html_node() here – this is because the form I want is actually the 2nd one on the page, so I get both of them and then extract the second one using magrittr::extract2(). Next, I pass that into html_form, which does some voodoo to tell R that this is a form.\r\nWhen we take a peek at the form, here’s what we see:\r\n\r\n\r\ndss_form\r\n\r\n\r\n<form> '<unnamed>' (POST /facility/search/cc2.cgi)\r\n  <input hidden> 'rm': Search\r\n  <input text> 'search_keywords_name': \r\n  <select> 'search_exact_fips' [0/135]\r\n  <input text> 'search_contains_zip': \r\n  <input checkbox> 'search_modifiers_mod_cde': 1\r\n  <input checkbox> 'search_quality_rating_1': 1\r\n  <input checkbox> 'search_quality_rating_2': 2\r\n  <input checkbox> 'search_quality_rating_3': 3\r\n  <input checkbox> 'search_quality_rating_4': 4\r\n  <input checkbox> 'search_quality_rating_5': 5\r\n  <input checkbox> 'search_quality_rating_all': 1,2,3,4,5\r\n  <input checkbox> 'search_require_client_code-2101': 1\r\n  <input checkbox> 'search_require_client_code-2102': 1\r\n  <input checkbox> 'search_require_client_code-2106': 1\r\n  <input checkbox> 'search_require_client_code-2105': 1\r\n  <input checkbox> 'search_require_client_code-2201': 1\r\n  <input checkbox> 'search_require_client_code-2104': 1\r\n  <input checkbox> 'search_require_client_code-3001': 1\r\n  <input checkbox> 'search_require_client_code-3002': 1\r\n  <input checkbox> 'search_require_client_code-3003': 1\r\n  <input checkbox> 'search_require_client_code-3004': 1\r\n  <input submit> '': Search\r\n\r\nIf you look back up at the screenshot of the page (or if you visit the actual page), you’ll notice that the input elements here are the things we can search by. Well, you might not notice because the “search_require_client_code-2102” element doesn’t scream that this is the checkbox for Family Day Home, but it is.\r\nWhat I did at this point was use the Inspector tool in Firefox to figure out which of these elements I want to select. This took more time and more submissions that returned the wrong values than I care to admit. It turns out that the relevant elements to select FDHs are 2102, 2201, and 3002.\r\n\r\nCool, so now we need to set the values of the elements in this form. I’m not sure if the default behavior for all checkboxes is to be checked in a form, but these are (which you can see by the value of 1 for all of them). This was not intuitive to me. Even less intuitive was how to uncheck them. It turns out that the way to do it is to set the value to NULL. For whatever reason, this also wasn’t intuitive to me – maybe because it was late at night, who knows. I tried 0s, empty strings, lots of different stuff before NULL saved me.\r\nRegardless, you can set the values in the form using the set_values() function. The code below will set the values of everything to NULL except for the checkboxes that correspond to family day homes. The object this returns is an updated form.\r\n\r\n\r\nfdh_val <- set_values(dss_form,\r\n                      `search_modifiers_mod_cde` = NULL,\r\n                      `search_quality_rating_1` = NULL,\r\n                      `search_quality_rating_2` = NULL,\r\n                      `search_quality_rating_3` = NULL,\r\n                      `search_quality_rating_4` = NULL,\r\n                      `search_quality_rating_5` = NULL,\r\n                      `search_quality_rating_all` = NULL,\r\n                      `search_require_client_code-2101` = NULL,\r\n                      `search_require_client_code-2102` = 1,\r\n                      `search_require_client_code-2106` = NULL,\r\n                      `search_require_client_code-2105` = NULL,\r\n                      `search_require_client_code-2201` = 1,\r\n                      `search_require_client_code-2104` = NULL,\r\n                      `search_require_client_code-3001` = NULL,\r\n                      `search_require_client_code-3002` = 1,\r\n                      `search_require_client_code-3003` = NULL,\r\n                      `search_require_client_code-3004` = NULL)\r\n\r\n\r\n\r\nNext, we need a way to submit the form, which apparently requires an html session. My understanding of html sessions is that they store data/values in the browser window temporarily – until it’s closed. This is pretty much what I know about them. But apparently we need one to submit a form, so here we go.\r\nAnd once the session is started, I’m going to submit the form – using submit_form() – with the updated values created just above and save the output in subbed. The submit_form function gives you an option to specify which submit button to use on the page if there are multiple, but that’s not an issue here.\r\n\r\n\r\ndss_session <- html_session(\"https://www.dss.virginia.gov/facility/search/cc2.cgi\")\r\nsubbed <- submit_form(dss_session, fdh_val)\r\n\r\n\r\n\r\nAfter submitting the form, the data will be populated in some new tables, so the next step is to extract these tables from the subbed session object. I’m using the html_nodes() function again but with the “tables” argument to pull all of the tables from the session.\r\n\r\n\r\ntbls <- subbed %>% html_nodes(\"table\")\r\nlength(tbls)\r\n\r\n\r\n[1] 4\r\n\r\nThere are 4 tables. I looked at these all using View() (e.g. View(tbls[[1]]) etc) and figured out that the ones I want are tbls 3 and 4. These correspond to licensed and unlicensed FDHs, respectively. To get these out, I’m going to use the html_table() function on each, which creates a tibble from the html table.\r\n\r\n\r\nfdh_tbl <- bind_rows(tbls[[3]] %>% html_table(),\r\n                       tbls[[4]] %>% html_table())\r\nglimpse(fdh_tbl)\r\n\r\n\r\nRows: 2,055\r\nColumns: 3\r\n$ Test...1 <chr> \"Abida Mufti\\n\\t\\t\\tVirginia Quality Level:\", \"A...\r\n$ Test...2 <chr> \"2201 Hunter Mill Road\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n...\r\n$ Test...3 <chr> \"(703) 281-7860\", \"(703) 360-6783\", \"(703) 435-4...\r\n\r\nThis gives us a three column table. The first column has the name of the FDH – which is typically just the name of the person who owns it – as well as an indicator of the program’s Virginia Quality level (which is our quality rating system for childcare). The second has the address. And the third column has the phone number. Doing a little bit of cleaning and column naming, we can get a nicer tibble to work with\r\n\r\n\r\nnames(fdh_tbl) <- c(\"name\", \"address\", \"phone\")\r\nfdh_tbl_clean <- fdh_tbl %>%\r\n  mutate(name = str_remove_all(name, \"\\\\\\n.*$\"),\r\n         address = str_replace(address, \"\\\\\\n\", \" \") %>%\r\n           str_remove_all(\"\\\\\\n|\\\\\\t\") %>%\r\n           str_to_title()\r\n         )\r\nglimpse(fdh_tbl_clean)\r\n\r\n\r\nRows: 2,055\r\nColumns: 3\r\n$ name    <chr> \"Abida Mufti\", \"Abida Munir\", \"Ada Lazo\", \"Adeela...\r\n$ address <chr> \"2201 Hunter Mill Road Vienna, Va  22181\", \"1902 ...\r\n$ phone   <chr> \"(703) 281-7860\", \"(703) 360-6783\", \"(703) 435-45...\r\n\r\nEt voila!\r\nThis got me to where I needed to be for my work project, but a next step here might be to geocode the addresses and then make a map. I’ll probably do this in the future as part of a different post, but I’m going to leave off the current post here for now. Hopefully this helps people with their first venture into web scraping! I know I’m excited to do more with it now that I’ve dipped my toes in.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-06-unconsciousness-in-the-xmen/",
    "title": "Unconsciousness in the Xmen",
    "description": "Practicing poisson regression using Xmen data.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-07-14",
    "categories": [],
    "contents": "\r\nA part of me has always wanted to get into comic books. I think it would be a really good fit for me – I’m definitely a nerd. I play video games, I read fantasy novels, I code/do data science for fun. Comic books should be right up my alley. But for whatever reason, I’ve never taken the plunge. Maybe it’s a time commitment thing. Maybe I know I’ll like them too much. Maybe it’s too daunting to figure out how to start. Regardless, even thought I’m not into comic books, they are intriguing to me, and the X-Men particularly so, which is why I wanted to take a little bit of time to analyze this X-men data promoted by the #tidytuesday project.\r\nThe other main purpose of this blog post is to toy around with running a Poisson regression. A few months ago, I saw a post about how the tidymodels framework had some new “parsnip-adjacent” packages, with one being {poissonreg} which fits – you guessed it – Poisson regressions. I haven’t had much reason to use Poisson regression in any of my previous work or in datasets I’ve toyed around with, but this X-men dataset seems like a good excuse to try it out. So, onward and upward!\r\nSetup\r\nFirst, I’ll load some packages, set some miscellaneous options, and import the data. This data comes from the Claremont Run project, which mines data from Chris Claremont’s run (1975-1991) writing the X-men comics. To learn more about the project, you can visit the website. There are several datasets available, but for this analysis, I’m going to use data from the characters dataset, the character_visualization dataset, and the locations dataset.\r\n\r\n\r\n\r\nExploring the Data\r\nLet’s first look at the characters dataset. In this dataset, each row corresponds to a character in an issue, and each column corresponds to actions or events relevant to that character. Here’s a glimpse of that data:\r\n\r\n\r\ncharacters %>%\r\n  glimpse()\r\n\r\n\r\nRows: 4,209\r\nColumns: 34\r\n$ issue                                         <dbl> 97, 97, 97,...\r\n$ character                                     <chr> \"Professor ...\r\n$ rendered_unconcious                           <dbl> 0, 0, 0, 1,...\r\n$ captured                                      <dbl> 0, 0, 0, 0,...\r\n$ declared_dead                                 <dbl> 0, 0, 0, 0,...\r\n$ redressed                                     <dbl> 0, 0, 0, 0,...\r\n$ depowered                                     <dbl> 0, 0, 0, 0,...\r\n$ clothing_torn                                 <dbl> 0, 0, 0, 0,...\r\n$ subject_to_torture                            <dbl> 0, 0, 0, 0,...\r\n$ quits_team                                    <dbl> 0, 0, 0, 0,...\r\n$ surrenders                                    <dbl> 0, 0, 0, 0,...\r\n$ number_of_kills_humans                        <dbl> 0, 0, 0, 0,...\r\n$ number_of_kills_non_humans                    <dbl> 0, 0, 0, 0,...\r\n$ initiates_physical_conflict                   <chr> NA, NA, \"1\"...\r\n$ expresses_reluctance_to_fight                 <dbl> NA, NA, 1, ...\r\n$ on_a_date_with_which_character                <chr> NA, NA, NA,...\r\n$ kiss_with_which_character                     <chr> NA, NA, NA,...\r\n$ hand_holding_with_which_character             <chr> \"Moira MacT...\r\n$ dancing_with_which_character                  <chr> NA, NA, NA,...\r\n$ flying_with_another_character                 <chr> NA, NA, NA,...\r\n$ arm_in_arm_with_which_character               <chr> NA, NA, NA,...\r\n$ hugging_with_which_character                  <chr> NA, NA, NA,...\r\n$ physical_contact_other                        <chr> \"Moira MacT...\r\n$ carrying_with_which_character                 <chr> NA, NA, NA,...\r\n$ shared_bed_with_which_character               <lgl> NA, NA, NA,...\r\n$ shared_room_domestically_with_which_character <lgl> NA, NA, NA,...\r\n$ explicitly_states_i_love_you_to_whom          <chr> NA, NA, NA,...\r\n$ shared_undress                                <chr> NA, NA, NA,...\r\n$ shower_number_of_panels_shower_lasts          <dbl> 0, 0, 0, 0,...\r\n$ bath_number_of_panels_bath_lasts              <dbl> 0, 0, 0, 0,...\r\n$ depicted_eating_food                          <dbl> 1, 0, 0, 0,...\r\n$ visible_tears_number_of_panels                <dbl> 0, 0, 0, 0,...\r\n$ visible_tears_number_of_intances              <dbl> 0, 0, 0, 0,...\r\n$ special_notes                                 <chr> NA, NA, NA,...\r\n\r\nSo, we can see in this dataset things like who Professor X held hands with in issue 97, how many humans were killed by Magneto in issue 105, etc. We see lots of NAs and 0s in this dataset. The only column I’m going to use from this is the rendered unconscious column, which will be outcome variable in the models later.\r\nIn the character_visualization dataset, each row represents a per-issue count of the number of times a character is depicted, speaks, thinks, has a narrative statement (I think this is probably only relevant for the narrator character?), either when the character is in costume or not in costume.\r\n\r\n\r\ncharacter_visualization %>%\r\n  glimpse()\r\n\r\n\r\nRows: 9,800\r\nColumns: 7\r\n$ issue     <dbl> 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...\r\n$ costume   <chr> \"Costume\", \"Costume\", \"Costume\", \"Costume\", \"Co...\r\n$ character <chr> \"Editor narration\", \"Omnipresent narration\", \"P...\r\n$ speech    <dbl> 0, 0, 0, 7, 24, 0, 11, 9, 10, 0, 0, 0, 0, 0, 0,...\r\n$ thought   <dbl> 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\r\n$ narrative <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\r\n$ depicted  <dbl> 0, 0, 0, 10, 23, 0, 9, 17, 17, 5, 0, 0, 0, 0, 0...\r\n\r\nIn the location dataset, each row corresponds to a location in which part of the issue takes place, with as many locations listed per issue as appear in that issue. The dataset also includes a “context” column that describes things like whether the location is shown in the present, as part of a flashback, in a dream, etc. Here’s a glimpse:\r\n\r\n\r\nlocations %>%\r\n  glimpse()\r\n\r\n\r\nRows: 1,413\r\nColumns: 4\r\n$ issue    <dbl> 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 99, 99, ...\r\n$ location <chr> \"Space\", \"X-Mansion\", \"Rio Diablo Research Facil...\r\n$ context  <chr> \"Dream\", \"Present\", \"Present\", \"Present\", \"Prese...\r\n$ notes    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Cut...\r\n\r\nAcross these datasets, it probably makes the most sense to aggegrate data up to the issue level, since that’s kind of the lowest common denominator here. So, essentially the question I’m going to try to answer in this blog post is:\r\nWhat features of an X-men issue predict how many characters are rendered unconscious in that issue?\r\nFirst, let’s look at the distribution of rendered unconscious:\r\n\r\n\r\ncharacters %>%\r\n  count(issue, wt = rendered_unconcious, sort = TRUE) %>%\r\n  ggplot(aes(x = n)) +\r\n  geom_histogram(fill = lann, bins = 8)\r\n\r\n\r\n\r\n\r\nRight, so, this is a pretty strongly right-skewed distribution, which is sort of what we’d expect from a Poisson distribution, especially one with a low expected number of events (which I’d imagine is the case in comic books).\r\nCleaning, Aggregating, and Joining\r\nNext, let’s aggregate our data up to the issue level. This will give us data where a row represents an issue rather than a character within an issue or a location within an issue. We’ll start with the characters dataset. There’s a lot we could do with this data, but because there are only 183 issues represented in this dataset, we need to be cognizant about how many predictors we’re including. So the only variable I’m going to use here is rendered unconscious as the outcome, which will represent the number of characters rendered unconscious in a given issue.\r\n\r\n\r\nrend_df <- characters %>%\r\n  group_by(issue) %>%\r\n  summarize(rendered_unconscious = sum(rendered_unconcious, na.rm = FALSE))\r\n\r\n\r\n\r\nNext, let’s work on the character_visualization dataset. Again, trying to keep the number of predictors relatively small, I’m going to winnow this down to represent counts of how many times a handful of key characters are depicted in each issue. I don’t know a ton about the X-men, but I know who some of the more important characters are, so I’m going to choose Wolverine, Professor X, Magneto, and Jean Grey here.\r\n\r\n\r\nchar_sum <- character_visualization %>%\r\n  filter(str_detect(character, \"Wolverine|Xavier|Jean Grey|Magneto\")) %>%\r\n  group_by(issue, character) %>%\r\n  summarize(depict = sum(depicted, na.rm = FALSE)) %>%\r\n  mutate(character = case_when(\r\n    str_detect(character, \"Jean Grey\") ~ \"Jean_Grey\",\r\n    str_detect(character, \"Wolv\") ~ \"Wolverine\",\r\n    str_detect(character, \"Magneto\") ~ \"Magneto\",\r\n    str_detect(character, \"Xavier\") ~ \"Professor_X\"\r\n  )) %>%\r\n  pivot_wider(\r\n    names_from = character,\r\n    values_from = depict\r\n  )\r\n\r\n\r\n\r\nNext, let’s work on our locations dataset. First, let’s look at the most common locations. Again, since we only have 183 rows in our dataset that we’re modeling with, I only want to choose a handful of variables to include in the model here.\r\n\r\n\r\nlocations %>%\r\n  count(location, sort = TRUE)\r\n\r\n\r\n# A tibble: 785 x 2\r\n   location                             n\r\n   <chr>                            <int>\r\n 1 X-Mansion                          100\r\n 2 Danger Room                         27\r\n 3 Space                               19\r\n 4 Muir Island, Scotland               14\r\n 5 Unspecified region in Australia     14\r\n 6 Eagle Plaza, Dallas Texas           11\r\n 7 Central Park                        10\r\n 8 Morlock residence under New York    10\r\n 9 Princess Lilandra's Home Planet     10\r\n10 San Francisco                       10\r\n# ... with 775 more rows\r\n\r\nOk, so, I’m just going to go with the 3 most common locations: the X-mansion, the Danger Room (whatever that is), and Space. Danger Room sounds to me like a place where people might be rendered unconscious.\r\n\r\n\r\nuse_locs <- locations %>%\r\n  count(location, sort = TRUE) %>%\r\n  top_n(3) %>%\r\n  pull(location)\r\nlocs_sum <- locations %>%\r\n  group_by(issue) %>%\r\n  summarize(mansion = use_locs[[1]] %in% location,\r\n            danger_room = use_locs[[2]] %in% location,\r\n            space = use_locs[[3]] %in% location) %>%\r\n  mutate(across(where(is_logical), as.numeric))\r\n\r\n\r\n\r\nThis will return a dataset that tells us whether a given issue has the X-mansion, the Danger Room, or Space as a location.\r\n\r\n\r\nlocs_sum %>%\r\n  glimpse()\r\n\r\n\r\nRows: 183\r\nColumns: 4\r\n$ issue       <dbl> 97, 98, 99, 100, 101, 102, 103, 104, 105, 106...\r\n$ mansion     <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, ...\r\n$ danger_room <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...\r\n$ space       <dbl> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, ...\r\n\r\nNow we can join the three datasets into one useful for modeling. I’m using an inner join here because, for whatever reason, the character visualization dataset has more issues represented than the others, and we only want issues that are represented in all 3 dataframes.\r\n\r\n\r\nissues_joined <- reduce(list(rend_df, char_sum, locs_sum), ~inner_join(.x, .y, by = \"issue\"))\r\n\r\n\r\n\r\nModeling\r\nCool, so now we’re done preprocessing our data – now we can specify our model.\r\nI mentioned before that one issue here is that this is a small set of data. We have 183 observations (again, each observation is an issue), which isn’t many. One way to make our modeling more robust is to use bootstrap resampling (see our good friend Wikipedia for an explanation) and to fit models to several resamples.\r\n\r\n\r\nset.seed(0408)\r\nbooties <- bootstraps(issues_joined, times = 100)\r\nhead(booties$splits, n = 5)\r\n\r\n\r\n[[1]]\r\n<Analysis/Assess/Total>\r\n<183/68/183>\r\n\r\n[[2]]\r\n<Analysis/Assess/Total>\r\n<183/66/183>\r\n\r\n[[3]]\r\n<Analysis/Assess/Total>\r\n<183/66/183>\r\n\r\n[[4]]\r\n<Analysis/Assess/Total>\r\n<183/70/183>\r\n\r\n[[5]]\r\n<Analysis/Assess/Total>\r\n<183/64/183>\r\n\r\nWhat we can see here is that every bootstrap sample has 183 rows in the analysis set, which is what the model will be trained on, and then some other number of rows in the assessment set. This other number is the out-of-bag sample – the rows that weren’t randomly sampled by the bootstrap process.\r\nNext, I’m going to set up a workflow. I think of this as like a little suitcase that can carry things I want to use in my model around – I think that analogy might be from Julia Silge? Anyway, I’m going to start by adding the formula I want to use in my model.\r\n\r\n\r\nxmen_wf <- workflow() %>%\r\n  add_formula(rendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + mansion + danger_room + space)\r\n\r\n\r\n\r\nNow we can further specify the model. Remember that since our outcome is a count, we’ll be fitting a Poisson regression. Looking at the outcome distribution earlier, I don’t think I need to use a zero-inflated model here (although maybe? Again, this isn’t really my expertise), so I’m just going to proceed with a regular Poisson regression, fit using the {glmnet} engine. I’m also going to tune the penalty and mixture arguments, which control the amount of total regularization applied to the model as well as the proportion of the penalty that is L1 (lasso) vs L2 (ridge regression).\r\nBrief Interpolation on what a Poisson regression is A Poisson regression is a generalized linear model (GLM) used to model count data. Like the name implies, GLMs are generalizations of linear models that use a link function, g(), to transform the expected value of the response (outcome) to a linear function of the predictor variables. Poisson regression uses a log link function to accomplish this transformation. For people interested in reading more, I really like John Fox’s book, Applied Regression Analysis.\r\n\r\n\r\nlibrary(poissonreg)\r\npoisson_mod <- poisson_reg(\r\n  penalty = tune(),\r\n  mixture = tune()\r\n) %>%\r\n  set_engine(\"glmnet\")\r\n\r\n\r\n\r\nSince I’m tuning a couple of parameters, I need to make a grid with possible values to tune across\r\n\r\n\r\npoisson_tune <- grid_max_entropy(\r\n  penalty(),\r\n  mixture(), \r\n  size = 10\r\n)\r\n\r\n\r\n\r\nAnd I’ll drop the model spec into the previous workflow.\r\n\r\n\r\nxmen_wf <- xmen_wf %>%\r\n  add_model(poisson_mod)\r\nxmen_wf\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Formula\r\nModel: poisson_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\nrendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + \r\n    mansion + danger_room + space\r\n\r\n-- Model -------------------------------------------------------------\r\nPoisson Regression Model Specification (regression)\r\n\r\nMain Arguments:\r\n  penalty = tune()\r\n  mixture = tune()\r\n\r\nComputational engine: glmnet \r\n\r\nAnd now we can fit the model using our bootstrap resamples.\r\n\r\n\r\nxmen_fit <- tune_grid(\r\n  xmen_wf,\r\n  resamples = booties,\r\n  grid = poisson_tune\r\n)\r\n\r\n\r\n\r\nOur models have fit, so now we can look at our results:\r\n\r\n\r\nxmen_fit %>%\r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 20 x 8\r\n    penalty mixture .metric .estimator   mean     n std_err .config   \r\n      <dbl>   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>     \r\n 1 8.81e- 4  0.0155 rmse    standard   1.93     100 0.0894  Preproces~\r\n 2 8.81e- 4  0.0155 rsq     standard   0.0277   100 0.00398 Preproces~\r\n 3 4.67e- 7  0.0676 rmse    standard   1.93     100 0.0896  Preproces~\r\n 4 4.67e- 7  0.0676 rsq     standard   0.0277   100 0.00398 Preproces~\r\n 5 5.56e- 1  0.148  rmse    standard   1.71     100 0.0201  Preproces~\r\n 6 5.56e- 1  0.148  rsq     standard   0.0288   100 0.00426 Preproces~\r\n 7 4.76e-10  0.190  rmse    standard   1.93     100 0.0895  Preproces~\r\n 8 4.76e-10  0.190  rsq     standard   0.0277   100 0.00398 Preproces~\r\n 9 1.09e- 2  0.500  rmse    standard   1.92     100 0.0841  Preproces~\r\n10 1.09e- 2  0.500  rsq     standard   0.0278   100 0.00403 Preproces~\r\n11 2.44e- 7  0.517  rmse    standard   1.94     100 0.0896  Preproces~\r\n12 2.44e- 7  0.517  rsq     standard   0.0277   100 0.00398 Preproces~\r\n13 1.73e-10  0.622  rmse    standard   1.94     100 0.0896  Preproces~\r\n14 1.73e-10  0.622  rsq     standard   0.0277   100 0.00398 Preproces~\r\n15 1.10e- 5  0.881  rmse    standard   1.94     100 0.0897  Preproces~\r\n16 1.10e- 5  0.881  rsq     standard   0.0277   100 0.00398 Preproces~\r\n17 1.99e- 1  0.942  rmse    standard   1.69     100 0.0190  Preproces~\r\n18 1.99e- 1  0.942  rsq     standard   0.0302   100 0.00404 Preproces~\r\n19 5.97e-10  0.985  rmse    standard   1.94     100 0.0897  Preproces~\r\n20 5.97e-10  0.985  rsq     standard   0.0277   100 0.00398 Preproces~\r\n\r\nOk, so, my limited understanding of Poisson regression is that neither RMSE or R-squared values are ideal metrics, and some googling led me to find that there’s an open issue to add a Poisson log loss metric to the yardstick package, so we’ll gloss over these for now.\r\nAnyway, let’s pick the best model here, finalize the model, and then fit it to our full training data.\r\n\r\n\r\nbest_params <- xmen_fit %>%\r\n  select_best(metric = \"rmse\")\r\nfinal_mod <- xmen_wf %>%\r\n  finalize_workflow(best_params) %>%\r\n  fit(data = issues_joined)\r\n\r\n\r\n\r\nAnd let’s check out how important how variables are. This should give us the coefficients from our model.\r\n\r\n\r\nfinal_mod %>%\r\n  pull_workflow_fit() %>% \r\n  vi()\r\n\r\n\r\n# A tibble: 7 x 3\r\n  Variable    Importance Sign \r\n  <chr>            <dbl> <chr>\r\n1 mansion        0.157   NEG  \r\n2 danger_room    0.113   NEG  \r\n3 Professor_X    0.0197  POS  \r\n4 Jean_Grey      0.0133  POS  \r\n5 Wolverine      0.00938 POS  \r\n6 Magneto        0.00701 POS  \r\n7 space          0       NEG  \r\n\r\n\r\n\r\nfinal_mod %>%\r\n  pull_workflow_fit() %>% \r\n  vip(num_features = 7, fill = lann)\r\n\r\n\r\n\r\n\r\nRight, so, one thing to keep in mind here is that the location variables and the character variables are on different scales, so the effects aren’t directly comparable. But the interpretation here is that more appearances of Professor X are more strongly associated with more characters rendered unconscious in an issue than are more appearances of Magneto, although all of these coefficients are positive, suggesting that more appearances of any of these four characters are associated with more renderings unconscious in that issue. Similarly, the effects of danger_room and mansion are negative, suggesting that if the issue features either of those locations, there tend to be fewer characters rendered unconscious. The coefficient for space is 0, which probably means it got regularized out. Probably the most important piece, here, though, is that these effects seem to be very small, which means they likely don’t actually matter.\r\nI’m going to call it right here. Even though the model I built doesn’t seem to have much explanatory power, it forced me to read some more about Poisson regression and to dig back into the tidymodels framework, which I’ll count as a win. Plus it gives me an excuse to gather “domain knowledge” about comic books so I can do a better job next time.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-06-unconsciousness-in-the-xmen/unconsciousness-in-the-xmen_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-05T08:24:31-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-06-exploring-emily-osters-covid-data/",
    "title": "Exploring Emily Oster's COVID Data",
    "description": "Examining child care survey data from Emily Oster",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-06-25",
    "categories": [],
    "contents": "\r\nI’m a big fan of Emily Oster. Well, her work; I don’t know her personally, but she seems cool. As someone with a background in academic research, and also as a new parent, I really appreciate the thoroughness, honesty, and approachability of Cribsheet. And I’m glad she summarized all of the academic articles so I didn’t have to look for them. More recently, I’ve been a big fan of her Parent Data newsletter, which I read (on days it comes out) while my daughter Emma is napping in the morning. Likewise, her COVID Explained website has been an equally thorough/honest/approachable website for all (or most, at least) things COVID.\r\nUnlike most other people who have data-centric websites and blogs, I’ve avoided doing any analyses related to COVID data. I don’t think the world needs yet another COVID-tracker website that someone built as part of a data science portfolio, and I’m honestly not interested enough in COVID to keep up with the data, which is changing seemingly by the second. That said, my actual job is in the Early Childhood office of the Virginia Department of Education, plus I have a 5 month old daughter who just started daycare. So early education/child care is kind of my jam, and so when I saw the Parent Data newsletter with some survey data Emily collected relating to childcare centers during COVID, I figured I could take a peek.\r\nSome Caveats\r\nBefore getting into the exploration here, I’m just going to copy and paste the description of the data & data collection process from the Parent Data newsletter:\r\n\r\nWhat did you do?\r\nI distributed, in various ways, a simple survey of child care providers who were open during the pandemic. I got some amazing help from Winnie.com, who sent the >survey out to all their providers. I also sent this out on Twitter, through newsletters, on Facebook, etc, etc. And then I collated responses.\r\nIs this a scientifically valid sample and do you plan to publish the results?\r\nNo and no. This is crowdsourced. I didn’t sample randomly and I cannot be sure of the biases in responses. I am of the view (which not everyone will agree with) that some data is better than none.\r\nIs the data perfect? Did you clean it?\r\nNo! Let me know if you see obvious errors. I did minimal cleaning - to remove places which reported fewer than two students during the pandemic or did not report any location data. Basically, I’m going to be cautious about not drawing too many conclusions from this, and so should you. I’m going to see where the data takes me, make some visualizations and summary tables, but I’m not going to, like, change my approach to public health right now as a result of these analyses.\r\n\r\nI’ll also include all of my code inline in this post. I think this helps people see what choices I’m making, but sorry to those who find this a distraction.\r\n\r\n\r\n\r\nCleaning Up Data\r\nLet’s take a glimpse at the data:\r\n\r\n\r\ndf %>%\r\n  glimpse()\r\n\r\n\r\nRows: 986\r\nColumns: 27\r\n$ State                                       <chr> \"Washington\",...\r\n$ `Town/County/City`                          <chr> NA, \"New Have...\r\n$ `Age Ranges`                                <chr> \"6 weeks - 6 ...\r\n$ `Single or Multiple Locations?`             <chr> \"Single\", \"Mu...\r\n$ `Opening Details`                           <chr> \"Open the who...\r\n$ `Number of Students Served During Pandemic` <dbl> 4, 25, 10, 60...\r\n$ `Number of Staff During Pandemic`           <dbl> NA, 19, NA, 2...\r\n$ `COVID-19 Cases in Children`                <dbl> 0, 0, 0, 2, 0...\r\n$ `COVID-19 Cases in Staff`                   <dbl> 0, 2, 0, 0, 1...\r\n$ ...10                                       <lgl> NA, NA, NA, N...\r\n$ Kids...11                                   <dbl> 0, 25, 0, 60,...\r\n$ Staff...12                                  <dbl> 0, 19, 0, 20,...\r\n$ `Kid COVID...13`                            <dbl> 0, 0, 0, 2, 0...\r\n$ `Staff COVID...14`                          <dbl> 0, 2, 0, 0, 1...\r\n$ Count...15                                  <dbl> 0, 1, 0, 1, 1...\r\n$ ...16                                       <lgl> NA, NA, NA, N...\r\n$ `Count of Kids`                             <dbl> 4, 0, 0, 0, 0...\r\n$ `Count of Staff`                            <dbl> NA, 0, 0, 0, ...\r\n$ `Kids COVID`                                <dbl> 0, 0, 0, 0, 0...\r\n$ `Staff COVID...20`                          <dbl> 0, 0, 0, 0, 0...\r\n$ `Total Centers`                             <dbl> 1, 0, 0, 0, 0...\r\n$ ...22                                       <lgl> NA, NA, NA, N...\r\n$ Kids...23                                   <dbl> NA, 4, 0, 0, ...\r\n$ Staff...24                                  <dbl> NA, NA, 0, 0,...\r\n$ `Kid COVID...25`                            <dbl> NA, 0, 0, 0, ...\r\n$ `Staff COVID...26`                          <dbl> NA, 0, 0, 0, ...\r\n$ Count...27                                  <dbl> NA, 1, 0, 0, ...\r\n\r\nOk, so we see a bunch of columns here that were hidden in the Google Sheet. I downloaded this to an .xlsx to check out what these are, and it looks like they’re columns that hold calculations that feed into the summary sheet. I’m going to drop these for now – I can always replicate the calculations if I need to later.\r\n\r\n\r\ndf <- df %>%\r\n  select(c(1:9)) %>%\r\n  clean_names() %>%\r\n  filter(str_detect(state, \"Minneaota\", negate = TRUE)) %>%\r\n  mutate(id = row_number()) %>%\r\n  select(id, everything())\r\nglimpse(df)\r\n\r\n\r\nRows: 982\r\nColumns: 10\r\n$ id                                        <int> 1, 2, 3, 4, 5, ...\r\n$ state                                     <chr> \"Washington\", \"...\r\n$ town_county_city                          <chr> NA, \"New Haven\"...\r\n$ age_ranges                                <chr> \"6 weeks - 6 mo...\r\n$ single_or_multiple_locations              <chr> \"Single\", \"Mult...\r\n$ opening_details                           <chr> \"Open the whole...\r\n$ number_of_students_served_during_pandemic <dbl> 4, 25, 10, 60, ...\r\n$ number_of_staff_during_pandemic           <dbl> NA, 19, NA, 20,...\r\n$ covid_19_cases_in_children                <dbl> 0, 0, 0, 2, 0, ...\r\n$ covid_19_cases_in_staff                   <dbl> 0, 2, 0, 0, 1, ...\r\n\r\nNow we have a data frame with 9 columns that we can explore. I think what each column represents is pretty obvious given the variable name, so I’m not going to describe each one. I will truncate the names a bit, though, just so I don’t have to type out the number_of_students_served_during_pandemic each time I want to use that variable. One thing that does seem important to point out, though is that each row/observation in the dataset represents a child care program, which could correspond to multiple sites or a single site.\r\n\r\n\r\ndf <- df %>%\r\n  rename(\r\n    location_type = single_or_multiple_locations,\r\n    num_kids = number_of_students_served_during_pandemic,\r\n    num_staff = number_of_staff_during_pandemic,\r\n    covid_kids = covid_19_cases_in_children,\r\n    covid_staff = covid_19_cases_in_staff\r\n  )\r\nglimpse(df)\r\n\r\n\r\nRows: 982\r\nColumns: 10\r\n$ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...\r\n$ state            <chr> \"Washington\", \"Connecticut\", \"Connecticu...\r\n$ town_county_city <chr> NA, \"New Haven\", \"Stanford\", \"Cook Count...\r\n$ age_ranges       <chr> \"6 weeks - 6 months, 6 months - 1 year, ...\r\n$ location_type    <chr> \"Single\", \"Multiple\", \"Single\", \"Multipl...\r\n$ opening_details  <chr> \"Open the whole time\", \"Open the whole t...\r\n$ num_kids         <dbl> 4, 25, 10, 60, 40, 12, 30, 250, 100, 5, ...\r\n$ num_staff        <dbl> NA, 19, NA, 20, NA, NA, NA, 41, 30, 1, 2...\r\n$ covid_kids       <dbl> 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r\n$ covid_staff      <dbl> 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0...\r\n\r\nWho Responded\r\nFirst, let’s take a look at who responded to the survey based on the state they live in\r\n\r\n\r\ndf %>%\r\n  count(state) %>%\r\n  top_n(20) %>%\r\n  ggplot(aes(x = n, y = fct_reorder(state, n))) +\r\n  geom_col(fill = purple) +\r\n  geom_text(aes(x = n-2, label = n), hjust = 1, color = \"white\", size = 3) +\r\n  labs(\r\n    x = \"Number of Programs Responding\",\r\n    y = \"State\",\r\n    title = \"Number of Programs Responding by State\",\r\n    caption = \"Only top 20 states included\"\r\n  )\r\n\r\n\r\n\r\n\r\nRight, so, one thing to keep in mind is that we have far more responses from a handful of states – California and Texas mostly, but also Washington and Maryland. Let’s look to see if we see this same pattern in the number of children attending these programs as well as the number of staff working at these programs.\r\n\r\n\r\nfacet_labs <- as_labeller(c(\"num_kids\" = \"Kids\", \"num_staff\" = \"Staff\"))\r\ndf %>%\r\n  pivot_longer(cols = c(\"num_kids\", \"num_staff\"),\r\n               names_to = \"name\",\r\n               values_to = \"value\") %>%\r\n  count(state, name, wt = value) %>%\r\n  group_by(name) %>%\r\n  top_n(10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = n, y = reorder_within(state, n, name), fill = name)) +\r\n  geom_col() +\r\n  facet_wrap(~name, nrow = 2, scales = \"free_y\", labeller = facet_labs) +\r\n  scale_y_reordered() +\r\n  scale_fill_ipsum() +\r\n  #theme_minimal() +\r\n  labs(\r\n    x = \"Count\",\r\n    y = \"\",\r\n    title = \"Number of Kids & Staff by State\",\r\n    caption = \"Top 10 states only\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\",\r\n    axis.text.y = element_text(size = 10)\r\n  )\r\n\r\n\r\n\r\n\r\nSo, something is def going on with Texas here – it looks like they have a ton of kids reported proportional to their number of staff. Let’s see if we can look at the ratios for Texas, California, and Minnesota to get a sense of things.\r\n\r\n\r\ndf %>%\r\n  filter(state %in% c(\"Texas\", \"California\", \"Minnesota\")) %>%\r\n  group_by(state) %>%\r\n  summarize(\r\n    kids = sum(num_kids, na.rm = TRUE),\r\n    staff = sum(num_staff, na.rm = TRUE)\r\n  ) %>%\r\n  ungroup() %>%\r\n  mutate(ratio = kids/staff) %>%\r\n  make_table()\r\n\r\n\r\n\r\nstate\r\n\r\n\r\nkids\r\n\r\n\r\nstaff\r\n\r\n\r\nratio\r\n\r\n\r\nCalifornia\r\n\r\n\r\n2189\r\n\r\n\r\n756\r\n\r\n\r\n2.895503\r\n\r\n\r\nMinnesota\r\n\r\n\r\n3142\r\n\r\n\r\n1145\r\n\r\n\r\n2.744105\r\n\r\n\r\nTexas\r\n\r\n\r\n4617\r\n\r\n\r\n1454\r\n\r\n\r\n3.175378\r\n\r\n\r\nThe ratio actually looks reasonable for Texas (and all are very low in the grand scheme of things) – for context, the legal ratio for infant classrooms in Virginia is 4:1, and these are all well below that.\r\nExploring COVID Cases\r\nNow that we have a general sense of who responded, let’s explore the data on the number and rate of COVID cases a bit. First, I’m going to make a scatter plot with the number of people at a center against the number of COVID cases in a center. We would expect the number of cases to increase as the number of people increases, but the shape and strength of this relationship could be interesting.\r\n\r\n\r\ndf_longer <- df %>%\r\n  pivot_longer(cols = c(\"num_kids\", \"num_staff\", \"covid_kids\", \"covid_staff\"),\r\n               names_to = \"name\",\r\n               values_to = \"value\") %>%\r\n  extract(col = \"name\",\r\n          into = c(\"type\", \"group\"),\r\n          regex = \"(.*)_(.*)\") %>%\r\n  pivot_wider(names_from = type,\r\n              values_from = value)\r\ndf_longer %>%\r\n  ggplot(aes(x = covid, y = num, color = group)) +\r\n  geom_point(alpha = .6) +\r\n  labs(\r\n    title = \"Program Population vs Number of COVID Cases\",\r\n    y = \"Population\",\r\n    x = \"COVID Cases\"\r\n  ) +\r\n  facet_wrap(~group, scales = \"free\") +\r\n  scale_color_ipsum() +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nWell, this doesn’t show what I thought it would. I expected a weak but positive association between total program population and number of COVID cases (i.e. as your program has more people, you also have more COVID cases), but we don’t see that here. Probably because COVID cases are so rare, especially in kids, we actually don’t see much of a relationship. Although note that if we plot regression lines (as in the plot below), the estimated relationship is positive, but this is due pretty much entirely to outliers. We could get into some regression diagnostics to confirm it more formally, but eyeballing it is good enough for a blog post.\r\nAnother related point is that it appears a couple of programs are reporting 300 staff. I’d assume this is a data entry error, but it’s also possible that this is a corporate program that’s taking a liberal view of what “staff” means here.\r\n\r\n\r\ndf_longer %>%\r\n  ggplot(aes(x = covid, y = num, color = group)) +\r\n  geom_point(alpha = .6) +\r\n  geom_smooth(method = \"lm\") +\r\n  labs(\r\n    title = \"Program Population vs Number of COVID Cases\",\r\n    y = \"Population\",\r\n    x = \"COVID Cases\"\r\n  ) +\r\n  facet_wrap(~group, scales = \"free\") +\r\n  scale_color_ipsum() +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nAfter seeing this, a better approach might be to plot the infection rate for each program and then take a look at which have particularly high infection rates. We can do this using a beeswarm plot:\r\n\r\n\r\ndf_longer %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  ggplot(aes(y = infect_rate, x = group, color = group)) +\r\n  geom_quasirandom(alpha = .7) +\r\n  labs(\r\n    x = \"\",\r\n    y = \"Infection Rate\",\r\n    title = \"Child Care COVID Infection Rate by Population Type\"\r\n  ) +\r\n  scale_color_ipsum() +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe see in this plot that most places are at 0% infections for kids and staff, but a handful show rates higher than 25% for kids and higher than 50% for staff. One program has a 200% infection rate for staff, which must be a data entry error. Since infections are incredibly infrequent, my suspicion is that these high-infection-rate programs have very small populations, so let’s take a look at the top 10:\r\n\r\n\r\ndf_longer %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  arrange(desc(infect_rate)) %>%\r\n  select(-c(\"id\", \"age_ranges\", \"location_type\", \"opening_details\")) %>%\r\n  head(10L) %>%\r\n  make_table()\r\n\r\n\r\n\r\nstate\r\n\r\n\r\ntown_county_city\r\n\r\n\r\ngroup\r\n\r\n\r\nnum\r\n\r\n\r\ncovid\r\n\r\n\r\ninfect_rate\r\n\r\n\r\nNew York\r\n\r\n\r\nWestchester\r\n\r\n\r\nstaff\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2.0000000\r\n\r\n\r\nMaryland\r\n\r\n\r\nAnne Arundel\r\n\r\n\r\nstaff\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.0000000\r\n\r\n\r\nTexas\r\n\r\n\r\nWilliamson\r\n\r\n\r\nstaff\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.0000000\r\n\r\n\r\nWashington\r\n\r\n\r\nKing\r\n\r\n\r\nstaff\r\n\r\n\r\n10\r\n\r\n\r\n9\r\n\r\n\r\n0.9000000\r\n\r\n\r\nMinnesota\r\n\r\n\r\nHennepin\r\n\r\n\r\nstaff\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n0.6666667\r\n\r\n\r\nVirginia\r\n\r\n\r\nArlington\r\n\r\n\r\nstaff\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n0.5000000\r\n\r\n\r\nVirginia\r\n\r\n\r\nArlington\r\n\r\n\r\nstaff\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n0.5000000\r\n\r\n\r\nIndiana\r\n\r\n\r\nMarion\r\n\r\n\r\nstaff\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n0.5000000\r\n\r\n\r\nMinnesota\r\n\r\n\r\nRamsey\r\n\r\n\r\nstaff\r\n\r\n\r\n5\r\n\r\n\r\n2\r\n\r\n\r\n0.4000000\r\n\r\n\r\nNorth Carolina\r\n\r\n\r\nHenderson\r\n\r\n\r\nstaff\r\n\r\n\r\n11\r\n\r\n\r\n4\r\n\r\n\r\n0.3636364\r\n\r\n\r\nOur 200% infection rate program is a site in NY that reported 1 staff member but 2 cases of COVID in staff members. The most surprising piece of data in this table, though, is the program in King, Washington, where 9 of the 10 staff contracted COVID. Also, it’s hard to tell, but I suspect programs in Arlington, VA might be duplicate entries since the data are identical.\r\nAnother question I’m thinking about now is which state had the highest rate of infection:\r\n\r\n\r\ndf_longer %>%\r\n  group_by(state, group) %>%\r\n  summarize(num = sum(num, na.rm = TRUE),\r\n            covid = sum(covid, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  group_by(group) %>%\r\n  top_n(10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(y = reorder_within(state, infect_rate, group), x = infect_rate, fill = group)) +\r\n  geom_col() +\r\n  facet_wrap(~group, nrow = 2, scales = \"free\") +\r\n  labs(\r\n    x = \"Infection Rate\",\r\n    y = \"\",\r\n    title = \"COVID Infection Rates in Child Care Centers by State and Population Type\",\r\n    caption = \"Only the top 10 states are shown \"\r\n  ) +\r\n  scale_y_reordered() +\r\n  scale_x_continuous(labels = scales::percent_format()) +\r\n  scale_fill_ipsum() +\r\n  theme(\r\n    legend.position = \"none\",\r\n    axis.text.y = element_text(size = 10),\r\n    plot.title = element_text(size = 16)\r\n  )\r\n\r\n\r\n\r\n\r\nHere, we see West VA and Tennessee as having the highest infection rates for kids and staff. This is probably driven by low response rates from those states – recall that in our first plot, neither of these states showed up as having a large number of programs responding – but let’s check it out:\r\n\r\n\r\ndf_longer %>%\r\n  filter(state %in% c(\"Tennessee\", \"West Virginia\")) %>%\r\n  group_by(state, group) %>%\r\n  summarize(num = sum(num, na.rm = TRUE),\r\n            covid = sum(covid, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  make_table()\r\n\r\n\r\n\r\nstate\r\n\r\n\r\ngroup\r\n\r\n\r\nnum\r\n\r\n\r\ncovid\r\n\r\n\r\ninfect_rate\r\n\r\n\r\nTennessee\r\n\r\n\r\nkids\r\n\r\n\r\n173\r\n\r\n\r\n0\r\n\r\n\r\n0.0000000\r\n\r\n\r\nTennessee\r\n\r\n\r\nstaff\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0.0000000\r\n\r\n\r\nWest Virginia\r\n\r\n\r\nkids\r\n\r\n\r\n60\r\n\r\n\r\n2\r\n\r\n\r\n0.0333333\r\n\r\n\r\nWest Virginia\r\n\r\n\r\nstaff\r\n\r\n\r\n40\r\n\r\n\r\n2\r\n\r\n\r\n0.0500000\r\n\r\n\r\nSo this puzzled me for a while. According to this table, Tennessee actually has a 0% infection rate for both kids and staff and not the ~3% and ~12% rates for kids and staff, respectively, that the above plot indicates. After I just checked over my code several times, it turns out that it’s due to a typo in the survey response – the fictional state of “Tenneessee” has a high infection rate; the actual state of Tennessee does not. West Virginia, though, is showing a higher rate due to sampling error and low response rate (as we can see in the table above).\r\nAnd, finally, since I’m a new dad, let’s take a look at cases in places that accept the youngest kids – ages 6 week through 6 months – and compare those to all programs, just looking at kids and ignoring staff.\r\n\r\n\r\ndf %>%\r\n  mutate(smol_kiddos = if_else(str_detect(age_ranges, \"6 weeks\"), \"Yes\", \"No\")) %>%\r\n  group_by(smol_kiddos) %>%\r\n  summarize(across(c(\"num_kids\", \"covid_kids\"), ~sum(.x, na.rm = TRUE))) %>%\r\n  filter(!is.na(smol_kiddos)) %>%\r\n  ungroup() %>%\r\n  adorn_totals(\"row\") %>%\r\n  mutate(infect_rate = round(100*(covid_kids/num_kids), 2)) %>%\r\n  make_table()\r\n\r\n\r\n\r\nsmol_kiddos\r\n\r\n\r\nnum_kids\r\n\r\n\r\ncovid_kids\r\n\r\n\r\ninfect_rate\r\n\r\n\r\nNo\r\n\r\n\r\n6055\r\n\r\n\r\n6\r\n\r\n\r\n0.10\r\n\r\n\r\nYes\r\n\r\n\r\n18201\r\n\r\n\r\n32\r\n\r\n\r\n0.18\r\n\r\n\r\nTotal\r\n\r\n\r\n24256\r\n\r\n\r\n38\r\n\r\n\r\n0.16\r\n\r\n\r\nSo, there really aren’t differences here between programs that accept the youngest kids and all programs. There might be a difference between programs that accept kids 6 weeks - 6 months and those that don’t, but again, the overall infection rate is so low that it’s hard to tell.\r\nFinal Thoughts\r\nThere’s definitely more I could do with this data, but I just wanted to give it a quick run through. I am intentionally avoiding statistical modeling here, though, because 1) I don’t want to take the time to clean the data thoroughly enough to model it, and 2) I don’t know very much about modeling zero-inflated count data, so we’ll just leave it at this for now. But I did appreciate the opportunity to dig in a little bit – thanks Emily!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-06-exploring-emily-osters-covid-data/exploring-emily-osters-covid-data_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-06-pulling-youtube-transcripts/",
    "title": "Pulling YouTube Transcripts",
    "description": "Example of pulling transcripts for an entire YouTube playlist.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-05-15",
    "categories": [],
    "contents": "\r\nI’ve been a fan of the Your Mom’s House Podcast for a long time now, and I thought it would be interesting to do some analysis of their speech patterns. If you follow the show at all, you know that the conversations are…special (you can check here for a visualization I did of their word usage over time if you’re so inclined). Fortunately, it’s possible to get transcripts of YouTube videos. Getting transcripts for a single video using the {youtubecaption} R package is fairly straightforward; getting transcripts for a full playlist is a touch more involved, so I wanted to create a quick walkthrough illustrating my process for doing this. Hopefully this will help others who might want to analyze text data from YouTube.\r\nSetup\r\nFirst, let’s load the packages we need to pull our data. I’m going to use the following:\r\n{tidyverse} for data wrangling\r\n{youtubecaption} for calling the YouTube API to get transcripts\r\n{janitor} pretty much just for the clean_names() function\r\n{lubridate} to work with the publication_date variable that’s part of the YT video data. (This is optional if you don’t want to work with this variable at all)\r\n\r\n\r\n\r\nGetting Transcripts for a Single Video\r\nLike I mentioned previously, getting transcripts for a single video is pretty easy thanks to the {youtubecaption} package. All we need is the URL for the video and the get_caption() function can go do its magic. I’ll illustrate that here using the most recent YMH podcast full episode.\r\n\r\n\r\nymh_new <- get_caption(\"https://www.youtube.com/watch?v=VMloBlnczzI\")\r\nglimpse(ymh_new)\r\n\r\n\r\nRows: 3,157\r\nColumns: 5\r\n$ segment_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,...\r\n$ text       <chr> \"this episode of your mom's house is\", \"brough...\r\n$ start      <dbl> 0.000, 1.140, 3.659, 7.859, 8.910, 14.820, 20....\r\n$ duration   <dbl> 3.659, 6.719, 5.251, 6.961, 11.879, 9.080, 3.1...\r\n$ vid        <chr> \"VMloBlnczzI\", \"VMloBlnczzI\", \"VMloBlnczzI\", \"...\r\n\r\nWe can see above that this gives us a tibble with the text (auto-transcribed by YouTube) broken apart into short segments and corresponding identifying information for each text segment.\r\nOne thing worth mentioning here is that the transcripts are automatically transcribed by a speech-to-text model. It seems really good, but it will make some mistakes, particularly around brand names and website addresses (in my limited experience).\r\nGetting Transcripts for Several Videos\r\nBut what if we want to get transcripts for several videos? The get_caption() function requires the URL of each video that we want to get a caption for. If you want to analyze transcripts from more than a handful of videos, it would get really tedious really quickly to go and grab the individual URLs. And, more specifically, what if you wanted to get the transcripts for all videos from a single playlist?\r\nGet URLS\r\nI found this tool that will take a YouTube playlist ID and provide an Excel file with, among other information, the URL for each video in the playlist, which is exactly what we need for the get_caption() function.\r\nI used the tool on 5/14/20 to get a file with the data for all of the videos in the YMH Podcast - Full Episodes playlist. I’ll go ahead an upload the file, plus do some light cleaning, in the code below.\r\n\r\n\r\nep_links <- read_csv(\"~/Data/YMH/Data/ymh_full_ep_links.csv\") %>%\r\n  clean_names() %>%\r\n  mutate(ep_num = str_replace_all(title, \".*Ep.*(\\\\d{3}).*\", \"\\\\1\") %>%\r\n           as.double(),\r\n         ep_num = if_else(ep_num == 19, 532, ep_num),\r\n         published_date = mdy_hm(published_date),\r\n         vid = str_replace_all(video_url, \".*=(.*)$\", \"\\\\1\"))\r\nglimpse(ep_links)\r\n\r\n\r\nRows: 223\r\nColumns: 7\r\n$ published_date <dttm> 2020-04-29 12:03:00, 2020-04-22 12:00:00,...\r\n$ video_url      <chr> \"https://www.youtube.com/watch?v=xw3KNj2yw...\r\n$ channel        <chr> \"YourMomsHousePodcast\", \"YourMomsHousePodc...\r\n$ title          <chr> \"Your Mom's House Podcast - Ep. 549\", \"You...\r\n$ description    <chr> \"Want an ad-free experience? Click here to...\r\n$ ep_num         <dbl> 549, 548, 547, 546, 545, 544, 543, NA, 542...\r\n$ vid            <chr> \"xw3KNj2ywVo\", \"_BVQvqPvu-8\", \"HvueqYO--tc...\r\n\r\nWe can see that this gives us the URLs for all 225 episodes in the playlist.\r\nThe cleaning steps for the published_date variable and the vid variable should be pretty universal. The step to get the episode number extracts that from the title of the video, and so this step is specific to the playlist I’m using.\r\n“Safely” Pull Transcripts\r\nNow that we have all of the URLs, we can iterate through all of them using the get_caption() function. Before we do that, though, we want to make the get_caption() robust to failure. Basically, we don’t want the whole series of iterations to fail if one returns an error. In other words, we want the function to get all of the transcripts that it can get and let us know which it can’t, but not to fail if it can’t get every transcript.\r\nTo do this, we just wrap the get_caption() function in the safely() function from {purrr}.\r\n\r\n\r\nsafe_cap <- safely(get_caption)\r\n\r\n\r\n\r\nYou can read more about safely() in the {purrr} documentation, but it basically returns, for each call, a 2-element list: 1 element with the “result” of the function and another with the “error.” If the function succeeds, “error” will be NULL and “result” will have the result of the function. If the function fails, “result” will be NULL and “error” will show the error message.\r\nNow that we have your safe_cap() function, we can use map() from {purrr} to pull transcripts from all of the videos we have URLs for.\r\n\r\n\r\nymh_trans <- map(ep_links$video_url,\r\n                 safe_cap)\r\nglimpse(head(ymh_trans))\r\n\r\n\r\nList of 6\r\n $ :List of 2\r\n  ..$ result: tibble [2,663 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,093 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,727 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [2,701 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,276 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,382 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n\r\nFormat Data\r\nThis returns a list the same length as our vector of URLs (225 in this case) in the format described above. We want to get the “result” element from each of these lists. (You might also be interested in looking at the errors, but any errors are all going to be the same here – basically that a transcript isn’t available for a specific video). To do that, we want to iterate over all elements of our transcript list (using map() again) and use the pluck() function from {purrr} to get the result object. We then used the compact() function to get rid of any NULL elements in this list (remember that the “result” element will be NULL if the function couldn’t get a transcript for the video). This will give us a list of transcripts that the function successfully fetched.\r\nNext, we use the bind_rows() function to take this list and turn it into a tibble. And finally, we can inner_join() this with our tibble that had the URLs so that metadata for each video and transcripts are in the same tibble.\r\n\r\n\r\nres <- map(1:length(ymh_trans),\r\n           ~pluck(ymh_trans, ., \"result\")) %>%\r\n  compact() %>%\r\n  bind_rows() %>%\r\n  inner_join(x = ep_links,\r\n            y = .,\r\n            by = \"vid\")\r\nglimpse(res)\r\n\r\n\r\nRows: 437,098\r\nColumns: 11\r\n$ published_date <dttm> 2020-04-29 12:03:00, 2020-04-29 12:03:00,...\r\n$ video_url      <chr> \"https://www.youtube.com/watch?v=xw3KNj2yw...\r\n$ channel        <chr> \"YourMomsHousePodcast\", \"YourMomsHousePodc...\r\n$ title          <chr> \"Your Mom's House Podcast - Ep. 549\", \"You...\r\n$ description    <chr> \"Want an ad-free experience? Click here to...\r\n$ ep_num         <dbl> 549, 549, 549, 549, 549, 549, 549, 549, 54...\r\n$ vid            <chr> \"xw3KNj2ywVo\", \"xw3KNj2ywVo\", \"xw3KNj2ywVo...\r\n$ segment_id     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\r\n$ text           <chr> \"oh snap there's hot gear merge method\", \"...\r\n$ start          <dbl> 0.030, 4.020, 6.629, 12.450, 14.730, 17.40...\r\n$ duration       <dbl> 6.599, 8.430, 8.101, 4.950, 4.530, 5.600, ...\r\n\r\nHopefully this helps folks & best of luck with your text analyses!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T08:24:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-writing-window-functions/",
    "title": "Writing Window Functions",
    "description": "Examples and tutorial of writing rolling aggregate functions.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-05-06",
    "categories": [],
    "contents": "\r\nI was recently working on exploring some time series data for a Kaggle competition and found myself wanting to calculate the rolling average of some sales. I don’t often work with time series data, so I had to look up functions to use to calculate rolling averages (n.b. that if you don’t know what a rolling average is, read here), and I was surprised that dplyr doesn’t have one built in. It turns out there are several packages that do have rolling aggregate (average, standard deviation, etc) functions, such as the RcppRoll package and the zoo package. But I also thought it provided a nice opportunity to practice writing some of my own rolling aggregate functions, which I’m going to walk through here.\r\nSetup\r\nFirst, I’m going to load packages. For this, I’m only using {tidyverse} (and within tidyverse, mostly {purrr} for iteration) and {RcppRoll} as a ground-truth to test my functions. I’m also going to use the {glue} package later on, but that’s less central and I’ll load it when I need it.\r\n\r\n\r\n\r\nNext, I’m going to set up a minimal tibble to use for calculations. This will have an day column and a val column. The val column is the one I’m going to be doing calculations on, and the day column is going to serve as an index for the rolling average.\r\n\r\n\r\nset.seed(0408)\r\ndf <- tibble(\r\n  day = c(1:250),\r\n  val = rnorm(250, mean = 5, sd = 1)\r\n)\r\ndf\r\n\r\n\r\n# A tibble: 250 x 2\r\n     day   val\r\n   <int> <dbl>\r\n 1     1  2.64\r\n 2     2  5.30\r\n 3     3  4.29\r\n 4     4  5.76\r\n 5     5  3.75\r\n 6     6  4.89\r\n 7     7  4.50\r\n 8     8  3.84\r\n 9     9  4.74\r\n10    10  6.41\r\n# ... with 240 more rows\r\n\r\nStep 1: Testing Iteration\r\nSo, my process for building this function is going to be to create something very basic with few variables first and then gradually abstract this out to make a more responsive function. Eventually, I’ll get to a point where the rolling aggregation function will be general enough to allow for the specification of arbitrary aggregate functions and windows.\r\nThe first step, then, is just to test the logic of the calculation I need to create to calculate rolling averages. I’ll do this by assuming a 28 day window (we’ll be able to change the window later), create a “truth” to test against using RcppRoll’s roll_mean() function, and then iterate using map().\r\n\r\n\r\ntruth <- roll_mean(df$val, n = 28, align = \"right\")\r\ntest <- map_dbl(\r\n  c(28:length(df$val)), #this represents the days I want to calculate the average for. I'm starting on day 28 (because I want a 28-day rolling average, \r\n  #and the first time I'll have 28 days of data is on day 28) and going through the last day\r\n  function(a) {\r\n    mean(df$val[(a - 27):a], na.rm = FALSE) \r\n  } #this specifies what I'm doing -- taking the mean of the 'val' column for each 28 day window \r\n  #(day 1-28, day 2-29, etc). If I don't subtract 1 window value when I subset, \r\n  #I'll actually get 29 days.\r\n)\r\nall.equal(truth, test) #this tests to see that the vectors are equal.\r\n\r\n\r\n[1] TRUE\r\n\r\nStep 2: Building Out Functions\r\nGreat, so the logic of the calculation works. Now, let’s extend it a little bit to create a function where I can specify the variable I want to use as well as the window I want to take the rolling average over.\r\n\r\n\r\nee_roll_mean <- function(x, window) {\r\n  map_dbl(\r\n    c(window:length(x)),\r\n    function(a) {\r\n      mean(x[(a - window+1):a], na.rm = FALSE)\r\n    }\r\n  )\r\n}\r\ntest_2 <- ee_roll_mean(df$val, 28)\r\nall.equal(test_2, truth)\r\n\r\n\r\n[1] TRUE\r\n\r\nIt works when we set the window value to 28, but let’s also test that it works when we use a different window just to be safe.\r\n\r\n\r\ntruth_win8 <- roll_mean(df$val, n = 8, align = \"right\")\r\ntest_win8 <- ee_roll_mean(df$val, window = 8)\r\nall.equal(truth_win8, test_win8)\r\n\r\n\r\n[1] TRUE\r\n\r\nThis works well for taking the rolling average – we can specify the values we want to take the average over as well as the window for that average. But there are other functions we might be interested in getting rolling aggregates for as well. For instance, we might want to know the minimum or standard deviation of a value during some windows of time. Rather than write separate functions to do this, we can just extend our previous function to allow us to supply whichever aggregation function we want.\r\n\r\n\r\nee_roll_func <- function(x, window, fn = mean) {\r\n  map_dbl(\r\n    c(window:length(x)),\r\n    function(a) {\r\n      fn(x[(a - window+1):a], na.rm = FALSE)\r\n    }\r\n  ) \r\n}\r\ntest_3 <- ee_roll_func(df$val, window = 8, fn = sd)\r\n#testing against the RcppRoll function that does the same thing\r\ntruth_3 <- roll_sd(df$val, n = 8, align = \"right\")\r\nall.equal(test_3, truth_3)\r\n\r\n\r\n[1] TRUE\r\n\r\nStep 3: Pad the Output\r\nOne thing I’m noticing when looking at the output of each of these functions is that the length of the output vectors differ depending on the value we pass to the window argument.\r\n\r\n\r\nlength(test)\r\n\r\n\r\n[1] 223\r\n\r\nlength(test_win8)\r\n\r\n\r\n[1] 243\r\n\r\nI’m also noticing that these outputs are shorter than the length of the input vector (which is length 250). This makes sense because the function can’t take, for example, the 28 day average before the 28th day, and so the length of the output vector will be 27 elements shorter than the length of the input vector.\r\nThis isn’t so great if we want to add the results of this function back into our original df, though, because all of the vectors in a df need to be the same length. One solution is to “pad” our output vector with the appropriate amount of NA values so that it is the same length as the input vector and can therefore get added as a column in our df. So let’s do that.\r\n\r\n\r\nee_roll_func_padded <- function(x, window, fn = mean) {\r\n  map_dbl(\r\n    c(window:length(x)),\r\n    function(a) {\r\n      fn(x[(a - window+1):a], na.rm = FALSE)\r\n    }\r\n  ) %>%\r\n    append(rep(NA_real_, times = window-1), values = .)   #this will pad the front with a number of NAs equal\r\n  #to the window value minus 1\r\n}\r\ntest_pad1 <- ee_roll_func_padded(df$val, window = 8) #note that if we don't supply a function, it will use the mean\r\ntest_pad2 <- ee_roll_func_padded(df$val, window = 20)\r\ntest_pad1\r\n\r\n\r\n  [1]       NA       NA       NA       NA       NA       NA       NA\r\n  [8] 4.372225 4.634703 4.773530 4.751241 4.837210 4.835834 4.947405\r\n [15] 5.023067 5.159392 5.259393 4.897024 5.154236 4.748580 4.790054\r\n [22] 4.403228 4.522648 4.519479 4.480582 4.687750 4.701154 4.851093\r\n [29] 4.652568 4.847791 4.811578 4.864686 4.672642 4.530416 4.582749\r\n [36] 4.682431 4.717240 4.746443 4.652665 4.466197 4.611190 4.706513\r\n [43] 4.568209 4.517622 4.872942 5.065789 5.186852 5.390533 5.395041\r\n [50] 5.507209 5.403271 5.174482 5.179670 5.038712 5.020135 4.838939\r\n [57] 4.875701 4.755078 4.865224 5.176775 5.202352 5.000563 4.797047\r\n [64] 4.894503 4.810376 5.004196 4.977340 4.848640 4.753013 4.961929\r\n [71] 5.142875 5.096611 5.248953 5.181127 4.941060 4.842180 4.693671\r\n [78] 4.603321 4.722901 4.707204 4.667018 4.490093 4.642128 4.688560\r\n [85] 4.940980 5.010917 4.865457 5.077085 4.943111 5.104771 5.225281\r\n [92] 5.405689 5.459406 5.772019 5.873998 5.653444 5.727537 5.800159\r\n [99] 5.719428 5.649400 5.519840 5.130266 4.799206 5.049435 4.941485\r\n[106] 4.868625 4.976469 5.154863 5.039641 5.037770 5.202060 4.829763\r\n[113] 5.054458 5.091318 5.113392 5.056769 4.999436 5.110106 5.070160\r\n[120] 5.305183 5.148242 5.163269 5.116071 5.209866 5.295613 5.295760\r\n[127] 5.642222 5.797642 5.800138 5.454873 5.221126 5.037245 5.077385\r\n[134] 5.216140 5.121762 4.768109 4.833714 5.100003 5.221173 5.314504\r\n[141] 5.166415 4.883192 4.762374 4.661057 4.620171 4.638887 4.789642\r\n[148] 4.625148 4.791990 5.013448 4.746997 5.084247 4.989471 4.899552\r\n[155] 4.728081 4.728852 4.656302 4.596832 4.789755 4.571342 4.750549\r\n[162] 4.828835 4.946644 4.904696 4.951820 4.962249 4.952014 5.015733\r\n[169] 4.920095 4.695109 4.624958 4.687815 5.038474 5.314062 5.471601\r\n[176] 5.659262 5.667469 5.904322 5.968823 6.073087 5.663232 5.407968\r\n[183] 5.177870 5.237016 5.445955 5.679831 5.614257 5.233444 5.227926\r\n[190] 5.097925 5.119121 4.940067 4.803742 4.593282 4.749424 5.008870\r\n[197] 4.902099 5.014811 5.048332 5.111487 5.059727 4.972699 4.866232\r\n[204] 4.952064 4.924344 5.077133 5.166955 5.172722 5.304330 5.370433\r\n[211] 5.299762 5.238768 5.450415 5.399515 5.197358 5.101200 5.005289\r\n[218] 5.243733 5.194603 5.205039 5.192346 5.082026 5.030877 5.072784\r\n[225] 5.032299 4.637538 4.781121 4.812846 4.758887 4.541770 4.712547\r\n[232] 4.636478 4.876790 5.177345 4.831910 4.870811 5.106333 5.162062\r\n[239] 4.990127 5.058875 4.603333 4.441803 4.618171 4.585108 4.444892\r\n[246] 4.505732 4.827083 4.840013 5.098275 5.081742\r\n\r\nNotice that when we call test_pad1 we get a vector with several NA values appended to the front. And when we look at the length of each of these vectors, we can see that they’re length 250\r\n\r\n\r\nlength(test_pad1)\r\n\r\n\r\n[1] 250\r\n\r\nlength(test_pad2)\r\n\r\n\r\n[1] 250\r\n\r\nStep 4: Use Functions to Add Columns to Data\r\nNow that we have a function that reliably outputs a vector the same length as the columns in our dataframe, we can use it in conjunction with other tidyverse operations to add columns to our dataframe.\r\n\r\n\r\ndf %>%\r\n  mutate(roll_avg = ee_roll_func_padded(val, window = 8, fn = mean))\r\n\r\n\r\n# A tibble: 250 x 3\r\n     day   val roll_avg\r\n   <int> <dbl>    <dbl>\r\n 1     1  2.64    NA   \r\n 2     2  5.30    NA   \r\n 3     3  4.29    NA   \r\n 4     4  5.76    NA   \r\n 5     5  3.75    NA   \r\n 6     6  4.89    NA   \r\n 7     7  4.50    NA   \r\n 8     8  3.84     4.37\r\n 9     9  4.74     4.63\r\n10    10  6.41     4.77\r\n# ... with 240 more rows\r\n\r\nFinally, what if we wanted to get the rolling mean, standard deviation, min, and max all as new columns in our dataframe using the function we created. Our function allows us to pass in whichever aggregation function we want to use (well, probably not any function), so we can use pmap() from {purrr} to iterate over multiple functions and, in combination with the {glue} package, also set meaningful names for the new variables.\r\nI’ll set up a dataframe called params that has the names of the new variables and the corresponding functions, then I’ll loop over these names and functions to create new columns in our original dataframe. I’m not going to go over all of the code here, but if you’re curious, it might be helpful to look at the documentation for {glue}, {purrr}, and possibly {rlang} (for the := operator).\r\n\r\n\r\nlibrary(glue)\r\nparams <- tibble(\r\n  names = c(\"roll_avg\", \"roll_sd\", \"roll_min\", \"roll_max\"),\r\n  fn = lst(mean, sd, min, max)\r\n)\r\nparams %>%\r\n  pmap_dfc(~df %>%\r\n             transmute(\"{.x}\" := ee_roll_func_padded(val, window = 8, fn = .y))) %>%\r\n  bind_cols(df, .)\r\n\r\n\r\n# A tibble: 250 x 6\r\n     day   val roll_avg roll_sd roll_min roll_max\r\n   <int> <dbl>    <dbl>   <dbl>    <dbl>    <dbl>\r\n 1     1  2.64    NA     NA        NA       NA   \r\n 2     2  5.30    NA     NA        NA       NA   \r\n 3     3  4.29    NA     NA        NA       NA   \r\n 4     4  5.76    NA     NA        NA       NA   \r\n 5     5  3.75    NA     NA        NA       NA   \r\n 6     6  4.89    NA     NA        NA       NA   \r\n 7     7  4.50    NA     NA        NA       NA   \r\n 8     8  3.84     4.37   0.982     2.64     5.76\r\n 9     9  4.74     4.63   0.691     3.75     5.76\r\n10    10  6.41     4.77   0.918     3.75     6.41\r\n# ... with 240 more rows\r\n\r\nThis gives us, for each 8-day window (e.g. day 1-8, day 2-9, etc) an average, standard deviation, minimum, and maximum of the val column.\r\nWrapping Up\r\nAs sort of a final note, this activity was meant to be both an exercise for me in working through some programming using window functions as well as a walkthrough/tutorial for others interested in writing functions. That said, when I dive back into the Kaggle data I mentioned earlier, I’ll use the functions from the {RcppRoll} package rather than my own. These are optimized to run quickly because they use C++ code and they’re going to be more efficient than anything I just wrote. This doesn’t matter much when we use a little 250 observation dataframe for demonstration, but it will make a difference working with several thousand observations at once.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-rva-pets/",
    "title": "RVA Pets",
    "description": "Analyzing pet ownership in RVA",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-04-23",
    "categories": [],
    "contents": "\r\nI recently stumbled across the RVA Open Data Portal and, when browsing through the datasets available, noticed they had one on pet licenses issued by the city. Since I’m a huge dog fan & love our pitty Nala more than most people in my life, I figured I’d splash around in the data a little bit to see what I can learn about pets in RVA. You can get the data here, although note that the most recent data is from April 2019.\r\nFirst, let’s load our packages and set our plot themes/colors\r\n\r\n\r\n\r\nNext, we’ll read in the data and clean it up a little bit. In this dataset, each row represents a licensed pet in Richmond, Virginia. The dataset includes animal type (dog, cat, puppy, kitten) and the address of the owners. Whoever set up the data was also nice enough to include longitude and latitude for each address in the dataset, which means I don’t need to go out and get it. For our purposes here, I’m going to lump puppies in with dogs and kittens in with cats. I’m also going to extract the “location” column into a few separate columns. Let’s take a look at the first few entries.\r\n\r\n\r\npets_raw <- read_csv(here::here(\"data/rva_pets_2019.csv\"))\r\npets_clean <- pets_raw %>%\r\n  clean_names() %>%\r\n  extract(col = location_1,\r\n          into = c(\"address\", \"zip\", \"lat\", \"long\"),\r\n          regex = \"(.*)\\n.*(\\\\d{5})\\n\\\\((.*), (.*)\\\\)\") %>%\r\n  mutate(animal_type = str_replace_all(animal_type, c(\"Puppy\" = \"Dog\", \"Kitten\" = \"Cat\")))\r\nhead(pets_clean) %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\r\n\r\n\r\n\r\nanimal_type\r\n\r\n\r\nanimal_name\r\n\r\n\r\naddress\r\n\r\n\r\nzip\r\n\r\n\r\nlat\r\n\r\n\r\nlong\r\n\r\n\r\nload_date\r\n\r\n\r\nCat\r\n\r\n\r\nMolly\r\n\r\n\r\n301 Virginia Street APT 1008\r\n\r\n\r\n23219\r\n\r\n\r\n37.53294\r\n\r\n\r\n-77.433825\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nSam\r\n\r\n\r\n1407 Wilmington Avenue\r\n\r\n\r\n23227\r\n\r\n\r\n37.58294\r\n\r\n\r\n-77.455213\r\n\r\n\r\n20180627\r\n\r\n\r\nCat\r\n\r\n\r\nTaffy\r\n\r\n\r\n114 N Harvie Street\r\n\r\n\r\n23220\r\n\r\n\r\n37.548414\r\n\r\n\r\n-77.45745\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nJackson\r\n\r\n\r\n4804 Riverside Drive\r\n\r\n\r\n23225\r\n\r\n\r\n37.527326\r\n\r\n\r\n-77.483249\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nCirrus\r\n\r\n\r\n3107 E Marshall Street\r\n\r\n\r\n23223\r\n\r\n\r\n37.52904\r\n\r\n\r\n-77.412272\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nHenri\r\n\r\n\r\n1900 Maple Shade Lane\r\n\r\n\r\n23227\r\n\r\n\r\n37.581979\r\n\r\n\r\n-77.466207\r\n\r\n\r\n20180627\r\n\r\n\r\nOk, now that our data is set up, let’s see if there are more cats or dogs in the city.\r\n\r\n\r\npets_clean %>%\r\n  count(animal_type) %>%\r\n  ggplot(aes(x = n, y = animal_type)) +\r\n  geom_col(color = pal[1], fill = pal[1]) +\r\n  geom_text(aes(x = n-50, label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n  labs(\r\n    title = \"Number of Cats vs Dogs\"\r\n  )\r\n\r\n\r\n\r\n\r\nAlright, so, lots more dogs. Like almost 4 to 1 dogs to cats. Which is something I can get behind. I’m a firm believer in the fact that dogs are wayyy better than cats.\r\nI’m also interested in the most common names for pets in RVA.\r\n\r\n\r\npets_clean %>%\r\n  group_by(animal_type) %>%\r\n  count(animal_name, sort = TRUE) %>%\r\n  slice(1:15) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = n, y = reorder_within(animal_name, n, animal_type))) +\r\n    geom_col(color = pal[1], fill = pal[1]) +\r\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - .25, n - 1), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n    facet_wrap(~animal_type, scales = \"free\") +\r\n    scale_y_reordered() +\r\n    labs(\r\n      title = \"Top Pet Names\",\r\n      y = NULL\r\n    )\r\n\r\n\r\n\r\n\r\nThese seem pretty standard to me, and unfortunately, nothing is screaming “RVA” here. No “Bagels,” no “Gwars,” etc.\r\nI also pulled out zip codes into their own column earlier, so we can take a look at which zip codes have the most dogs and cats.\r\n\r\n\r\npets_clean %>%\r\n  filter(!is.na(zip)) %>%\r\n  group_by(zip) %>%\r\n  count(animal_type, sort = TRUE)%>%\r\n  ungroup() %>%\r\n  group_by(animal_type) %>%\r\n  top_n(n = 10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = n, y = reorder_within(zip, n, animal_type))) +\r\n    geom_col(color = pal[1], fill = pal[1]) +\r\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - 1, n - 4), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n    facet_wrap(~animal_type, scales = \"free\") +\r\n    scale_y_reordered() +\r\n    labs(\r\n      title = \"Number of Pets by Zipcode\",\r\n      y = NULL\r\n    )\r\n\r\n\r\n\r\n\r\nAlright, so most of the pets here live in Forest Hill/generally south of the river in 23225, and another big chunk live in 23220, which covers a few neighborhoods & includes The Fan, which is probably where most of the pet action is.\r\nAnd finally, since we have the latitude and longitude, I can put together a streetmap of the city showing where all of these little critters live. To do this, I’m going to grab some shape files through the OpenStreetMaps API and plot the pet datapoints on top of those.\r\n\r\n\r\npets_map <- st_as_sf(pets_clean %>%\r\n                       filter(!is.na(long)), coords = c(\"long\", \"lat\"),\r\n                     crs = 4326)\r\nget_rva_maps <- function(key, value) {\r\n  getbb(\"Richmond Virginia United States\") %>%\r\n    opq() %>%\r\n    add_osm_feature(key = key,\r\n                    value = value) %>%\r\n    osmdata_sf()\r\n}\r\nrva_streets <- get_rva_maps(key = \"highway\", value = c(\"motorway\", \"primary\", \"secondary\", \"tertiary\"))\r\nsmall_streets <- get_rva_maps(key = \"highway\", value = c(\"residential\", \"living_street\",\r\n                                                         \"unclassified\",\r\n                                                         \"service\", \"footway\", \"cycleway\"))\r\nriver <- get_rva_maps(key = \"waterway\", value = \"river\")\r\ndf <- tibble(\r\n  type = c(\"big_streets\", \"small_streets\", \"river\"),\r\n  lines = map(\r\n    .x = lst(rva_streets, small_streets, river),\r\n    .f = ~pluck(., \"osm_lines\")\r\n  )\r\n)\r\ncoords <- pluck(rva_streets, \"bbox\")\r\nannotations <- tibble(\r\n  label = c(\"<span style='color:#FFFFFF'><span style='color:#EBCC2A'>**Cats**<\/span> and <span style='color:#3B9AB2'>**Dogs**<\/span> in RVA<\/span>\"),\r\n  x = c(-77.555),\r\n  y = c(37.605),\r\n  hjust = c(0)\r\n)\r\nrva_pets <- ggplot() +\r\n  geom_sf(data = df$lines[[1]],\r\n          inherit.aes = FALSE,\r\n          size = .3,\r\n          alpha = .8, \r\n          color = \"white\") +\r\n  geom_sf(data = pets_map, aes(color = animal_type), alpha = .6, size = .75) +\r\n  geom_richtext(data = annotations, aes(x = x, y = y, label = label, hjust = hjust), fill = NA, label.color = NA, \r\n                label.padding = grid::unit(rep(0, 4), \"pt\"), size = 11, family = \"Bahnschrift\") + \r\n  coord_sf(\r\n    xlim = c(-77.55, -77.4),\r\n    ylim = c(37.5, 37.61),\r\n    expand = TRUE\r\n  ) +\r\n  theme_void() +\r\n  scale_color_manual(\r\n    values = colors\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\",\r\n    plot.background = element_rect(fill = \"grey10\"),\r\n    panel.background = element_rect(fill = \"grey10\"),\r\n    text = element_markdown(family = \"Bahnschrift\")\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-04-rva-pets/rva-pets_files/figure-html5/counts-1.png",
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-04-scrantonicity-part-2/",
    "title": "Scrantonicity - Part 2",
    "description": "K means clustering using The Office dialogue",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-04-16",
    "categories": [],
    "contents": "\r\nWould I rather be feared or loved? Easy. Both.\r\nA few weeks ago, I did some exploratory analyses of dialogue from The Office. That blog could easily have been a lot longer than it was, and so instead of writing some gigantic post that would have taken 30 minutes+ to read, I decided to separate it out into several different blog posts. And so here’s volume 2.\r\nIn this post, I want to try using k-means clustering to identify patterns in who talks to whom in different episodes.\r\nOnce again, huge thanks to Brad Lindblad, the creator of the {schrute} package for R, which makes the dialogue from The Office easy to work with.\r\nSetup\r\nAs in the previous blog, I’ll be using the {schrute} package to get the transcripts from the show, and I’m going to limit the dialogue to the first 7 seasons of the show, which is when Michael Scott was around. I’ll also use a handful of other packages for data cleaning, analysis, and visualization. Let’s load all of this in and do some general setup.\r\n\r\n\r\n\r\nI’m not superstitious, but I am a little stitious.\r\nNow that we have our data read in and our packages loaded, let’s start with the cluster analysis. The goal here is going to be to figure out if there are certain “types” (clusters, groups, whatever you want to call them) of episodes. There are several frameworks we could use to go about doing this. One approach would be a mixture modeling approach (e.g. latent profile analysis, latent class analysis). I’m not doing that here because I want each episode to be an observation when we cluster, and I’m not sure we have enough episodes here to get good model fits using this approach. Instead, I’m going to use k-means clustering, which basically places observations (episodes, in this case) into one of k groups (where k is supplied by the user) by trying to minimize the “distance” between each observation and the center of the group. The algorithm iteratively assigns observations to groups, updates the center of each group, reassigns observations to groups, etc. until it reaches a stable solution.\r\nWe can also include all sorts of different variables in the k-means algorithm to serve as indicators. For this analysis, I’m going to use the number of exchanges between different characters per episode – i.e. the number of exchanges between Michael and Jim, between Jim and Dwight, etc. – to estimate groups. This could tell us, for instance, that one “type” of Office episode features lots of exchanges between Michael and Dwight, lots between Pam and Jim, and few between Pam and Michael. One consideration when we use the k-means algorithm is that, because we’re looking at distance between observations, we typically want our observations to be on the same scale. Fortunately, since all of our indicators will be “number of lines per episode,” they’re already on the same scale, so we don’t need to worry about standardizing.\r\nLet’s go ahead and set up our data. I’m also going to decide to only use the 5 characters who speak the most during the first 7 seasons in this analysis, otherwise the number of combinations of possible exchanges would be huge. These five characters are:\r\n\r\n\r\ntop5_chars <- office %>%\r\n  count(character, sort = TRUE) %>%\r\n  top_n(5) %>%\r\n  pull(character)\r\ntop5_chars\r\n\r\n\r\n[1] \"Michael\" \"Dwight\"  \"Jim\"     \"Pam\"     \"Andy\"   \r\n\r\nOk, so our top 5 characters here are Michael, Dwight, Jim, Pam, and Andy. Since Andy doesn’t join the show until season 3, I’m actually going to narrow our window of usable episodes to those in seasons 3-7. Otherwise, the clustering algorithm would likely group episodes with a focus on those in seasons 1 and 2, where Andy will obviously have 0 lines, vs episodes in later seasons.\r\nAdditionally, we want to code our changes so that Michael & Jim is the same as Jim & Michael.\r\n\r\n\r\ncombos <- t(combn(top5_chars, 2)) %>%\r\n  as_tibble() %>%\r\n  mutate(comb = glue::glue(\"{V1}&{V2}\"),\r\n         comb_inv = glue::glue(\"{V2}&{V1}\"))\r\nreplace_comb <- combos$comb\r\nnames(replace_comb) <- combos$comb_inv\r\noffice_exchanges <- office %>%\r\n  filter(as.numeric(season) >= 3) %>%\r\n  mutate(char2 = lead(character)) %>% #this will tell us who the speaker is talking to\r\n  filter(character %in% top5_chars &\r\n         char2 %in% top5_chars &\r\n         character != char2) %>% #this filters down to just exchanges between our top 5 characters\r\n  mutate(exchange = glue::glue(\"{character}&{char2}\") %>%\r\n           str_replace_all(replace_comb)) %>% #these lines ensure that, e.g. Michael & Jim is coded the same as Jim & Michael\r\n  select(season, episode_name, character, char2, exchange) %>%\r\n  count(season, episode_name, exchange) %>%\r\n  pivot_wider(names_from = exchange,\r\n              values_from = n,\r\n              values_fill = list(n = 0))\r\nhead(office_exchanges)\r\n\r\n\r\n# A tibble: 6 x 12\r\n  season episode_name `Dwight&Andy` `Dwight&Jim` `Dwight&Pam`\r\n   <int> <chr>                <int>        <int>        <int>\r\n1      3 A Benihana ~             6           10           17\r\n2      3 Back from V~             1           16            6\r\n3      3 Beach Games              8            8            3\r\n4      3 Ben Franklin             0           14            2\r\n5      3 Branch Clos~             0            5            1\r\n6      3 Business Sc~             0           10            3\r\n# ... with 7 more variables: `Jim&Andy` <int>, `Jim&Pam` <int>,\r\n#   `Michael&Andy` <int>, `Michael&Dwight` <int>,\r\n#   `Michael&Jim` <int>, `Michael&Pam` <int>, `Pam&Andy` <int>\r\n\r\nGreat – now our data is all set up so that we know the number of lines exchanged between main characters in each episode. We can run some clustering algorithms now to see if there are patterns in these exchanges. To do this, we’ll fit models testing out 1-10 clusters. We’ll then look at the error for each of these models graphically and use this to choose how many clusters we want to include in our final model.\r\n\r\n\r\nclusters_fit <- tibble(\r\n  k = c(1:10),\r\n  km_fit = map(c(1:10), ~kmeans(office_exchanges %>% select(-c(1:2)), centers = .))\r\n) %>%\r\n  mutate(within_ss = map_dbl(km_fit, ~pluck(., 5)))\r\nclusters_fit %>%\r\n  ggplot(aes(x = k, y = within_ss)) +\r\n  geom_point() +\r\n  geom_line() +\r\n  labs(\r\n    title = \"Within Cluster Sum of Squares vs K\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see that error decreases as we add more clusters, and error will always decrease as k increases. But we can also see that the rate of decrease slows down a bit as we increase our number of clusters. Ideally, there would be a definitive bend, or “elbow” in this plot where the rate of decrease levels off (which is also the number of clusters we’d choose), but that’s not quite the case here. It seems like there’s some slight elbow-ing at 5 clusters, so let’s just go ahead and choose that. Now we can look at the patterns of exchanges in each of these clusters.\r\n\r\n\r\noffice_clustered <- augment(clusters_fit$km_fit[[5]], data = office_exchanges)\r\nclusters_long <- office_clustered %>%\r\n  mutate(season = as_factor(season)) %>%\r\n  group_by(.cluster) %>%\r\n  summarize_if(is.numeric, mean, na.rm = TRUE) %>%\r\n  ungroup() %>%\r\n  pivot_longer(cols = -c(\".cluster\"),\r\n               names_to = \"chars\",\r\n               values_to = \"lines\")\r\nclusters_long %>%\r\n  ggplot(aes(x = lines, y = chars, fill = .cluster)) +\r\n    geom_col() +\r\n    facet_wrap(~.cluster, ncol = 2, scales = \"free_y\") +\r\n    #scale_y_reordered() +\r\n    scale_fill_ipsum() +\r\n    theme_minimal() +\r\n    labs(\r\n      title = \"Types of Office Episodes\"\r\n    ) +\r\n    theme(\r\n      legend.position = \"none\"\r\n    )\r\n\r\n\r\n\r\n\r\nSo, these plots show us the average number of exchanges between characters by cluster. Cluster 1 episodes seem to center around exchanges between Michael and Pam, and we also see a fair amount of exchanges between Michael & Jim, Michael & Dwight, and Jim & Pam. Cluster 2 episodes overwhelmingly feature interactions between Michael and Dwight. Cluster 3 episodes have relatively few exchanges between all of our main characters – this probably means that there’s a lot of side character action going on (recall that we didn’t include exchanges between anyone other than Michael, Dwight, Jim, Pam, and Andy in our clustering algorithm). Cluster 4 episodes have a lot of Michael and Andy interactions, along with a fair number of Michael-Dwight and Jim-Pam interactions. And Cluster 5 seems to be predominantly Michael and Jim, but also a fair amount of Michael-Dwight and Dwight-Jim, which makes sense. Usually when Jim talks to Michael in the show, Dwight finds a way to intrude.\r\nOne thing to remember is that these clusters aren’t necessarily balanced. As the table below shows, most episodes fit into Cluster 3.\r\n\r\n\r\noffice_clustered %>%\r\n  count(.cluster, name = \"num_episodes\") %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\", \"hover\"))\r\n\r\n\r\n\r\n.cluster\r\n\r\n\r\nnum_episodes\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n2\r\n\r\n\r\n10\r\n\r\n\r\n3\r\n\r\n\r\n60\r\n\r\n\r\n4\r\n\r\n\r\n8\r\n\r\n\r\n5\r\n\r\n\r\n17\r\n\r\n\r\nAnother thing to keep in mind is that, across the all of the characters, Michael has far and away the most lines, so his interactions tend to drive this clustering. If we centered and scaled our variables, this would likely change, but we’d also lose some of the interpretability that comes with working in the raw metrics.\r\nFinally, let’s just choose a random episode from each cluster to see which episodes are falling into which categories.\r\n\r\n\r\noffice_clustered %>%\r\n  group_by(.cluster) %>%\r\n  sample_n(size = 1) %>%\r\n  select(.cluster, season, episode_name) %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"striped\"))\r\n\r\n\r\n\r\n.cluster\r\n\r\n\r\nseason\r\n\r\n\r\nepisode_name\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\nWomen’s Appreciation\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\nThe Coup\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\nDiwali\r\n\r\n\r\n4\r\n\r\n\r\n7\r\n\r\n\r\nAndy’s Play\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\nThe Merger\r\n\r\n\r\nThat’s all for now. I might do one more with some predictive modeling in the future.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-04-scrantonicity-part-2/scrantonicity-part-2_files/figure-html5/cluster-1.png",
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-04-riddler-express-march-20-2020/",
    "title": "Riddler Express - March 20, 2020",
    "description": "Solving a math puzzle and exploring the accumulate() function.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\r\nOne of my personal goals for 2020 is to improve my proficiency doing data-y things – mostly using R, but potentially other software as well. Typically, I’ve been using data from the #TidyTuesday project to practice data visualization and data from Kaggle, personal research projects, and other potentially interesting datasets to work on statistical modeling. I recently discovered The Riddler series – a weekly math/logic puzzle – that seems to be a good medium for brushing up on other skills (e.g. certain types of math and programming) that may not come up as often when I do visualizations or statistics.\r\nThe Problem\r\nAnyway, this post solves the Riddler Express puzzle from March 20, 2020. The problem is this:\r\n\r\nA manager is trying to produce sales of his company’s widget, so he instructs his team to hold a sale every morning, lowering the price of the widget by 10 percent. However, he gives very specific instructions as to what should happen in the afternoon: Increase the price by 10 percent from the sale price, with the (incorrect) idea that it would return it to the original price. The team follows his instructions quite literally, lowering and then raising the price by 10 percent every day.\r\n\r\n\r\nAfter N days, the manager walks through the store in the evening, horrified to see that the widgets are marked more than 50 percent off of their original price. What is the smallest possible value of N?\r\n\r\nI’ll walk through a couple of ways to solve this – first, I’ll solve it algebraically, and next, I’ll solve it by “brute force” using the accumulate() function from the {purrr} package.\r\nSolving Algebraically\r\nSo, the first thing that strikes me when reading this is that it’s essentially a compounding interest problem, except in this case the interest is negative. That is, rather than gaining value exponentially over the number of compounding periods, we’re losing value exponentially. The formula for calculating compound interest is:\r\n\\[A = P(1 + r)^n\\]\r\nwhere A equals the final amount, P equals the principal (our initial value), r equals the interest rate, and n equals the number of compounding periods (the number of days in this case). We’re interested in solving for the value of n where our final amount, A, is less than .5. Our principal amount, P, in this case, is 1 (i.e. 100% of the value). So, our equation looks like this:\r\n\\[.5 > ((1-1*.1)*1.1)^n\\]\r\nThe internal logic here is that we subtract 10% from our initial value (1-1*.1) to represent the 10% decrease in price in the morning, then multiply this resulting value by 1.1 to represent the subsequent 10& increase in price in the afternoon. This simplifies to:\r\n\\[.5 > .99^n\\]\r\nFrom here, we can just solve by taking the log of each side and then dividing, which get us our answer\r\n\r\n[1] 68.96756\r\n\r\nRounding this up (since we’re dealing in full days), we can say that after 69 days, the price of the widget will be below 50% of its initial price.\r\nSolving using accumulate()\r\nWe can also solve this problem using the accumulate() function from the {purrr} package, which is part of the {tidyverse}. Essentially, accumulate() will take a function, evaluate it, and then pass the result of the evaluation back into the function, evaluate it again, pass the new result back into the function, etc. This makes it useful for solving problems like this one, where the end price of the widget on the previous day is the starting price of the widget on the current day.\r\nFirst, let’s load our packages. For this, we’ll just use {tidyverse}\r\n\r\n\r\n\r\nNext, let’s set up a function that, if we give it the price of the widget at the beginning of the day, will calculate the price of the widget at the end of the day.\r\n\r\n\r\ndiscount_func <- function(x) {\r\n  (x-x*.1)*1.1\r\n}\r\n\r\n\r\n\r\nAnd then let’s test this function manually a few times.\r\n\r\n\r\ndiscount_func(1)\r\n\r\n\r\n[1] 0.99\r\n\r\ndiscount_func(.99)\r\n\r\n\r\n[1] 0.9801\r\n\r\ndiscount_func(.9801)\r\n\r\n\r\n[1] 0.970299\r\n\r\nNow, we can use accumulate() to automate what we just did manually. The first argument in accumulate() is, in this case, each day that we want to pass into the function. In the code below, I’m testing this for days 0-3 (but coded as 1-4 because we want the start value to be 1). The second argument is the function we just wrote.\r\n\r\n\r\naccumulate(1:4, ~discount_func(.))\r\n\r\n\r\n[1] 1.000000 0.990000 0.980100 0.970299\r\n\r\nAnd we can see that the values returned match our manual tests above, which is good!\r\nNow, we can use accumulate() to make a table with the end price of the widget each day. Note that because we want to start the widget price at 1, our first “day” in the table is day 0, which represents the beginning price of the widget on day 1.\r\n\r\n\r\ndays_tbl <- tibble(\r\n  day = c(0:1000),\r\n  end_price = accumulate(c(1:1001), ~discount_func(.))\r\n)\r\nhead(days_tbl)\r\n\r\n\r\n# A tibble: 6 x 2\r\n    day end_price\r\n  <int>     <dbl>\r\n1     0     1    \r\n2     1     0.99 \r\n3     2     0.980\r\n4     3     0.970\r\n5     4     0.961\r\n6     5     0.951\r\n\r\nAnd then we can plot the end price over time. I’ve added a little bit of transparency to each point so we can more easily see the clustering/overlap.\r\n\r\n\r\nggplot(days_tbl, aes(x = day, y = end_price)) +\r\n  geom_point(alpha = .3) +\r\n  theme_minimal() +\r\n  labs(\r\n    title = \"End Price of Widget over Time\"\r\n  )\r\n\r\n\r\n\r\n\r\nFinally, we can find the day where the end price is below .5 by filtering our table to only those where the price is less than .5 and then returning the first row.\r\n\r\n\r\ndays_tbl %>%\r\n  filter(end_price <= .5) %>%\r\n  slice(1)\r\n\r\n\r\n# A tibble: 1 x 2\r\n    day end_price\r\n  <int>     <dbl>\r\n1    69     0.500\r\n\r\nAnd we can see that this matches our algebraic result – great success!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-02-scrantonicity-part-1/",
    "title": "Scrantonicity - Part 1",
    "description": "An initial exploration of dialogue from The Office",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-03-14",
    "categories": [],
    "contents": "\r\nI Just Want to Lie on the Beach and Eat Hog Dogs\r\nWho doesn’t love The Office? I went through high school and college following the on-again off-again romance of Jim and Pam, the Icarus-esque ascendancy and fall of Ryan the Temp, and the perpetual cringey-ness of Michael Scott. And aside from that handful of people who fled the room in a cold panic at even the mention of “Scott’s Tots,” I think this was probably true for most of my generation. You’d be hard pressed to go to a Halloween party in the late aughts without seeing someone dressed in the tan-and-yellow palette of Dwight Schrute, and before the modern era of Netflix and Hulu, we regularly set aside Thursday nights to tune into NBC.\r\nAnd although I was a big Office fan several years ago, I haven’t thought too too much about it recently – at least until I stumbled across the release of the {schrute} package recently. {schrute} is an R package with one purpose – presenting the entire transcripts of The Office in tibble format, making the dialogue of the show much easier to analyze. I played around with the package and a quick sentiment analysis back in December when I looked at the sentiments expressed by Jim and Pam over the course of the series:\r\n\r\nThere’s a ton more we can do with the package, though, and with the transcripts available and in a clean format, plus all of the tools R has available for text analysis, I figured I’d do a mini-series of blog posts analyzing some of the data. The plan (as of now) is to start this first post with some exploratory analyses and visualizations, then move into some other types of modeling in later posts. I’ll also include all of my code throughout.\r\n\r\nAs a quick aside, a lot of the text analyses I’m going to work through in this first post come from the Text Mining with R book by Julia Silge and David Robinson. I’d strongly recommend this to anyone looking to dive into analyzing text data.\r\nSetup\r\nFirst, let’s read in the data. I’m also going to limit the data to the first seven seasons, which spans the “Michael Scott” era. Not only because these are the best seasons (which they undoubtedly are), but also because doing so eliminates a major confounding factor (i.e. Steve Carell leaving the show) from the analysis.\r\n\r\n\r\noffice <- theoffice %>%\r\n  filter(as.numeric(season) <= 7)\r\n\r\nglimpse(office)\r\n\r\n\r\nRows: 41,348\r\nColumns: 12\r\n$ index            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...\r\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pil...\r\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\"...\r\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Dan...\r\n$ character        <chr> \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Mic...\r\n$ text             <chr> \"All right Jim. Your quarterlies look ve...\r\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look ve...\r\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, ...\r\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706...\r\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005...\r\n\r\nJust to check that the data we have matches what we’re expecting, let’s take a look at which seasons we have, plus how many episodes we have per season.\r\n\r\n\r\noffice %>%\r\n  distinct(season, episode) %>%\r\n  count(season, name = \"num_episodes\")\r\n\r\n\r\n# A tibble: 7 x 2\r\n  season num_episodes\r\n   <int>        <int>\r\n1      1            6\r\n2      2           22\r\n3      3           23\r\n4      4           14\r\n5      5           26\r\n6      6           24\r\n7      7           24\r\n\r\nThis generally matches what Wikipedia is telling me once we account for two-part episodes, and we can see that we only have the first seven seasons.\r\nMe think, why waste time say lot word, when few word do trick\r\nA few questions we can ask here involve how much/how often different characters speak. Probably the most basic question is: who has the most lines?\r\n\r\n\r\ntop_20_chars <- office %>%\r\n  count(character, sort = TRUE) %>%\r\n  top_n(20) %>%\r\n  pull(character)\r\noffice %>%\r\n  filter(is.element(character, top_20_chars)) %>%\r\n  count(character, sort = TRUE) %>%\r\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\r\n  geom_col(fill = purple) +\r\n  labs(\r\n    x = \"\",\r\n    y = \"Number of Lines\",\r\n    title = \"Who Has the Most Lines?\"\r\n  ) +\r\n  coord_flip()\r\n\r\n\r\n\r\n\r\nIt’s not surprising to me that Michael has the most lines, but the magnitude of the difference between him and Dwight is a bit surprising.\r\nWhat if we look at the number of lines per season?\r\n\r\n\r\noffice %>%\r\n  filter(is.element(character, top_20_chars)) %>%\r\n  count(character, season, sort = TRUE) %>%\r\n  ggplot(aes(x = as.numeric(season), y = n, color = character)) +\r\n    geom_line() +\r\n    geom_point()\r\n\r\n\r\n\r\n\r\nThis isn’t terribly informative – let’s go back to our bar graph.\r\n\r\n\r\noffice %>%\r\n  filter(is.element(character, top_20_chars)) %>%\r\n  count(character, season, sort = TRUE) %>%\r\n  group_by(season) %>%\r\n  top_n(n = 5) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\r\n    geom_col(fill = purple) +\r\n    coord_flip() +\r\n    facet_wrap(~season, scales = \"free\") +\r\n    labs(\r\n      title = \"Number of Lines by Season\",\r\n      x = \"\",\r\n      y = \"\"\r\n    ) +\r\n    theme_minimal()\r\n\r\n\r\n\r\n\r\nAgain, not surprising that Michael has the most lines across all seasons. Dwight, Jim, and Pam are always the next three, but the orders change a bit between seasons. The fifth spot is where we see some movement, with Oscar and Jan sneaking in before Andy joins the show in Season 3. And check out Ryan in S4!\r\nSometimes I’ll start a sentence and I don’t even know where it’s going\r\nSo, above, we just looked at the number of lines each character had. Another option is to do some analyses at the word level. For instance, we can look at patterns of word usage for individual characters, between characters, and over time.\r\nTo start with this, I’m going to restructure the data so we have one word per row in our tibble. I’m also going to remove “stop words” (e.g. “a,” “the,” “at”), since these will show up a lot but (for our purposes) aren’t actually all that meaningful:\r\n\r\n\r\noffice_words <- office %>%\r\n  unnest_tokens(word, text) %>%\r\n  anti_join(stop_words)\r\nglimpse(office_words)\r\n\r\n\r\nRows: 125,040\r\nColumns: 12\r\n$ index            <int> 1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 6, 6, 6, 6...\r\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pil...\r\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\"...\r\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Dan...\r\n$ character        <chr> \"Michael\", \"Michael\", \"Michael\", \"Jim\", ...\r\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look ve...\r\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, ...\r\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706...\r\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005...\r\n$ word             <chr> \"jim\", \"quarterlies\", \"library\", \"told\",...\r\n\r\nWe can see that we have a new column, word, with one word per row. We can also see that the only words in the first line of dialogue (All right Jim. Your quarterlies look very good. How are things at the library?) that make it through the stop words filter are jim, quarterlies, and library. We could fiddle with the stop words list if we wanted to keep words like “good” or “things,” but I’m not too concerned about that for now.\r\nAs a first pass, let’s take a look at our 20 characters with the most lines of dialogue and see what each character’s most commonly-used word is:\r\n\r\n\r\noffice_words %>%\r\n  filter(is.element(character, top_20_chars)) %>%\r\n  count(character, word, sort = TRUE) %>%\r\n  group_by(character) %>%\r\n  top_n(n = 1) %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"hover\"))\r\n\r\n\r\n\r\ncharacter\r\n\r\n\r\nword\r\n\r\n\r\nn\r\n\r\n\r\nMichael\r\n\r\n\r\nyeah\r\n\r\n\r\n563\r\n\r\n\r\nDwight\r\n\r\n\r\nmichael\r\n\r\n\r\n280\r\n\r\n\r\nJim\r\n\r\n\r\nyeah\r\n\r\n\r\n274\r\n\r\n\r\nPam\r\n\r\n\r\nmichael\r\n\r\n\r\n257\r\n\r\n\r\nJan\r\n\r\n\r\nmichael\r\n\r\n\r\n159\r\n\r\n\r\nAndy\r\n\r\n\r\nyeah\r\n\r\n\r\n138\r\n\r\n\r\nKevin\r\n\r\n\r\nyeah\r\n\r\n\r\n79\r\n\r\n\r\nDavid\r\n\r\n\r\nmichael\r\n\r\n\r\n67\r\n\r\n\r\nRyan\r\n\r\n\r\nyeah\r\n\r\n\r\n66\r\n\r\n\r\nOscar\r\n\r\n\r\nmichael\r\n\r\n\r\n65\r\n\r\n\r\nPhyllis\r\n\r\n\r\nmichael\r\n\r\n\r\n59\r\n\r\n\r\nToby\r\n\r\n\r\nmichael\r\n\r\n\r\n50\r\n\r\n\r\nDarryl\r\n\r\n\r\nna\r\n\r\n\r\n48\r\n\r\n\r\nKelly\r\n\r\n\r\ngod\r\n\r\n\r\n44\r\n\r\n\r\nAngela\r\n\r\n\r\ndwight\r\n\r\n\r\n40\r\n\r\n\r\nHolly\r\n\r\n\r\nyeah\r\n\r\n\r\n39\r\n\r\n\r\nErin\r\n\r\n\r\nmichael\r\n\r\n\r\n37\r\n\r\n\r\nKaren\r\n\r\n\r\nyeah\r\n\r\n\r\n28\r\n\r\n\r\nStanley\r\n\r\n\r\nmichael\r\n\r\n\r\n27\r\n\r\n\r\nMeredith\r\n\r\n\r\nwait\r\n\r\n\r\n22\r\n\r\n\r\nSo, that’s not great. We can see that our stop words didn’t pick up “yeah.” One way around this would be to filter out additional words like “yeah,” “hey,” etc. that aren’t in our stop words list. But we’ll probably still leave out some common words that we might not want to show up in our exploration. A better approach is probably to use the tf-idf statistics (term frequency-inverse document frequency), which adjusts the weight a term is given in the analysis for each character by how commonly the word is used by all characters, with more common words getting lower weights. Essentially, this lets us figure out which words are important/unique to each of our characters.\r\n\r\n\r\noffice_words %>%\r\n  filter(is.element(character, top_20_chars)) %>%\r\n  count(character, word, sort = TRUE) %>%\r\n  bind_tf_idf(word, character, n) %>%\r\n  group_by(character) %>%\r\n  top_n(n = 5, wt = tf_idf) %>%\r\n  slice(1:5) %>%\r\n  ungroup() %>%\r\n  ggplot() +\r\n    geom_col(aes(x = reorder_within(word, tf_idf, within = character), y = tf_idf), fill = purple) +\r\n    facet_wrap(~character, scales = \"free\") +\r\n    coord_flip() +\r\n    scale_x_reordered() +\r\n    theme_minimal() +\r\n    labs(\r\n      x = \"\",\r\n      y = \"\",\r\n      title = \"Which Words are Important to Which Characters?\"\r\n    ) +\r\n    theme(\r\n      axis.text.x = element_blank()\r\n    )\r\n\r\n\r\n\r\n\r\nThis looks right – we see that “tuna” and “nard” are important to Andy, which totally makes sense. Some other gems in here are “wuphf” for Ryan, “wimowheh” for Jim, and “awesome” for Kevin.\r\nNext, let’s take a closer look at how Michael’s speech compares to some of the other main characters – Dwight, Jim, and Pam. We’ll also leave Kelly in here because I think she’ll be interesting to compare to Michael.\r\n\r\n\r\nmain_char_words <-  office_words %>%\r\n  filter(character %in% c(\"Michael\", \"Dwight\", \"Jim\", \"Pam\", \"Kelly\"),\r\n         str_detect(word, \"\\\\d+\", negate = TRUE)) %>%\r\n  count(character, word) %>%\r\n  group_by(character) %>%\r\n  mutate(word_prop = n/sum(n)) %>%\r\n  ungroup() %>%\r\n  select(-n) %>%\r\n  pivot_wider(names_from = character,\r\n              values_from = word_prop)\r\nchar_plot <- function(df, char) {\r\n  df %>%\r\n  select(word, Michael, {{char}}) %>%\r\n  mutate(color = log(abs(Michael-{{char}}))) %>%\r\n  ggplot(aes(y = Michael, x = {{char}})) +\r\n    geom_text(aes(label = word, color = color), check_overlap = TRUE, vjust = 1) +\r\n    geom_abline(color = \"grey50\", lty = 2) +\r\n    scale_x_log10(labels = scales::percent_format()) +\r\n    scale_y_log10(labels = scales::percent_format()) +\r\n    scale_color_distiller(\r\n      type = \"seq\",\r\n      palette = \"Purples\",\r\n      direction = 1\r\n    ) +\r\n    theme_minimal() +\r\n    theme(\r\n      legend.position = \"none\"\r\n    )\r\n}\r\nmain_char_words %>%\r\n  char_plot(Dwight)\r\n\r\n\r\n\r\n\r\nOk, so let’s walk through how to read this. For a given word, the y-axis shows how frequently Michael uses that word, and the x-axis shows how frequently Dwight uses that word. The diagonal dotted line represents equal usage – words that appear on or close to the line are words that Michael and Dwight use about as frequently as one another. Words above the line are those that Michael uses more; words below the line are those that Dwight uses more. Words closer to the line will appear lighter in the graph, whereas words farther way will have more color. So, looking at the graph, we can see that Dwight and Michael both say “hey” pretty often and use the word more or less equally. Dwight says “Mose” way more often than Michael does (because it’s farther from the line), whereas Michael says “Scott” more often than Dwight.\r\nLet’s take a look at what these graphs look like for Jim and Pam\r\n\r\n\r\nmain_char_words %>%\r\n  char_plot(Jim)\r\n\r\n\r\n\r\n\r\n\r\n\r\nmain_char_words %>%\r\n  char_plot(Pam)\r\n\r\n\r\n\r\n\r\nAand let’s throw Kelly in there too because it might be interesting.\r\n\r\n\r\nmain_char_words %>%\r\n  char_plot(Kelly)\r\n\r\n\r\n\r\n\r\nWhat we see here is that, at least when compared to Michael, Kelly’s speech is pretty idiosyncratic – there are lots of words (“blah”, “bitch”, “god”) that she uses waaaayy more frequently than Michael does.\r\nAnd finally (for this section), I would be remiss if I made it through an analysis of how characters from The Office speak without giving a “that’s what she said” tally…\r\n\r\n\r\noffice %>%\r\n  filter(str_detect(text, \"what she said\")) %>%\r\n  count(character) %>%\r\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\r\n    geom_col(fill = purple) +\r\n    labs(\r\n      x = \"\",\r\n      y = \"Count\",\r\n      title = \"That's What She Said!\"\r\n    ) +\r\n    coord_flip()\r\n\r\n\r\n\r\n\r\nNot at all a surprise….\r\nIdentity theft is not a joke, Jim!\r\nFinally, I want to visualize who characters talk to. To do this, I’m going to put together a network plot showing links between characters based on how frequently they interact.\r\n\r\n\r\nset.seed(0408)\r\noffice_links <- office %>%\r\n  filter(character %in% top_20_chars) %>%\r\n  group_by(episode) %>%\r\n  mutate(to = lead(character)) %>%\r\n  ungroup() %>%\r\n  rename(from = character) %>%\r\n  count(from, to) %>%\r\n  filter(from != to,\r\n         !is.na(to),\r\n         n > 25)\r\noffice_verts <- office_links %>%\r\n  group_by(from) %>%\r\n  summarize(size = log(sum(n), base = 2)) %>%\r\n  ungroup()\r\nnetwork_graph <- graph_from_data_frame(office_links, vertices = office_verts)\r\nnetwork_graph %>%\r\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\r\n  geom_edge_link(aes(edge_alpha = n^.5), color = purple, edge_width = 1) +\r\n  geom_node_point(aes(size = size, color = size)) +\r\n  geom_node_text(aes(label = name, size = size), repel = TRUE, family = \"Garamond\", fontface = \"bold\") +\r\n  scale_color_distiller(\r\n      type = \"seq\",\r\n      palette = \"Purples\",\r\n      direction = 1\r\n    ) +\r\n  labs(\r\n    title = \"Who Talks to Whom in The Office?\"\r\n  ) +\r\n  theme_void() +\r\n  theme(\r\n    legend.position = \"none\",\r\n    plot.title = element_text(hjust = .5)\r\n  )\r\n\r\n\r\n\r\n\r\nThe network graph shows links between characters. The size and color of the node (point) associated with a person corresponds to the the total number of interactions they have, with larger and purple-r nodes representing more interactions. The color of the link between characters also corresponds to the number of interactions between two characters, with darker purple links representing more interactions and lighter links representing fewer interactions. Also, characters with more total interactions are sorted toward the center of the network, which is where we see Michael, Jim, Pam, and Dwight. Finally, interactions are only shown if characters have more than 25 total interactions (this prevents the graph from showing a jillion lines).\r\nI’m going to wrap this one up here, but later on I’ll probably play around a bit with doing some statistical modeling – predicting who is speaking, who a character is speaking to, something like that.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-05T10:29:20-05:00",
    "input_file": {}
  }
]
