[
  {
    "path": "posts/2021-01-06-unconsciousness-in-the-xmen/",
    "title": "Unconsciousness in the Xmen",
    "description": "Practicing poisson regression using Xmen data.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-07-14",
    "categories": [],
    "contents": "\r\nA part of me has always wanted to get into comic books. I think it would be a really good fit for me – I’m definitely a nerd. I play video games, I read fantasy novels, I code/do data science for fun. Comic books should be right up my alley. But for whatever reason, I’ve never taken the plunge. Maybe it’s a time commitment thing. Maybe I know I’ll like them too much. Maybe it’s too daunting to figure out how to start. Regardless, even thought I’m not into comic books, they are intriguing to me, and the X-Men particularly so, which is why I wanted to take a little bit of time to analyze this X-men data promoted by the #tidytuesday project.\r\nThe other main purpose of this blog post is to toy around with running a Poisson regression. A few months ago, I saw a post about how the tidymodels framework had some new “parsnip-adjacent” packages, with one being {poissonreg} which fits – you guessed it – Poisson regressions. I haven’t had much reason to use Poisson regression in any of my previous work or in datasets I’ve toyed around with, but this X-men dataset seems like a good excuse to try it out. So, onward and upward!\r\nSetup\r\nFirst, I’ll load some packages, set some miscellaneous options, and import the data. This data comes from the Claremont Run project, which mines data from Chris Claremont’s run (1975-1991) writing the X-men comics. To learn more about the project, you can visit the website. There are several datasets available, but for this analysis, I’m going to use data from the characters dataset, the character_visualization dataset, and the locations dataset.\r\n\r\n\r\n\r\nExploring the Data\r\nLet’s first look at the characters dataset. In this dataset, each row corresponds to a character in an issue, and each column corresponds to actions or events relevant to that character. Here’s a glimpse of that data:\r\n\r\n\r\ncharacters %>%\r\n  glimpse()\r\n\r\n\r\nRows: 4,209\r\nColumns: 34\r\n$ issue                                         <dbl> 97, 97, 97,...\r\n$ character                                     <chr> \"Professor ...\r\n$ rendered_unconcious                           <dbl> 0, 0, 0, 1,...\r\n$ captured                                      <dbl> 0, 0, 0, 0,...\r\n$ declared_dead                                 <dbl> 0, 0, 0, 0,...\r\n$ redressed                                     <dbl> 0, 0, 0, 0,...\r\n$ depowered                                     <dbl> 0, 0, 0, 0,...\r\n$ clothing_torn                                 <dbl> 0, 0, 0, 0,...\r\n$ subject_to_torture                            <dbl> 0, 0, 0, 0,...\r\n$ quits_team                                    <dbl> 0, 0, 0, 0,...\r\n$ surrenders                                    <dbl> 0, 0, 0, 0,...\r\n$ number_of_kills_humans                        <dbl> 0, 0, 0, 0,...\r\n$ number_of_kills_non_humans                    <dbl> 0, 0, 0, 0,...\r\n$ initiates_physical_conflict                   <chr> NA, NA, \"1\"...\r\n$ expresses_reluctance_to_fight                 <dbl> NA, NA, 1, ...\r\n$ on_a_date_with_which_character                <chr> NA, NA, NA,...\r\n$ kiss_with_which_character                     <chr> NA, NA, NA,...\r\n$ hand_holding_with_which_character             <chr> \"Moira MacT...\r\n$ dancing_with_which_character                  <chr> NA, NA, NA,...\r\n$ flying_with_another_character                 <chr> NA, NA, NA,...\r\n$ arm_in_arm_with_which_character               <chr> NA, NA, NA,...\r\n$ hugging_with_which_character                  <chr> NA, NA, NA,...\r\n$ physical_contact_other                        <chr> \"Moira MacT...\r\n$ carrying_with_which_character                 <chr> NA, NA, NA,...\r\n$ shared_bed_with_which_character               <lgl> NA, NA, NA,...\r\n$ shared_room_domestically_with_which_character <lgl> NA, NA, NA,...\r\n$ explicitly_states_i_love_you_to_whom          <chr> NA, NA, NA,...\r\n$ shared_undress                                <chr> NA, NA, NA,...\r\n$ shower_number_of_panels_shower_lasts          <dbl> 0, 0, 0, 0,...\r\n$ bath_number_of_panels_bath_lasts              <dbl> 0, 0, 0, 0,...\r\n$ depicted_eating_food                          <dbl> 1, 0, 0, 0,...\r\n$ visible_tears_number_of_panels                <dbl> 0, 0, 0, 0,...\r\n$ visible_tears_number_of_intances              <dbl> 0, 0, 0, 0,...\r\n$ special_notes                                 <chr> NA, NA, NA,...\r\n\r\nSo, we can see in this dataset things like who Professor X held hands with in issue 97, how many humans were killed by Magneto in issue 105, etc. We see lots of NAs and 0s in this dataset. The only column I’m going to use from this is the rendered unconscious column, which will be outcome variable in the models later.\r\nIn the character_visualization dataset, each row represents a per-issue count of the number of times a character is depicted, speaks, thinks, has a narrative statement (I think this is probably only relevant for the narrator character?), either when the character is in costume or not in costume.\r\n\r\n\r\ncharacter_visualization %>%\r\n  glimpse()\r\n\r\n\r\nRows: 9,800\r\nColumns: 7\r\n$ issue     <dbl> 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...\r\n$ costume   <chr> \"Costume\", \"Costume\", \"Costume\", \"Costume\", \"Co...\r\n$ character <chr> \"Editor narration\", \"Omnipresent narration\", \"P...\r\n$ speech    <dbl> 0, 0, 0, 7, 24, 0, 11, 9, 10, 0, 0, 0, 0, 0, 0,...\r\n$ thought   <dbl> 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\r\n$ narrative <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\r\n$ depicted  <dbl> 0, 0, 0, 10, 23, 0, 9, 17, 17, 5, 0, 0, 0, 0, 0...\r\n\r\nIn the location dataset, each row corresponds to a location in which part of the issue takes place, with as many locations listed per issue as appear in that issue. The dataset also includes a “context” column that describes things like whether the location is shown in the present, as part of a flashback, in a dream, etc. Here’s a glimpse:\r\n\r\n\r\nlocations %>%\r\n  glimpse()\r\n\r\n\r\nRows: 1,413\r\nColumns: 4\r\n$ issue    <dbl> 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 99, 99, ...\r\n$ location <chr> \"Space\", \"X-Mansion\", \"Rio Diablo Research Facil...\r\n$ context  <chr> \"Dream\", \"Present\", \"Present\", \"Present\", \"Prese...\r\n$ notes    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Cut...\r\n\r\nAcross these datasets, it probably makes the most sense to aggegrate data up to the issue level, since that’s kind of the lowest common denominator here. So, essentially the question I’m going to try to answer in this blog post is:\r\nWhat features of an X-men issue predict how many characters are rendered unconscious in that issue?\r\nFirst, let’s look at the distribution of rendered unconscious:\r\n\r\n\r\ncharacters %>%\r\n  count(issue, wt = rendered_unconcious, sort = TRUE) %>%\r\n  ggplot(aes(x = n)) +\r\n  geom_histogram(fill = lann, bins = 8)\r\n\r\n\r\n\r\n\r\nRight, so, this is a pretty strongly right-skewed distribution, which is sort of what we’d expect from a Poisson distribution, especially one with a low expected number of events (which I’d imagine is the case in comic books).\r\nCleaning, Aggregating, and Joining\r\nNext, let’s aggregate our data up to the issue level. This will give us data where a row represents an issue rather than a character within an issue or a location within an issue. We’ll start with the characters dataset. There’s a lot we could do with this data, but because there are only 183 issues represented in this dataset, we need to be cognizant about how many predictors we’re including. So the only variable I’m going to use here is rendered unconscious as the outcome, which will represent the number of characters rendered unconscious in a given issue.\r\n\r\n\r\nrend_df <- characters %>%\r\n  group_by(issue) %>%\r\n  summarize(rendered_unconscious = sum(rendered_unconcious, na.rm = FALSE))\r\n\r\n\r\n\r\nNext, let’s work on the character_visualization dataset. Again, trying to keep the number of predictors relatively small, I’m going to winnow this down to represent counts of how many times a handful of key characters are depicted in each issue. I don’t know a ton about the X-men, but I know who some of the more important characters are, so I’m going to choose Wolverine, Professor X, Magneto, and Jean Grey here.\r\n\r\n\r\nchar_sum <- character_visualization %>%\r\n  filter(str_detect(character, \"Wolverine|Xavier|Jean Grey|Magneto\")) %>%\r\n  group_by(issue, character) %>%\r\n  summarize(depict = sum(depicted, na.rm = FALSE)) %>%\r\n  mutate(character = case_when(\r\n    str_detect(character, \"Jean Grey\") ~ \"Jean_Grey\",\r\n    str_detect(character, \"Wolv\") ~ \"Wolverine\",\r\n    str_detect(character, \"Magneto\") ~ \"Magneto\",\r\n    str_detect(character, \"Xavier\") ~ \"Professor_X\"\r\n  )) %>%\r\n  pivot_wider(\r\n    names_from = character,\r\n    values_from = depict\r\n  )\r\n\r\n\r\n\r\nNext, let’s work on our locations dataset. First, let’s look at the most common locations. Again, since we only have 183 rows in our dataset that we’re modeling with, I only want to choose a handful of variables to include in the model here.\r\n\r\n\r\nlocations %>%\r\n  count(location, sort = TRUE)\r\n\r\n\r\n# A tibble: 785 x 2\r\n   location                             n\r\n   <chr>                            <int>\r\n 1 X-Mansion                          100\r\n 2 Danger Room                         27\r\n 3 Space                               19\r\n 4 Muir Island, Scotland               14\r\n 5 Unspecified region in Australia     14\r\n 6 Eagle Plaza, Dallas Texas           11\r\n 7 Central Park                        10\r\n 8 Morlock residence under New York    10\r\n 9 Princess Lilandra's Home Planet     10\r\n10 San Francisco                       10\r\n# ... with 775 more rows\r\n\r\nOk, so, I’m just going to go with the 3 most common locations: the X-mansion, the Danger Room (whatever that is), and Space. Danger Room sounds to me like a place where people might be rendered unconscious.\r\n\r\n\r\nuse_locs <- locations %>%\r\n  count(location, sort = TRUE) %>%\r\n  top_n(3) %>%\r\n  pull(location)\r\nlocs_sum <- locations %>%\r\n  group_by(issue) %>%\r\n  summarize(mansion = use_locs[[1]] %in% location,\r\n            danger_room = use_locs[[2]] %in% location,\r\n            space = use_locs[[3]] %in% location) %>%\r\n  mutate(across(where(is_logical), as.numeric))\r\n\r\n\r\n\r\nThis will return a dataset that tells us whether a given issue has the X-mansion, the Danger Room, or Space as a location.\r\n\r\n\r\nlocs_sum %>%\r\n  glimpse()\r\n\r\n\r\nRows: 183\r\nColumns: 4\r\n$ issue       <dbl> 97, 98, 99, 100, 101, 102, 103, 104, 105, 106...\r\n$ mansion     <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, ...\r\n$ danger_room <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...\r\n$ space       <dbl> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, ...\r\n\r\nNow we can join the three datasets into one useful for modeling. I’m using an inner join here because, for whatever reason, the character visualization dataset has more issues represented than the others, and we only want issues that are represented in all 3 dataframes.\r\n\r\n\r\nissues_joined <- reduce(list(rend_df, char_sum, locs_sum), ~inner_join(.x, .y, by = \"issue\"))\r\n\r\n\r\n\r\nModeling\r\nCool, so now we’re done preprocessing our data – now we can specify our model.\r\nI mentioned before that one issue here is that this is a small set of data. We have 183 observations (again, each observation is an issue), which isn’t many. One way to make our modeling more robust is to use bootstrap resampling (see our good friend Wikipedia for an explanation) and to fit models to several resamples.\r\n\r\n\r\nset.seed(0408)\r\nbooties <- bootstraps(issues_joined, times = 100)\r\nhead(booties$splits, n = 5)\r\n\r\n\r\n[[1]]\r\n<Analysis/Assess/Total>\r\n<183/68/183>\r\n\r\n[[2]]\r\n<Analysis/Assess/Total>\r\n<183/66/183>\r\n\r\n[[3]]\r\n<Analysis/Assess/Total>\r\n<183/66/183>\r\n\r\n[[4]]\r\n<Analysis/Assess/Total>\r\n<183/70/183>\r\n\r\n[[5]]\r\n<Analysis/Assess/Total>\r\n<183/64/183>\r\n\r\nWhat we can see here is that every bootstrap sample has 183 rows in the analysis set, which is what the model will be trained on, and then some other number of rows in the assessment set. This other number is the out-of-bag sample – the rows that weren’t randomly sampled by the bootstrap process.\r\nNext, I’m going to set up a workflow. I think of this as like a little suitcase that can carry things I want to use in my model around – I think that analogy might be from Julia Silge? Anyway, I’m going to start by adding the formula I want to use in my model.\r\n\r\n\r\nxmen_wf <- workflow() %>%\r\n  add_formula(rendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + mansion + danger_room + space)\r\n\r\n\r\n\r\nNow we can further specify the model. Remember that since our outcome is a count, we’ll be fitting a Poisson regression. Looking at the outcome distribution earlier, I don’t think I need to use a zero-inflated model here (although maybe? Again, this isn’t really my expertise), so I’m just going to proceed with a regular Poisson regression, fit using the {glmnet} engine. I’m also going to tune the penalty and mixture arguments, which control the amount of total regularization applied to the model as well as the proportion of the penalty that is L1 (lasso) vs L2 (ridge regression).\r\nBrief Interpolation on what a Poisson regression is A Poisson regression is a generalized linear model (GLM) used to model count data. Like the name implies, GLMs are generalizations of linear models that use a link function, g(), to transform the expected value of the response (outcome) to a linear function of the predictor variables. Poisson regression uses a log link function to accomplish this transformation. For people interested in reading more, I really like John Fox’s book, Applied Regression Analysis.\r\n\r\n\r\nlibrary(poissonreg)\r\npoisson_mod <- poisson_reg(\r\n  penalty = tune(),\r\n  mixture = tune()\r\n) %>%\r\n  set_engine(\"glmnet\")\r\n\r\n\r\n\r\nSince I’m tuning a couple of parameters, I need to make a grid with possible values to tune across\r\n\r\n\r\npoisson_tune <- grid_max_entropy(\r\n  penalty(),\r\n  mixture(), \r\n  size = 10\r\n)\r\n\r\n\r\n\r\nAnd I’ll drop the model spec into the previous workflow.\r\n\r\n\r\nxmen_wf <- xmen_wf %>%\r\n  add_model(poisson_mod)\r\nxmen_wf\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Formula\r\nModel: poisson_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\nrendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + \r\n    mansion + danger_room + space\r\n\r\n-- Model -------------------------------------------------------------\r\nPoisson Regression Model Specification (regression)\r\n\r\nMain Arguments:\r\n  penalty = tune()\r\n  mixture = tune()\r\n\r\nComputational engine: glmnet \r\n\r\nAnd now we can fit the model using our bootstrap resamples.\r\n\r\n\r\nxmen_fit <- tune_grid(\r\n  xmen_wf,\r\n  resamples = booties,\r\n  grid = poisson_tune\r\n)\r\n\r\n\r\n\r\nOur models have fit, so now we can look at our results:\r\n\r\n\r\nxmen_fit %>%\r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 20 x 8\r\n    penalty mixture .metric .estimator   mean     n std_err .config   \r\n      <dbl>   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>     \r\n 1 8.81e- 4  0.0155 rmse    standard   1.93     100 0.0894  Preproces~\r\n 2 8.81e- 4  0.0155 rsq     standard   0.0277   100 0.00398 Preproces~\r\n 3 4.67e- 7  0.0676 rmse    standard   1.93     100 0.0896  Preproces~\r\n 4 4.67e- 7  0.0676 rsq     standard   0.0277   100 0.00398 Preproces~\r\n 5 5.56e- 1  0.148  rmse    standard   1.71     100 0.0201  Preproces~\r\n 6 5.56e- 1  0.148  rsq     standard   0.0288   100 0.00426 Preproces~\r\n 7 4.76e-10  0.190  rmse    standard   1.93     100 0.0895  Preproces~\r\n 8 4.76e-10  0.190  rsq     standard   0.0277   100 0.00398 Preproces~\r\n 9 1.09e- 2  0.500  rmse    standard   1.92     100 0.0841  Preproces~\r\n10 1.09e- 2  0.500  rsq     standard   0.0278   100 0.00403 Preproces~\r\n11 2.44e- 7  0.517  rmse    standard   1.94     100 0.0896  Preproces~\r\n12 2.44e- 7  0.517  rsq     standard   0.0277   100 0.00398 Preproces~\r\n13 1.73e-10  0.622  rmse    standard   1.94     100 0.0896  Preproces~\r\n14 1.73e-10  0.622  rsq     standard   0.0277   100 0.00398 Preproces~\r\n15 1.10e- 5  0.881  rmse    standard   1.94     100 0.0897  Preproces~\r\n16 1.10e- 5  0.881  rsq     standard   0.0277   100 0.00398 Preproces~\r\n17 1.99e- 1  0.942  rmse    standard   1.69     100 0.0190  Preproces~\r\n18 1.99e- 1  0.942  rsq     standard   0.0302   100 0.00404 Preproces~\r\n19 5.97e-10  0.985  rmse    standard   1.94     100 0.0897  Preproces~\r\n20 5.97e-10  0.985  rsq     standard   0.0277   100 0.00398 Preproces~\r\n\r\nOk, so, my limited understanding of Poisson regression is that neither RMSE or R-squared values are ideal metrics, and some googling led me to find that there’s an open issue to add a Poisson log loss metric to the yardstick package, so we’ll gloss over these for now.\r\nAnyway, let’s pick the best model here, finalize the model, and then fit it to our full training data.\r\n\r\n\r\nbest_params <- xmen_fit %>%\r\n  select_best(metric = \"rmse\")\r\nfinal_mod <- xmen_wf %>%\r\n  finalize_workflow(best_params) %>%\r\n  fit(data = issues_joined)\r\n\r\n\r\n\r\nAnd let’s check out how important how variables are. This should give us the coefficients from our model.\r\n\r\n\r\nfinal_mod %>%\r\n  pull_workflow_fit() %>% \r\n  vi()\r\n\r\n\r\n# A tibble: 7 x 3\r\n  Variable    Importance Sign \r\n  <chr>            <dbl> <chr>\r\n1 mansion        0.157   NEG  \r\n2 danger_room    0.113   NEG  \r\n3 Professor_X    0.0197  POS  \r\n4 Jean_Grey      0.0133  POS  \r\n5 Wolverine      0.00938 POS  \r\n6 Magneto        0.00701 POS  \r\n7 space          0       NEG  \r\n\r\n\r\n\r\nfinal_mod %>%\r\n  pull_workflow_fit() %>% \r\n  vip(num_features = 7, fill = lann)\r\n\r\n\r\n\r\n\r\nRight, so, one thing to keep in mind here is that the location variables and the character variables are on different scales, so the effects aren’t directly comparable. But the interpretation here is that more appearances of Professor X are more strongly associated with more characters rendered unconscious in an issue than are more appearances of Magneto, although all of these coefficients are positive, suggesting that more appearances of any of these four characters are associated with more renderings unconscious in that issue. Similarly, the effects of danger_room and mansion are negative, suggesting that if the issue features either of those locations, there tend to be fewer characters rendered unconscious. The coefficient for space is 0, which probably means it got regularized out. Probably the most important piece, here, though, is that these effects seem to be very small, which means they likely don’t actually matter.\r\nI’m going to call it right here. Even though the model I built doesn’t seem to have much explanatory power, it forced me to read some more about Poisson regression and to dig back into the tidymodels framework, which I’ll count as a win. Plus it gives me an excuse to gather “domain knowledge” about comic books so I can do a better job next time.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-06-unconsciousness-in-the-xmen/unconsciousness-in-the-xmen_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-01-06T21:44:37-05:00",
    "input_file": "unconsciousness-in-the-xmen.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-06-exploring-emily-osters-covid-data/",
    "title": "Exploring Emily Oster's COVID Data",
    "description": "Examining child care survey data from Emily Oster",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-06-25",
    "categories": [],
    "contents": "\r\nI’m a big fan of Emily Oster. Well, her work; I don’t know her personally, but she seems cool. As someone with a background in academic research, and also as a new parent, I really appreciate the thoroughness, honesty, and approachability of Cribsheet. And I’m glad she summarized all of the academic articles so I didn’t have to look for them. More recently, I’ve been a big fan of her Parent Data newsletter, which I read (on days it comes out) while my daughter Emma is napping in the morning. Likewise, her COVID Explained website has been an equally thorough/honest/approachable website for all (or most, at least) things COVID.\r\nUnlike most other people who have data-centric websites and blogs, I’ve avoided doing any analyses related to COVID data. I don’t think the world needs yet another COVID-tracker website that someone built as part of a data science portfolio, and I’m honestly not interested enough in COVID to keep up with the data, which is changing seemingly by the second. That said, my actual job is in the Early Childhood office of the Virginia Department of Education, plus I have a 5 month old daughter who just started daycare. So early education/child care is kind of my jam, and so when I saw the Parent Data newsletter with some survey data Emily collected relating to childcare centers during COVID, I figured I could take a peek.\r\nSome Caveats\r\nBefore getting into the exploration here, I’m just going to copy and paste the description of the data & data collection process from the Parent Data newsletter:\r\n\r\nWhat did you do?\r\nI distributed, in various ways, a simple survey of child care providers who were open during the pandemic. I got some amazing help from Winnie.com, who sent the >survey out to all their providers. I also sent this out on Twitter, through newsletters, on Facebook, etc, etc. And then I collated responses.\r\nIs this a scientifically valid sample and do you plan to publish the results?\r\nNo and no. This is crowdsourced. I didn’t sample randomly and I cannot be sure of the biases in responses. I am of the view (which not everyone will agree with) that some data is better than none.\r\nIs the data perfect? Did you clean it?\r\nNo! Let me know if you see obvious errors. I did minimal cleaning - to remove places which reported fewer than two students during the pandemic or did not report any location data. Basically, I’m going to be cautious about not drawing too many conclusions from this, and so should you. I’m going to see where the data takes me, make some visualizations and summary tables, but I’m not going to, like, change my approach to public health right now as a result of these analyses.\r\n\r\nI’ll also include all of my code inline in this post. I think this helps people see what choices I’m making, but sorry to those who find this a distraction.\r\n\r\n\r\n\r\nCleaning Up Data\r\nLet’s take a glimpse at the data:\r\n\r\n\r\ndf %>%\r\n  glimpse()\r\n\r\n\r\nRows: 986\r\nColumns: 27\r\n$ State                                       <chr> \"Washington\",...\r\n$ `Town/County/City`                          <chr> NA, \"New Have...\r\n$ `Age Ranges`                                <chr> \"6 weeks - 6 ...\r\n$ `Single or Multiple Locations?`             <chr> \"Single\", \"Mu...\r\n$ `Opening Details`                           <chr> \"Open the who...\r\n$ `Number of Students Served During Pandemic` <dbl> 4, 25, 10, 60...\r\n$ `Number of Staff During Pandemic`           <dbl> NA, 19, NA, 2...\r\n$ `COVID-19 Cases in Children`                <dbl> 0, 0, 0, 2, 0...\r\n$ `COVID-19 Cases in Staff`                   <dbl> 0, 2, 0, 0, 1...\r\n$ ...10                                       <lgl> NA, NA, NA, N...\r\n$ Kids...11                                   <dbl> 0, 25, 0, 60,...\r\n$ Staff...12                                  <dbl> 0, 19, 0, 20,...\r\n$ `Kid COVID...13`                            <dbl> 0, 0, 0, 2, 0...\r\n$ `Staff COVID...14`                          <dbl> 0, 2, 0, 0, 1...\r\n$ Count...15                                  <dbl> 0, 1, 0, 1, 1...\r\n$ ...16                                       <lgl> NA, NA, NA, N...\r\n$ `Count of Kids`                             <dbl> 4, 0, 0, 0, 0...\r\n$ `Count of Staff`                            <dbl> NA, 0, 0, 0, ...\r\n$ `Kids COVID`                                <dbl> 0, 0, 0, 0, 0...\r\n$ `Staff COVID...20`                          <dbl> 0, 0, 0, 0, 0...\r\n$ `Total Centers`                             <dbl> 1, 0, 0, 0, 0...\r\n$ ...22                                       <lgl> NA, NA, NA, N...\r\n$ Kids...23                                   <dbl> NA, 4, 0, 0, ...\r\n$ Staff...24                                  <dbl> NA, NA, 0, 0,...\r\n$ `Kid COVID...25`                            <dbl> NA, 0, 0, 0, ...\r\n$ `Staff COVID...26`                          <dbl> NA, 0, 0, 0, ...\r\n$ Count...27                                  <dbl> NA, 1, 0, 0, ...\r\n\r\nOk, so we see a bunch of columns here that were hidden in the Google Sheet. I downloaded this to an .xlsx to check out what these are, and it looks like they’re columns that hold calculations that feed into the summary sheet. I’m going to drop these for now – I can always replicate the calculations if I need to later.\r\n\r\n\r\ndf <- df %>%\r\n  select(c(1:9)) %>%\r\n  clean_names() %>%\r\n  filter(str_detect(state, \"Minneaota\", negate = TRUE)) %>%\r\n  mutate(id = row_number()) %>%\r\n  select(id, everything())\r\nglimpse(df)\r\n\r\n\r\nRows: 982\r\nColumns: 10\r\n$ id                                        <int> 1, 2, 3, 4, 5, ...\r\n$ state                                     <chr> \"Washington\", \"...\r\n$ town_county_city                          <chr> NA, \"New Haven\"...\r\n$ age_ranges                                <chr> \"6 weeks - 6 mo...\r\n$ single_or_multiple_locations              <chr> \"Single\", \"Mult...\r\n$ opening_details                           <chr> \"Open the whole...\r\n$ number_of_students_served_during_pandemic <dbl> 4, 25, 10, 60, ...\r\n$ number_of_staff_during_pandemic           <dbl> NA, 19, NA, 20,...\r\n$ covid_19_cases_in_children                <dbl> 0, 0, 0, 2, 0, ...\r\n$ covid_19_cases_in_staff                   <dbl> 0, 2, 0, 0, 1, ...\r\n\r\nNow we have a data frame with 9 columns that we can explore. I think what each column represents is pretty obvious given the variable name, so I’m not going to describe each one. I will truncate the names a bit, though, just so I don’t have to type out the number_of_students_served_during_pandemic each time I want to use that variable. One thing that does seem important to point out, though is that each row/observation in the dataset represents a child care program, which could correspond to multiple sites or a single site.\r\n\r\n\r\ndf <- df %>%\r\n  rename(\r\n    location_type = single_or_multiple_locations,\r\n    num_kids = number_of_students_served_during_pandemic,\r\n    num_staff = number_of_staff_during_pandemic,\r\n    covid_kids = covid_19_cases_in_children,\r\n    covid_staff = covid_19_cases_in_staff\r\n  )\r\nglimpse(df)\r\n\r\n\r\nRows: 982\r\nColumns: 10\r\n$ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...\r\n$ state            <chr> \"Washington\", \"Connecticut\", \"Connecticu...\r\n$ town_county_city <chr> NA, \"New Haven\", \"Stanford\", \"Cook Count...\r\n$ age_ranges       <chr> \"6 weeks - 6 months, 6 months - 1 year, ...\r\n$ location_type    <chr> \"Single\", \"Multiple\", \"Single\", \"Multipl...\r\n$ opening_details  <chr> \"Open the whole time\", \"Open the whole t...\r\n$ num_kids         <dbl> 4, 25, 10, 60, 40, 12, 30, 250, 100, 5, ...\r\n$ num_staff        <dbl> NA, 19, NA, 20, NA, NA, NA, 41, 30, 1, 2...\r\n$ covid_kids       <dbl> 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r\n$ covid_staff      <dbl> 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0...\r\n\r\nWho Responded\r\nFirst, let’s take a look at who responded to the survey based on the state they live in\r\n\r\n\r\ndf %>%\r\n  count(state) %>%\r\n  top_n(20) %>%\r\n  ggplot(aes(x = n, y = fct_reorder(state, n))) +\r\n  geom_col(fill = purple) +\r\n  geom_text(aes(x = n-2, label = n), hjust = 1, color = \"white\", size = 3) +\r\n  labs(\r\n    x = \"Number of Programs Responding\",\r\n    y = \"State\",\r\n    title = \"Number of Programs Responding by State\",\r\n    caption = \"Only top 20 states included\"\r\n  )\r\n\r\n\r\n\r\n\r\nRight, so, one thing to keep in mind is that we have far more responses from a handful of states – California and Texas mostly, but also Washington and Maryland. Let’s look to see if we see this same pattern in the number of children attending these programs as well as the number of staff working at these programs.\r\n\r\n\r\nfacet_labs <- as_labeller(c(\"num_kids\" = \"Kids\", \"num_staff\" = \"Staff\"))\r\ndf %>%\r\n  pivot_longer(cols = c(\"num_kids\", \"num_staff\"),\r\n               names_to = \"name\",\r\n               values_to = \"value\") %>%\r\n  count(state, name, wt = value) %>%\r\n  group_by(name) %>%\r\n  top_n(10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = n, y = reorder_within(state, n, name), fill = name)) +\r\n  geom_col() +\r\n  facet_wrap(~name, nrow = 2, scales = \"free_y\", labeller = facet_labs) +\r\n  scale_y_reordered() +\r\n  scale_fill_ipsum() +\r\n  #theme_minimal() +\r\n  labs(\r\n    x = \"Count\",\r\n    y = \"\",\r\n    title = \"Number of Kids & Staff by State\",\r\n    caption = \"Top 10 states only\"\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\",\r\n    axis.text.y = element_text(size = 10)\r\n  )\r\n\r\n\r\n\r\n\r\nSo, something is def going on with Texas here – it looks like they have a ton of kids reported proportional to their number of staff. Let’s see if we can look at the ratios for Texas, California, and Minnesota to get a sense of things.\r\n\r\n\r\ndf %>%\r\n  filter(state %in% c(\"Texas\", \"California\", \"Minnesota\")) %>%\r\n  group_by(state) %>%\r\n  summarize(\r\n    kids = sum(num_kids, na.rm = TRUE),\r\n    staff = sum(num_staff, na.rm = TRUE)\r\n  ) %>%\r\n  ungroup() %>%\r\n  mutate(ratio = kids/staff) %>%\r\n  make_table()\r\n\r\n\r\n\r\nstate\r\n\r\n\r\nkids\r\n\r\n\r\nstaff\r\n\r\n\r\nratio\r\n\r\n\r\nCalifornia\r\n\r\n\r\n2189\r\n\r\n\r\n756\r\n\r\n\r\n2.895503\r\n\r\n\r\nMinnesota\r\n\r\n\r\n3142\r\n\r\n\r\n1145\r\n\r\n\r\n2.744105\r\n\r\n\r\nTexas\r\n\r\n\r\n4617\r\n\r\n\r\n1454\r\n\r\n\r\n3.175378\r\n\r\n\r\nThe ratio actually looks reasonable for Texas (and all are very low in the grand scheme of things) – for context, the legal ratio for infant classrooms in Virginia is 4:1, and these are all well below that.\r\nExploring COVID Cases\r\nNow that we have a general sense of who responded, let’s explore the data on the number and rate of COVID cases a bit. First, I’m going to make a scatter plot with the number of people at a center against the number of COVID cases in a center. We would expect the number of cases to increase as the number of people increases, but the shape and strength of this relationship could be interesting.\r\n\r\n\r\ndf_longer <- df %>%\r\n  pivot_longer(cols = c(\"num_kids\", \"num_staff\", \"covid_kids\", \"covid_staff\"),\r\n               names_to = \"name\",\r\n               values_to = \"value\") %>%\r\n  extract(col = \"name\",\r\n          into = c(\"type\", \"group\"),\r\n          regex = \"(.*)_(.*)\") %>%\r\n  pivot_wider(names_from = type,\r\n              values_from = value)\r\ndf_longer %>%\r\n  ggplot(aes(x = covid, y = num, color = group)) +\r\n  geom_point(alpha = .6) +\r\n  labs(\r\n    title = \"Program Population vs Number of COVID Cases\",\r\n    y = \"Population\",\r\n    x = \"COVID Cases\"\r\n  ) +\r\n  facet_wrap(~group, scales = \"free\") +\r\n  scale_color_ipsum() +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nWell, this doesn’t show what I thought it would. I expected a weak but positive association between total program population and number of COVID cases (i.e. as your program has more people, you also have more COVID cases), but we don’t see that here. Probably because COVID cases are so rare, especially in kids, we actually don’t see much of a relationship. Although note that if we plot regression lines (as in the plot below), the estimated relationship is positive, but this is due pretty much entirely to outliers. We could get into some regression diagnostics to confirm it more formally, but eyeballing it is good enough for a blog post.\r\nAnother related point is that it appears a couple of programs are reporting 300 staff. I’d assume this is a data entry error, but it’s also possible that this is a corporate program that’s taking a liberal view of what “staff” means here.\r\n\r\n\r\ndf_longer %>%\r\n  ggplot(aes(x = covid, y = num, color = group)) +\r\n  geom_point(alpha = .6) +\r\n  geom_smooth(method = \"lm\") +\r\n  labs(\r\n    title = \"Program Population vs Number of COVID Cases\",\r\n    y = \"Population\",\r\n    x = \"COVID Cases\"\r\n  ) +\r\n  facet_wrap(~group, scales = \"free\") +\r\n  scale_color_ipsum() +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nAfter seeing this, a better approach might be to plot the infection rate for each program and then take a look at which have particularly high infection rates. We can do this using a beeswarm plot:\r\n\r\n\r\ndf_longer %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  ggplot(aes(y = infect_rate, x = group, color = group)) +\r\n  geom_quasirandom(alpha = .7) +\r\n  labs(\r\n    x = \"\",\r\n    y = \"Infection Rate\",\r\n    title = \"Child Care COVID Infection Rate by Population Type\"\r\n  ) +\r\n  scale_color_ipsum() +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  theme(\r\n    legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe see in this plot that most places are at 0% infections for kids and staff, but a handful show rates higher than 25% for kids and higher than 50% for staff. One program has a 200% infection rate for staff, which must be a data entry error. Since infections are incredibly infrequent, my suspicion is that these high-infection-rate programs have very small populations, so let’s take a look at the top 10:\r\n\r\n\r\ndf_longer %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  arrange(desc(infect_rate)) %>%\r\n  select(-c(\"id\", \"age_ranges\", \"location_type\", \"opening_details\")) %>%\r\n  head(10L) %>%\r\n  make_table()\r\n\r\n\r\n\r\nstate\r\n\r\n\r\ntown_county_city\r\n\r\n\r\ngroup\r\n\r\n\r\nnum\r\n\r\n\r\ncovid\r\n\r\n\r\ninfect_rate\r\n\r\n\r\nNew York\r\n\r\n\r\nWestchester\r\n\r\n\r\nstaff\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2.0000000\r\n\r\n\r\nMaryland\r\n\r\n\r\nAnne Arundel\r\n\r\n\r\nstaff\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.0000000\r\n\r\n\r\nTexas\r\n\r\n\r\nWilliamson\r\n\r\n\r\nstaff\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.0000000\r\n\r\n\r\nWashington\r\n\r\n\r\nKing\r\n\r\n\r\nstaff\r\n\r\n\r\n10\r\n\r\n\r\n9\r\n\r\n\r\n0.9000000\r\n\r\n\r\nMinnesota\r\n\r\n\r\nHennepin\r\n\r\n\r\nstaff\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n0.6666667\r\n\r\n\r\nVirginia\r\n\r\n\r\nArlington\r\n\r\n\r\nstaff\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n0.5000000\r\n\r\n\r\nVirginia\r\n\r\n\r\nArlington\r\n\r\n\r\nstaff\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n0.5000000\r\n\r\n\r\nIndiana\r\n\r\n\r\nMarion\r\n\r\n\r\nstaff\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n0.5000000\r\n\r\n\r\nMinnesota\r\n\r\n\r\nRamsey\r\n\r\n\r\nstaff\r\n\r\n\r\n5\r\n\r\n\r\n2\r\n\r\n\r\n0.4000000\r\n\r\n\r\nNorth Carolina\r\n\r\n\r\nHenderson\r\n\r\n\r\nstaff\r\n\r\n\r\n11\r\n\r\n\r\n4\r\n\r\n\r\n0.3636364\r\n\r\n\r\nOur 200% infection rate program is a site in NY that reported 1 staff member but 2 cases of COVID in staff members. The most surprising piece of data in this table, though, is the program in King, Washington, where 9 of the 10 staff contracted COVID. Also, it’s hard to tell, but I suspect programs in Arlington, VA might be duplicate entries since the data are identical.\r\nAnother question I’m thinking about now is which state had the highest rate of infection:\r\n\r\n\r\ndf_longer %>%\r\n  group_by(state, group) %>%\r\n  summarize(num = sum(num, na.rm = TRUE),\r\n            covid = sum(covid, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  group_by(group) %>%\r\n  top_n(10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(y = reorder_within(state, infect_rate, group), x = infect_rate, fill = group)) +\r\n  geom_col() +\r\n  facet_wrap(~group, nrow = 2, scales = \"free\") +\r\n  labs(\r\n    x = \"Infection Rate\",\r\n    y = \"\",\r\n    title = \"COVID Infection Rates in Child Care Centers by State and Population Type\",\r\n    caption = \"Only the top 10 states are shown \"\r\n  ) +\r\n  scale_y_reordered() +\r\n  scale_x_continuous(labels = scales::percent_format()) +\r\n  scale_fill_ipsum() +\r\n  theme(\r\n    legend.position = \"none\",\r\n    axis.text.y = element_text(size = 10),\r\n    plot.title = element_text(size = 16)\r\n  )\r\n\r\n\r\n\r\n\r\nHere, we see West VA and Tennessee as having the highest infection rates for kids and staff. This is probably driven by low response rates from those states – recall that in our first plot, neither of these states showed up as having a large number of programs responding – but let’s check it out:\r\n\r\n\r\ndf_longer %>%\r\n  filter(state %in% c(\"Tennessee\", \"West Virginia\")) %>%\r\n  group_by(state, group) %>%\r\n  summarize(num = sum(num, na.rm = TRUE),\r\n            covid = sum(covid, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  mutate(infect_rate = covid/num) %>%\r\n  make_table()\r\n\r\n\r\n\r\nstate\r\n\r\n\r\ngroup\r\n\r\n\r\nnum\r\n\r\n\r\ncovid\r\n\r\n\r\ninfect_rate\r\n\r\n\r\nTennessee\r\n\r\n\r\nkids\r\n\r\n\r\n173\r\n\r\n\r\n0\r\n\r\n\r\n0.0000000\r\n\r\n\r\nTennessee\r\n\r\n\r\nstaff\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0.0000000\r\n\r\n\r\nWest Virginia\r\n\r\n\r\nkids\r\n\r\n\r\n60\r\n\r\n\r\n2\r\n\r\n\r\n0.0333333\r\n\r\n\r\nWest Virginia\r\n\r\n\r\nstaff\r\n\r\n\r\n40\r\n\r\n\r\n2\r\n\r\n\r\n0.0500000\r\n\r\n\r\nSo this puzzled me for a while. According to this table, Tennessee actually has a 0% infection rate for both kids and staff and not the ~3% and ~12% rates for kids and staff, respectively, that the above plot indicates. After I just checked over my code several times, it turns out that it’s due to a typo in the survey response – the fictional state of “Tenneessee” has a high infection rate; the actual state of Tennessee does not. West Virginia, though, is showing a higher rate due to sampling error and low response rate (as we can see in the table above).\r\nAnd, finally, since I’m a new dad, let’s take a look at cases in places that accept the youngest kids – ages 6 week through 6 months – and compare those to all programs, just looking at kids and ignoring staff.\r\n\r\n\r\ndf %>%\r\n  mutate(smol_kiddos = if_else(str_detect(age_ranges, \"6 weeks\"), \"Yes\", \"No\")) %>%\r\n  group_by(smol_kiddos) %>%\r\n  summarize(across(c(\"num_kids\", \"covid_kids\"), ~sum(.x, na.rm = TRUE))) %>%\r\n  filter(!is.na(smol_kiddos)) %>%\r\n  ungroup() %>%\r\n  adorn_totals(\"row\") %>%\r\n  mutate(infect_rate = round(100*(covid_kids/num_kids), 2)) %>%\r\n  make_table()\r\n\r\n\r\n\r\nsmol_kiddos\r\n\r\n\r\nnum_kids\r\n\r\n\r\ncovid_kids\r\n\r\n\r\ninfect_rate\r\n\r\n\r\nNo\r\n\r\n\r\n6055\r\n\r\n\r\n6\r\n\r\n\r\n0.10\r\n\r\n\r\nYes\r\n\r\n\r\n18201\r\n\r\n\r\n32\r\n\r\n\r\n0.18\r\n\r\n\r\nTotal\r\n\r\n\r\n24256\r\n\r\n\r\n38\r\n\r\n\r\n0.16\r\n\r\n\r\nSo, there really aren’t differences here between programs that accept the youngest kids and all programs. There might be a difference between programs that accept kids 6 weeks - 6 months and those that don’t, but again, the overall infection rate is so low that it’s hard to tell.\r\nFinal Thoughts\r\nThere’s definitely more I could do with this data, but I just wanted to give it a quick run through. I am intentionally avoiding statistical modeling here, though, because 1) I don’t want to take the time to clean the data thoroughly enough to model it, and 2) I don’t know very much about modeling zero-inflated count data, so we’ll just leave it at this for now. But I did appreciate the opportunity to dig in a little bit – thanks Emily!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-06-exploring-emily-osters-covid-data/exploring-emily-osters-covid-data_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-01-06T21:38:05-05:00",
    "input_file": "exploring-emily-osters-covid-data.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-06-pulling-youtube-transcripts/",
    "title": "Pulling YouTube Transcripts",
    "description": "Example of pulling transcripts for an entire YouTube playlist.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-05-15",
    "categories": [],
    "contents": "\r\nI’ve been a fan of the Your Mom’s House Podcast for a long time now, and I thought it would be interesting to do some analysis of their speech patterns. If you follow the show at all, you know that the conversations are…special (you can check here for a visualization I did of their word usage over time if you’re so inclined). Fortunately, it’s possible to get transcripts of YouTube videos. Getting transcripts for a single video using the {youtubecaption} R package is fairly straightforward; getting transcripts for a full playlist is a touch more involved, so I wanted to create a quick walkthrough illustrating my process for doing this. Hopefully this will help others who might want to analyze text data from YouTube.\r\nSetup\r\nFirst, let’s load the packages we need to pull our data. I’m going to use the following:\r\n{tidyverse} for data wrangling\r\n{youtubecaption} for calling the YouTube API to get transcripts\r\n{janitor} pretty much just for the clean_names() function\r\n{lubridate} to work with the publication_date variable that’s part of the YT video data. (This is optional if you don’t want to work with this variable at all)\r\n\r\n\r\n\r\nGetting Transcripts for a Single Video\r\nLike I mentioned previously, getting transcripts for a single video is pretty easy thanks to the {youtubecaption} package. All we need is the URL for the video and the get_caption() function can go do its magic. I’ll illustrate that here using the most recent YMH podcast full episode.\r\n\r\n\r\nymh_new <- get_caption(\"https://www.youtube.com/watch?v=VMloBlnczzI\")\r\nglimpse(ymh_new)\r\n\r\n\r\nRows: 3,157\r\nColumns: 5\r\n$ segment_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,...\r\n$ text       <chr> \"this episode of your mom's house is\", \"brough...\r\n$ start      <dbl> 0.000, 1.140, 3.659, 7.859, 8.910, 14.820, 20....\r\n$ duration   <dbl> 3.659, 6.719, 5.251, 6.961, 11.879, 9.080, 3.1...\r\n$ vid        <chr> \"VMloBlnczzI\", \"VMloBlnczzI\", \"VMloBlnczzI\", \"...\r\n\r\nWe can see above that this gives us a tibble with the text (auto-transcribed by YouTube) broken apart into short segments and corresponding identifying information for each text segment.\r\nOne thing worth mentioning here is that the transcripts are automatically transcribed by a speech-to-text model. It seems really good, but it will make some mistakes, particularly around brand names and website addresses (in my limited experience).\r\nGetting Transcripts for Several Videos\r\nBut what if we want to get transcripts for several videos? The get_caption() function requires the URL of each video that we want to get a caption for. If you want to analyze transcripts from more than a handful of videos, it would get really tedious really quickly to go and grab the individual URLs. And, more specifically, what if you wanted to get the transcripts for all videos from a single playlist?\r\nGet URLS\r\nI found this tool that will take a YouTube playlist ID and provide an Excel file with, among other information, the URL for each video in the playlist, which is exactly what we need for the get_caption() function.\r\nI used the tool on 5/14/20 to get a file with the data for all of the videos in the YMH Podcast - Full Episodes playlist. I’ll go ahead an upload the file, plus do some light cleaning, in the code below.\r\n\r\n\r\nep_links <- read_csv(\"~/Data/YMH/Data/ymh_full_ep_links.csv\") %>%\r\n  clean_names() %>%\r\n  mutate(ep_num = str_replace_all(title, \".*Ep.*(\\\\d{3}).*\", \"\\\\1\") %>%\r\n           as.double(),\r\n         ep_num = if_else(ep_num == 19, 532, ep_num),\r\n         published_date = mdy_hm(published_date),\r\n         vid = str_replace_all(video_url, \".*=(.*)$\", \"\\\\1\"))\r\nglimpse(ep_links)\r\n\r\n\r\nRows: 223\r\nColumns: 7\r\n$ published_date <dttm> 2020-04-29 12:03:00, 2020-04-22 12:00:00,...\r\n$ video_url      <chr> \"https://www.youtube.com/watch?v=xw3KNj2yw...\r\n$ channel        <chr> \"YourMomsHousePodcast\", \"YourMomsHousePodc...\r\n$ title          <chr> \"Your Mom's House Podcast - Ep. 549\", \"You...\r\n$ description    <chr> \"Want an ad-free experience? Click here to...\r\n$ ep_num         <dbl> 549, 548, 547, 546, 545, 544, 543, NA, 542...\r\n$ vid            <chr> \"xw3KNj2ywVo\", \"_BVQvqPvu-8\", \"HvueqYO--tc...\r\n\r\nWe can see that this gives us the URLs for all 225 episodes in the playlist.\r\nThe cleaning steps for the published_date variable and the vid variable should be pretty universal. The step to get the episode number extracts that from the title of the video, and so this step is specific to the playlist I’m using.\r\n“Safely” Pull Transcripts\r\nNow that we have all of the URLs, we can iterate through all of them using the get_caption() function. Before we do that, though, we want to make the get_caption() robust to failure. Basically, we don’t want the whole series of iterations to fail if one returns an error. In other words, we want the function to get all of the transcripts that it can get and let us know which it can’t, but not to fail if it can’t get every transcript.\r\nTo do this, we just wrap the get_caption() function in the safely() function from {purrr}.\r\n\r\n\r\nsafe_cap <- safely(get_caption)\r\n\r\n\r\n\r\nYou can read more about safely() in the {purrr} documentation, but it basically returns, for each call, a 2-element list: 1 element with the “result” of the function and another with the “error.” If the function succeeds, “error” will be NULL and “result” will have the result of the function. If the function fails, “result” will be NULL and “error” will show the error message.\r\nNow that we have your safe_cap() function, we can use map() from {purrr} to pull transcripts from all of the videos we have URLs for.\r\n\r\n\r\nymh_trans <- map(ep_links$video_url,\r\n                 safe_cap)\r\nglimpse(head(ymh_trans))\r\n\r\n\r\nList of 6\r\n $ :List of 2\r\n  ..$ result: tibble [2,663 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,093 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,727 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [2,701 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,276 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n $ :List of 2\r\n  ..$ result: tibble [3,382 x 5] (S3: tbl_df/tbl/data.frame)\r\n  ..$ error : NULL\r\n\r\nFormat Data\r\nThis returns a list the same length as our vector of URLs (225 in this case) in the format described above. We want to get the “result” element from each of these lists. (You might also be interested in looking at the errors, but any errors are all going to be the same here – basically that a transcript isn’t available for a specific video). To do that, we want to iterate over all elements of our transcript list (using map() again) and use the pluck() function from {purrr} to get the result object. We then used the compact() function to get rid of any NULL elements in this list (remember that the “result” element will be NULL if the function couldn’t get a transcript for the video). This will give us a list of transcripts that the function successfully fetched.\r\nNext, we use the bind_rows() function to take this list and turn it into a tibble. And finally, we can inner_join() this with our tibble that had the URLs so that metadata for each video and transcripts are in the same tibble.\r\n\r\n\r\nres <- map(1:length(ymh_trans),\r\n           ~pluck(ymh_trans, ., \"result\")) %>%\r\n  compact() %>%\r\n  bind_rows() %>%\r\n  inner_join(x = ep_links,\r\n            y = .,\r\n            by = \"vid\")\r\nglimpse(res)\r\n\r\n\r\nRows: 437,098\r\nColumns: 11\r\n$ published_date <dttm> 2020-04-29 12:03:00, 2020-04-29 12:03:00,...\r\n$ video_url      <chr> \"https://www.youtube.com/watch?v=xw3KNj2yw...\r\n$ channel        <chr> \"YourMomsHousePodcast\", \"YourMomsHousePodc...\r\n$ title          <chr> \"Your Mom's House Podcast - Ep. 549\", \"You...\r\n$ description    <chr> \"Want an ad-free experience? Click here to...\r\n$ ep_num         <dbl> 549, 549, 549, 549, 549, 549, 549, 549, 54...\r\n$ vid            <chr> \"xw3KNj2ywVo\", \"xw3KNj2ywVo\", \"xw3KNj2ywVo...\r\n$ segment_id     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\r\n$ text           <chr> \"oh snap there's hot gear merge method\", \"...\r\n$ start          <dbl> 0.030, 4.020, 6.629, 12.450, 14.730, 17.40...\r\n$ duration       <dbl> 6.599, 8.430, 8.101, 4.950, 4.530, 5.600, ...\r\n\r\nHopefully this helps folks & best of luck with your text analyses!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-06T21:32:21-05:00",
    "input_file": "pulling-youtube-transcripts.utf8.md"
  },
  {
    "path": "posts/2021-01-04-writing-window-functions/",
    "title": "Writing Window Functions",
    "description": "Examples and tutorial of writing rolling aggregate functions.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-05-06",
    "categories": [],
    "contents": "\r\nI was recently working on exploring some time series data for a Kaggle competition and found myself wanting to calculate the rolling average of some sales. I don’t often work with time series data, so I had to look up functions to use to calculate rolling averages (n.b. that if you don’t know what a rolling average is, read here), and I was surprised that dplyr doesn’t have one built in. It turns out there are several packages that do have rolling aggregate (average, standard deviation, etc) functions, such as the RcppRoll package and the zoo package. But I also thought it provided a nice opportunity to practice writing some of my own rolling aggregate functions, which I’m going to walk through here.\r\nSetup\r\nFirst, I’m going to load packages. For this, I’m only using {tidyverse} (and within tidyverse, mostly {purrr} for iteration) and {RcppRoll} as a ground-truth to test my functions. I’m also going to use the {glue} package later on, but that’s less central and I’ll load it when I need it.\r\n\r\n\r\n\r\nNext, I’m going to set up a minimal tibble to use for calculations. This will have an day column and a val column. The val column is the one I’m going to be doing calculations on, and the day column is going to serve as an index for the rolling average.\r\n\r\n\r\nset.seed(0408)\r\ndf <- tibble(\r\n  day = c(1:250),\r\n  val = rnorm(250, mean = 5, sd = 1)\r\n)\r\ndf\r\n\r\n\r\n# A tibble: 250 x 2\r\n     day   val\r\n   <int> <dbl>\r\n 1     1  2.64\r\n 2     2  5.30\r\n 3     3  4.29\r\n 4     4  5.76\r\n 5     5  3.75\r\n 6     6  4.89\r\n 7     7  4.50\r\n 8     8  3.84\r\n 9     9  4.74\r\n10    10  6.41\r\n# ... with 240 more rows\r\n\r\nStep 1: Testing Iteration\r\nSo, my process for building this function is going to be to create something very basic with few variables first and then gradually abstract this out to make a more responsive function. Eventually, I’ll get to a point where the rolling aggregation function will be general enough to allow for the specification of arbitrary aggregate functions and windows.\r\nThe first step, then, is just to test the logic of the calculation I need to create to calculate rolling averages. I’ll do this by assuming a 28 day window (we’ll be able to change the window later), create a “truth” to test against using RcppRoll’s roll_mean() function, and then iterate using map().\r\n\r\n\r\ntruth <- roll_mean(df$val, n = 28, align = \"right\")\r\ntest <- map_dbl(\r\n  c(28:length(df$val)), #this represents the days I want to calculate the average for. I'm starting on day 28 (because I want a 28-day rolling average, \r\n  #and the first time I'll have 28 days of data is on day 28) and going through the last day\r\n  function(a) {\r\n    mean(df$val[(a - 27):a], na.rm = FALSE) \r\n  } #this specifies what I'm doing -- taking the mean of the 'val' column for each 28 day window \r\n  #(day 1-28, day 2-29, etc). If I don't subtract 1 window value when I subset, \r\n  #I'll actually get 29 days.\r\n)\r\nall.equal(truth, test) #this tests to see that the vectors are equal.\r\n\r\n\r\n[1] TRUE\r\n\r\nStep 2: Building Out Functions\r\nGreat, so the logic of the calculation works. Now, let’s extend it a little bit to create a function where I can specify the variable I want to use as well as the window I want to take the rolling average over.\r\n\r\n\r\nee_roll_mean <- function(x, window) {\r\n  map_dbl(\r\n    c(window:length(x)),\r\n    function(a) {\r\n      mean(x[(a - window+1):a], na.rm = FALSE)\r\n    }\r\n  )\r\n}\r\ntest_2 <- ee_roll_mean(df$val, 28)\r\nall.equal(test_2, truth)\r\n\r\n\r\n[1] TRUE\r\n\r\nIt works when we set the window value to 28, but let’s also test that it works when we use a different window just to be safe.\r\n\r\n\r\ntruth_win8 <- roll_mean(df$val, n = 8, align = \"right\")\r\ntest_win8 <- ee_roll_mean(df$val, window = 8)\r\nall.equal(truth_win8, test_win8)\r\n\r\n\r\n[1] TRUE\r\n\r\nThis works well for taking the rolling average – we can specify the values we want to take the average over as well as the window for that average. But there are other functions we might be interested in getting rolling aggregates for as well. For instance, we might want to know the minimum or standard deviation of a value during some windows of time. Rather than write separate functions to do this, we can just extend our previous function to allow us to supply whichever aggregation function we want.\r\n\r\n\r\nee_roll_func <- function(x, window, fn = mean) {\r\n  map_dbl(\r\n    c(window:length(x)),\r\n    function(a) {\r\n      fn(x[(a - window+1):a], na.rm = FALSE)\r\n    }\r\n  ) \r\n}\r\ntest_3 <- ee_roll_func(df$val, window = 8, fn = sd)\r\n#testing against the RcppRoll function that does the same thing\r\ntruth_3 <- roll_sd(df$val, n = 8, align = \"right\")\r\nall.equal(test_3, truth_3)\r\n\r\n\r\n[1] TRUE\r\n\r\nStep 3: Pad the Output\r\nOne thing I’m noticing when looking at the output of each of these functions is that the length of the output vectors differ depending on the value we pass to the window argument.\r\n\r\n\r\nlength(test)\r\n\r\n\r\n[1] 223\r\n\r\nlength(test_win8)\r\n\r\n\r\n[1] 243\r\n\r\nI’m also noticing that these outputs are shorter than the length of the input vector (which is length 250). This makes sense because the function can’t take, for example, the 28 day average before the 28th day, and so the length of the output vector will be 27 elements shorter than the length of the input vector.\r\nThis isn’t so great if we want to add the results of this function back into our original df, though, because all of the vectors in a df need to be the same length. One solution is to “pad” our output vector with the appropriate amount of NA values so that it is the same length as the input vector and can therefore get added as a column in our df. So let’s do that.\r\n\r\n\r\nee_roll_func_padded <- function(x, window, fn = mean) {\r\n  map_dbl(\r\n    c(window:length(x)),\r\n    function(a) {\r\n      fn(x[(a - window+1):a], na.rm = FALSE)\r\n    }\r\n  ) %>%\r\n    append(rep(NA_real_, times = window-1), values = .)   #this will pad the front with a number of NAs equal\r\n  #to the window value minus 1\r\n}\r\ntest_pad1 <- ee_roll_func_padded(df$val, window = 8) #note that if we don't supply a function, it will use the mean\r\ntest_pad2 <- ee_roll_func_padded(df$val, window = 20)\r\ntest_pad1\r\n\r\n\r\n  [1]       NA       NA       NA       NA       NA       NA       NA\r\n  [8] 4.372225 4.634703 4.773530 4.751241 4.837210 4.835834 4.947405\r\n [15] 5.023067 5.159392 5.259393 4.897024 5.154236 4.748580 4.790054\r\n [22] 4.403228 4.522648 4.519479 4.480582 4.687750 4.701154 4.851093\r\n [29] 4.652568 4.847791 4.811578 4.864686 4.672642 4.530416 4.582749\r\n [36] 4.682431 4.717240 4.746443 4.652665 4.466197 4.611190 4.706513\r\n [43] 4.568209 4.517622 4.872942 5.065789 5.186852 5.390533 5.395041\r\n [50] 5.507209 5.403271 5.174482 5.179670 5.038712 5.020135 4.838939\r\n [57] 4.875701 4.755078 4.865224 5.176775 5.202352 5.000563 4.797047\r\n [64] 4.894503 4.810376 5.004196 4.977340 4.848640 4.753013 4.961929\r\n [71] 5.142875 5.096611 5.248953 5.181127 4.941060 4.842180 4.693671\r\n [78] 4.603321 4.722901 4.707204 4.667018 4.490093 4.642128 4.688560\r\n [85] 4.940980 5.010917 4.865457 5.077085 4.943111 5.104771 5.225281\r\n [92] 5.405689 5.459406 5.772019 5.873998 5.653444 5.727537 5.800159\r\n [99] 5.719428 5.649400 5.519840 5.130266 4.799206 5.049435 4.941485\r\n[106] 4.868625 4.976469 5.154863 5.039641 5.037770 5.202060 4.829763\r\n[113] 5.054458 5.091318 5.113392 5.056769 4.999436 5.110106 5.070160\r\n[120] 5.305183 5.148242 5.163269 5.116071 5.209866 5.295613 5.295760\r\n[127] 5.642222 5.797642 5.800138 5.454873 5.221126 5.037245 5.077385\r\n[134] 5.216140 5.121762 4.768109 4.833714 5.100003 5.221173 5.314504\r\n[141] 5.166415 4.883192 4.762374 4.661057 4.620171 4.638887 4.789642\r\n[148] 4.625148 4.791990 5.013448 4.746997 5.084247 4.989471 4.899552\r\n[155] 4.728081 4.728852 4.656302 4.596832 4.789755 4.571342 4.750549\r\n[162] 4.828835 4.946644 4.904696 4.951820 4.962249 4.952014 5.015733\r\n[169] 4.920095 4.695109 4.624958 4.687815 5.038474 5.314062 5.471601\r\n[176] 5.659262 5.667469 5.904322 5.968823 6.073087 5.663232 5.407968\r\n[183] 5.177870 5.237016 5.445955 5.679831 5.614257 5.233444 5.227926\r\n[190] 5.097925 5.119121 4.940067 4.803742 4.593282 4.749424 5.008870\r\n[197] 4.902099 5.014811 5.048332 5.111487 5.059727 4.972699 4.866232\r\n[204] 4.952064 4.924344 5.077133 5.166955 5.172722 5.304330 5.370433\r\n[211] 5.299762 5.238768 5.450415 5.399515 5.197358 5.101200 5.005289\r\n[218] 5.243733 5.194603 5.205039 5.192346 5.082026 5.030877 5.072784\r\n[225] 5.032299 4.637538 4.781121 4.812846 4.758887 4.541770 4.712547\r\n[232] 4.636478 4.876790 5.177345 4.831910 4.870811 5.106333 5.162062\r\n[239] 4.990127 5.058875 4.603333 4.441803 4.618171 4.585108 4.444892\r\n[246] 4.505732 4.827083 4.840013 5.098275 5.081742\r\n\r\nNotice that when we call test_pad1 we get a vector with several NA values appended to the front. And when we look at the length of each of these vectors, we can see that they’re length 250\r\n\r\n\r\nlength(test_pad1)\r\n\r\n\r\n[1] 250\r\n\r\nlength(test_pad2)\r\n\r\n\r\n[1] 250\r\n\r\nStep 4: Use Functions to Add Columns to Data\r\nNow that we have a function that reliably outputs a vector the same length as the columns in our dataframe, we can use it in conjunction with other tidyverse operations to add columns to our dataframe.\r\n\r\n\r\ndf %>%\r\n  mutate(roll_avg = ee_roll_func_padded(val, window = 8, fn = mean))\r\n\r\n\r\n# A tibble: 250 x 3\r\n     day   val roll_avg\r\n   <int> <dbl>    <dbl>\r\n 1     1  2.64    NA   \r\n 2     2  5.30    NA   \r\n 3     3  4.29    NA   \r\n 4     4  5.76    NA   \r\n 5     5  3.75    NA   \r\n 6     6  4.89    NA   \r\n 7     7  4.50    NA   \r\n 8     8  3.84     4.37\r\n 9     9  4.74     4.63\r\n10    10  6.41     4.77\r\n# ... with 240 more rows\r\n\r\nFinally, what if we wanted to get the rolling mean, standard deviation, min, and max all as new columns in our dataframe using the function we created. Our function allows us to pass in whichever aggregation function we want to use (well, probably not any function), so we can use pmap() from {purrr} to iterate over multiple functions and, in combination with the {glue} package, also set meaningful names for the new variables.\r\nI’ll set up a dataframe called params that has the names of the new variables and the corresponding functions, then I’ll loop over these names and functions to create new columns in our original dataframe. I’m not going to go over all of the code here, but if you’re curious, it might be helpful to look at the documentation for {glue}, {purrr}, and possibly {rlang} (for the := operator).\r\n\r\n\r\nlibrary(glue)\r\nparams <- tibble(\r\n  names = c(\"roll_avg\", \"roll_sd\", \"roll_min\", \"roll_max\"),\r\n  fn = lst(mean, sd, min, max)\r\n)\r\nparams %>%\r\n  pmap_dfc(~df %>%\r\n             transmute(\"{.x}\" := ee_roll_func_padded(val, window = 8, fn = .y))) %>%\r\n  bind_cols(df, .)\r\n\r\n\r\n# A tibble: 250 x 6\r\n     day   val roll_avg roll_sd roll_min roll_max\r\n   <int> <dbl>    <dbl>   <dbl>    <dbl>    <dbl>\r\n 1     1  2.64    NA     NA        NA       NA   \r\n 2     2  5.30    NA     NA        NA       NA   \r\n 3     3  4.29    NA     NA        NA       NA   \r\n 4     4  5.76    NA     NA        NA       NA   \r\n 5     5  3.75    NA     NA        NA       NA   \r\n 6     6  4.89    NA     NA        NA       NA   \r\n 7     7  4.50    NA     NA        NA       NA   \r\n 8     8  3.84     4.37   0.982     2.64     5.76\r\n 9     9  4.74     4.63   0.691     3.75     5.76\r\n10    10  6.41     4.77   0.918     3.75     6.41\r\n# ... with 240 more rows\r\n\r\nThis gives us, for each 8-day window (e.g. day 1-8, day 2-9, etc) an average, standard deviation, minimum, and maximum of the val column.\r\nWrapping Up\r\nAs sort of a final note, this activity was meant to be both an exercise for me in working through some programming using window functions as well as a walkthrough/tutorial for others interested in writing functions. That said, when I dive back into the Kaggle data I mentioned earlier, I’ll use the functions from the {RcppRoll} package rather than my own. These are optimized to run quickly because they use C++ code and they’re going to be more efficient than anything I just wrote. This doesn’t matter much when we use a little 250 observation dataframe for demonstration, but it will make a difference working with several thousand observations at once.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-04T06:37:42-05:00",
    "input_file": "writing-window-functions.utf8.md"
  },
  {
    "path": "posts/2021-01-04-rva-pets/",
    "title": "RVA Pets",
    "description": "Analyzing pet ownership in RVA",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-04-23",
    "categories": [],
    "contents": "\r\nI recently stumbled across the RVA Open Data Portal and, when browsing through the datasets available, noticed they had one on pet licenses issued by the city. Since I’m a huge dog fan & love our pitty Nala more than most people in my life, I figured I’d splash around in the data a little bit to see what I can learn about pets in RVA. You can get the data here, although note that the most recent data is from April 2019.\r\nFirst, let’s load our packages and set our plot themes/colors\r\n\r\n\r\n\r\nNext, we’ll read in the data and clean it up a little bit. In this dataset, each row represents a licensed pet in Richmond, Virginia. The dataset includes animal type (dog, cat, puppy, kitten) and the address of the owners. Whoever set up the data was also nice enough to include longitude and latitude for each address in the dataset, which means I don’t need to go out and get it. For our purposes here, I’m going to lump puppies in with dogs and kittens in with cats. I’m also going to extract the “location” column into a few separate columns. Let’s take a look at the first few entries.\r\n\r\n\r\npets_raw <- read_csv(here::here(\"data/rva_pets_2019.csv\"))\r\npets_clean <- pets_raw %>%\r\n  clean_names() %>%\r\n  extract(col = location_1,\r\n          into = c(\"address\", \"zip\", \"lat\", \"long\"),\r\n          regex = \"(.*)\\n.*(\\\\d{5})\\n\\\\((.*), (.*)\\\\)\") %>%\r\n  mutate(animal_type = str_replace_all(animal_type, c(\"Puppy\" = \"Dog\", \"Kitten\" = \"Cat\")))\r\nhead(pets_clean) %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\r\n\r\n\r\n\r\nanimal_type\r\n\r\n\r\nanimal_name\r\n\r\n\r\naddress\r\n\r\n\r\nzip\r\n\r\n\r\nlat\r\n\r\n\r\nlong\r\n\r\n\r\nload_date\r\n\r\n\r\nCat\r\n\r\n\r\nMolly\r\n\r\n\r\n301 Virginia Street APT 1008\r\n\r\n\r\n23219\r\n\r\n\r\n37.53294\r\n\r\n\r\n-77.433825\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nSam\r\n\r\n\r\n1407 Wilmington Avenue\r\n\r\n\r\n23227\r\n\r\n\r\n37.58294\r\n\r\n\r\n-77.455213\r\n\r\n\r\n20180627\r\n\r\n\r\nCat\r\n\r\n\r\nTaffy\r\n\r\n\r\n114 N Harvie Street\r\n\r\n\r\n23220\r\n\r\n\r\n37.548414\r\n\r\n\r\n-77.45745\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nJackson\r\n\r\n\r\n4804 Riverside Drive\r\n\r\n\r\n23225\r\n\r\n\r\n37.527326\r\n\r\n\r\n-77.483249\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nCirrus\r\n\r\n\r\n3107 E Marshall Street\r\n\r\n\r\n23223\r\n\r\n\r\n37.52904\r\n\r\n\r\n-77.412272\r\n\r\n\r\n20180627\r\n\r\n\r\nDog\r\n\r\n\r\nHenri\r\n\r\n\r\n1900 Maple Shade Lane\r\n\r\n\r\n23227\r\n\r\n\r\n37.581979\r\n\r\n\r\n-77.466207\r\n\r\n\r\n20180627\r\n\r\n\r\nOk, now that our data is set up, let’s see if there are more cats or dogs in the city.\r\n\r\n\r\npets_clean %>%\r\n  count(animal_type) %>%\r\n  ggplot(aes(x = n, y = animal_type)) +\r\n  geom_col(color = pal[1], fill = pal[1]) +\r\n  geom_text(aes(x = n-50, label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n  labs(\r\n    title = \"Number of Cats vs Dogs\"\r\n  )\r\n\r\n\r\n\r\n\r\nAlright, so, lots more dogs. Like almost 4 to 1 dogs to cats. Which is something I can get behind. I’m a firm believer in the fact that dogs are wayyy better than cats.\r\nI’m also interested in the most common names for pets in RVA.\r\n\r\n\r\npets_clean %>%\r\n  group_by(animal_type) %>%\r\n  count(animal_name, sort = TRUE) %>%\r\n  slice(1:15) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = n, y = reorder_within(animal_name, n, animal_type))) +\r\n    geom_col(color = pal[1], fill = pal[1]) +\r\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - .25, n - 1), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n    facet_wrap(~animal_type, scales = \"free\") +\r\n    scale_y_reordered() +\r\n    labs(\r\n      title = \"Top Pet Names\",\r\n      y = NULL\r\n    )\r\n\r\n\r\n\r\n\r\nThese seem pretty standard to me, and unfortunately, nothing is screaming “RVA” here. No “Bagels,” no “Gwars,” etc.\r\nI also pulled out zip codes into their own column earlier, so we can take a look at which zip codes have the most dogs and cats.\r\n\r\n\r\npets_clean %>%\r\n  filter(!is.na(zip)) %>%\r\n  group_by(zip) %>%\r\n  count(animal_type, sort = TRUE)%>%\r\n  ungroup() %>%\r\n  group_by(animal_type) %>%\r\n  top_n(n = 10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = n, y = reorder_within(zip, n, animal_type))) +\r\n    geom_col(color = pal[1], fill = pal[1]) +\r\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - 1, n - 4), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\r\n    facet_wrap(~animal_type, scales = \"free\") +\r\n    scale_y_reordered() +\r\n    labs(\r\n      title = \"Number of Pets by Zipcode\",\r\n      y = NULL\r\n    )\r\n\r\n\r\n\r\n\r\nAlright, so most of the pets here live in Forest Hill/generally south of the river in 23225, and another big chunk live in 23220, which covers a few neighborhoods & includes The Fan, which is probably where most of the pet action is.\r\nAnd finally, since we have the latitude and longitude, I can put together a streetmap of the city showing where all of these little critters live. To do this, I’m going to grab some shape files through the OpenStreetMaps API and plot the pet datapoints on top of those.\r\n\r\n\r\npets_map <- st_as_sf(pets_clean %>%\r\n                       filter(!is.na(long)), coords = c(\"long\", \"lat\"),\r\n                     crs = 4326)\r\nget_rva_maps <- function(key, value) {\r\n  getbb(\"Richmond Virginia United States\") %>%\r\n    opq() %>%\r\n    add_osm_feature(key = key,\r\n                    value = value) %>%\r\n    osmdata_sf()\r\n}\r\nrva_streets <- get_rva_maps(key = \"highway\", value = c(\"motorway\", \"primary\", \"secondary\", \"tertiary\"))\r\nsmall_streets <- get_rva_maps(key = \"highway\", value = c(\"residential\", \"living_street\",\r\n                                                         \"unclassified\",\r\n                                                         \"service\", \"footway\", \"cycleway\"))\r\nriver <- get_rva_maps(key = \"waterway\", value = \"river\")\r\ndf <- tibble(\r\n  type = c(\"big_streets\", \"small_streets\", \"river\"),\r\n  lines = map(\r\n    .x = lst(rva_streets, small_streets, river),\r\n    .f = ~pluck(., \"osm_lines\")\r\n  )\r\n)\r\ncoords <- pluck(rva_streets, \"bbox\")\r\nannotations <- tibble(\r\n  label = c(\"<span style='color:#FFFFFF'><span style='color:#EBCC2A'>**Cats**<\/span> and <span style='color:#3B9AB2'>**Dogs**<\/span> in RVA<\/span>\"),\r\n  x = c(-77.555),\r\n  y = c(37.605),\r\n  hjust = c(0)\r\n)\r\nrva_pets <- ggplot() +\r\n  geom_sf(data = df$lines[[1]],\r\n          inherit.aes = FALSE,\r\n          size = .3,\r\n          alpha = .8, \r\n          color = \"white\") +\r\n  geom_sf(data = pets_map, aes(color = animal_type), alpha = .6, size = .75) +\r\n  geom_richtext(data = annotations, aes(x = x, y = y, label = label, hjust = hjust), fill = NA, label.color = NA, \r\n                label.padding = grid::unit(rep(0, 4), \"pt\"), size = 11, family = \"Bahnschrift\") + \r\n  coord_sf(\r\n    xlim = c(-77.55, -77.4),\r\n    ylim = c(37.5, 37.61),\r\n    expand = TRUE\r\n  ) +\r\n  theme_void() +\r\n  scale_color_manual(\r\n    values = colors\r\n  ) +\r\n  theme(\r\n    legend.position = \"none\",\r\n    plot.background = element_rect(fill = \"grey10\"),\r\n    panel.background = element_rect(fill = \"grey10\"),\r\n    text = element_markdown(family = \"Bahnschrift\")\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-04-rva-pets/rva-pets_files/figure-html5/counts-1.png",
    "last_modified": "2021-01-04T06:32:07-05:00",
    "input_file": "rva-pets.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-04-scrantonicity-part-2/",
    "title": "Scrantonicity - Part 2",
    "description": "K means clustering using The Office dialogue",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-04-16",
    "categories": [],
    "contents": "\r\nWould I rather be feared or loved? Easy. Both.\r\nA few weeks ago, I did some exploratory analyses of dialogue from The Office. That blog could easily have been a lot longer than it was, and so instead of writing some gigantic post that would have taken 30 minutes+ to read, I decided to separate it out into several different blog posts. And so here’s volume 2.\r\nIn this post, I want to try using k-means clustering to identify patterns in who talks to whom in different episodes.\r\nOnce again, huge thanks to Brad Lindblad, the creator of the {schrute} package for R, which makes the dialogue from The Office easy to work with.\r\nSetup\r\nAs in the previous blog, I’ll be using the {schrute} package to get the transcripts from the show, and I’m going to limit the dialogue to the first 7 seasons of the show, which is when Michael Scott was around. I’ll also use a handful of other packages for data cleaning, analysis, and visualization. Let’s load all of this in and do some general setup.\r\n\r\n\r\n\r\nI’m not superstitious, but I am a little stitious.\r\nNow that we have our data read in and our packages loaded, let’s start with the cluster analysis. The goal here is going to be to figure out if there are certain “types” (clusters, groups, whatever you want to call them) of episodes. There are several frameworks we could use to go about doing this. One approach would be a mixture modeling approach (e.g. latent profile analysis, latent class analysis). I’m not doing that here because I want each episode to be an observation when we cluster, and I’m not sure we have enough episodes here to get good model fits using this approach. Instead, I’m going to use k-means clustering, which basically places observations (episodes, in this case) into one of k groups (where k is supplied by the user) by trying to minimize the “distance” between each observation and the center of the group. The algorithm iteratively assigns observations to groups, updates the center of each group, reassigns observations to groups, etc. until it reaches a stable solution.\r\nWe can also include all sorts of different variables in the k-means algorithm to serve as indicators. For this analysis, I’m going to use the number of exchanges between different characters per episode – i.e. the number of exchanges between Michael and Jim, between Jim and Dwight, etc. – to estimate groups. This could tell us, for instance, that one “type” of Office episode features lots of exchanges between Michael and Dwight, lots between Pam and Jim, and few between Pam and Michael. One consideration when we use the k-means algorithm is that, because we’re looking at distance between observations, we typically want our observations to be on the same scale. Fortunately, since all of our indicators will be “number of lines per episode,” they’re already on the same scale, so we don’t need to worry about standardizing.\r\nLet’s go ahead and set up our data. I’m also going to decide to only use the 5 characters who speak the most during the first 7 seasons in this analysis, otherwise the number of combinations of possible exchanges would be huge. These five characters are:\r\n\r\n\r\ntop5_chars <- office %>%\r\n  count(character, sort = TRUE) %>%\r\n  top_n(5) %>%\r\n  pull(character)\r\ntop5_chars\r\n\r\n\r\n[1] \"Michael\" \"Dwight\"  \"Jim\"     \"Pam\"     \"Andy\"   \r\n\r\nOk, so our top 5 characters here are Michael, Dwight, Jim, Pam, and Andy. Since Andy doesn’t join the show until season 3, I’m actually going to narrow our window of usable episodes to those in seasons 3-7. Otherwise, the clustering algorithm would likely group episodes with a focus on those in seasons 1 and 2, where Andy will obviously have 0 lines, vs episodes in later seasons.\r\nAdditionally, we want to code our changes so that Michael & Jim is the same as Jim & Michael.\r\n\r\n\r\ncombos <- t(combn(top5_chars, 2)) %>%\r\n  as_tibble() %>%\r\n  mutate(comb = glue::glue(\"{V1}&{V2}\"),\r\n         comb_inv = glue::glue(\"{V2}&{V1}\"))\r\nreplace_comb <- combos$comb\r\nnames(replace_comb) <- combos$comb_inv\r\noffice_exchanges <- office %>%\r\n  filter(as.numeric(season) >= 3) %>%\r\n  mutate(char2 = lead(character)) %>% #this will tell us who the speaker is talking to\r\n  filter(character %in% top5_chars &\r\n         char2 %in% top5_chars &\r\n         character != char2) %>% #this filters down to just exchanges between our top 5 characters\r\n  mutate(exchange = glue::glue(\"{character}&{char2}\") %>%\r\n           str_replace_all(replace_comb)) %>% #these lines ensure that, e.g. Michael & Jim is coded the same as Jim & Michael\r\n  select(season, episode_name, character, char2, exchange) %>%\r\n  count(season, episode_name, exchange) %>%\r\n  pivot_wider(names_from = exchange,\r\n              values_from = n,\r\n              values_fill = list(n = 0))\r\nhead(office_exchanges)\r\n\r\n\r\n# A tibble: 6 x 12\r\n  season episode_name `Dwight&Andy` `Dwight&Jim` `Dwight&Pam`\r\n   <int> <chr>                <int>        <int>        <int>\r\n1      3 A Benihana ~             6           10           17\r\n2      3 Back from V~             1           16            6\r\n3      3 Beach Games              8            8            3\r\n4      3 Ben Franklin             0           14            2\r\n5      3 Branch Clos~             0            5            1\r\n6      3 Business Sc~             0           10            3\r\n# ... with 7 more variables: `Jim&Andy` <int>, `Jim&Pam` <int>,\r\n#   `Michael&Andy` <int>, `Michael&Dwight` <int>,\r\n#   `Michael&Jim` <int>, `Michael&Pam` <int>, `Pam&Andy` <int>\r\n\r\nGreat – now our data is all set up so that we know the number of lines exchanged between main characters in each episode. We can run some clustering algorithms now to see if there are patterns in these exchanges. To do this, we’ll fit models testing out 1-10 clusters. We’ll then look at the error for each of these models graphically and use this to choose how many clusters we want to include in our final model.\r\n\r\n\r\nclusters_fit <- tibble(\r\n  k = c(1:10),\r\n  km_fit = map(c(1:10), ~kmeans(office_exchanges %>% select(-c(1:2)), centers = .))\r\n) %>%\r\n  mutate(within_ss = map_dbl(km_fit, ~pluck(., 5)))\r\nclusters_fit %>%\r\n  ggplot(aes(x = k, y = within_ss)) +\r\n  geom_point() +\r\n  geom_line() +\r\n  labs(\r\n    title = \"Within Cluster Sum of Squares vs K\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see that error decreases as we add more clusters, and error will always decrease as k increases. But we can also see that the rate of decrease slows down a bit as we increase our number of clusters. Ideally, there would be a definitive bend, or “elbow” in this plot where the rate of decrease levels off (which is also the number of clusters we’d choose), but that’s not quite the case here. It seems like there’s some slight elbow-ing at 5 clusters, so let’s just go ahead and choose that. Now we can look at the patterns of exchanges in each of these clusters.\r\n\r\n\r\noffice_clustered <- augment(clusters_fit$km_fit[[5]], data = office_exchanges)\r\nclusters_long <- office_clustered %>%\r\n  mutate(season = as_factor(season)) %>%\r\n  group_by(.cluster) %>%\r\n  summarize_if(is.numeric, mean, na.rm = TRUE) %>%\r\n  ungroup() %>%\r\n  pivot_longer(cols = -c(\".cluster\"),\r\n               names_to = \"chars\",\r\n               values_to = \"lines\")\r\nclusters_long %>%\r\n  ggplot(aes(x = lines, y = chars, fill = .cluster)) +\r\n    geom_col() +\r\n    facet_wrap(~.cluster, ncol = 2, scales = \"free_y\") +\r\n    #scale_y_reordered() +\r\n    scale_fill_ipsum() +\r\n    theme_minimal() +\r\n    labs(\r\n      title = \"Types of Office Episodes\"\r\n    ) +\r\n    theme(\r\n      legend.position = \"none\"\r\n    )\r\n\r\n\r\n\r\n\r\nSo, these plots show us the average number of exchanges between characters by cluster. Cluster 1 episodes seem to center around exchanges between Michael and Pam, and we also see a fair amount of exchanges between Michael & Jim, Michael & Dwight, and Jim & Pam. Cluster 2 episodes overwhelmingly feature interactions between Michael and Dwight. Cluster 3 episodes have relatively few exchanges between all of our main characters – this probably means that there’s a lot of side character action going on (recall that we didn’t include exchanges between anyone other than Michael, Dwight, Jim, Pam, and Andy in our clustering algorithm). Cluster 4 episodes have a lot of Michael and Andy interactions, along with a fair number of Michael-Dwight and Jim-Pam interactions. And Cluster 5 seems to be predominantly Michael and Jim, but also a fair amount of Michael-Dwight and Dwight-Jim, which makes sense. Usually when Jim talks to Michael in the show, Dwight finds a way to intrude.\r\nOne thing to remember is that these clusters aren’t necessarily balanced. As the table below shows, most episodes fit into Cluster 3.\r\n\r\n\r\noffice_clustered %>%\r\n  count(.cluster, name = \"num_episodes\") %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\", \"hover\"))\r\n\r\n\r\n\r\n.cluster\r\n\r\n\r\nnum_episodes\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n2\r\n\r\n\r\n10\r\n\r\n\r\n3\r\n\r\n\r\n60\r\n\r\n\r\n4\r\n\r\n\r\n8\r\n\r\n\r\n5\r\n\r\n\r\n17\r\n\r\n\r\nAnother thing to keep in mind is that, across the all of the characters, Michael has far and away the most lines, so his interactions tend to drive this clustering. If we centered and scaled our variables, this would likely change, but we’d also lose some of the interpretability that comes with working in the raw metrics.\r\nFinally, let’s just choose a random episode from each cluster to see which episodes are falling into which categories.\r\n\r\n\r\noffice_clustered %>%\r\n  group_by(.cluster) %>%\r\n  sample_n(size = 1) %>%\r\n  select(.cluster, season, episode_name) %>%\r\n  kable(format = \"html\") %>%\r\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"striped\"))\r\n\r\n\r\n\r\n.cluster\r\n\r\n\r\nseason\r\n\r\n\r\nepisode_name\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\nWomen’s Appreciation\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\nThe Coup\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\nDiwali\r\n\r\n\r\n4\r\n\r\n\r\n7\r\n\r\n\r\nAndy’s Play\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\nThe Merger\r\n\r\n\r\nThat’s all for now. I might do one more with some predictive modeling in the future.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-04-scrantonicity-part-2/scrantonicity-part-2_files/figure-html5/cluster-1.png",
    "last_modified": "2021-01-04T06:17:46-05:00",
    "input_file": "scrantonicity-part-2.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-04-riddler-express-march-20-2020/",
    "title": "Riddler Express - March 20, 2020",
    "description": "Solving a math puzzle and exploring the accumulate() function.",
    "author": [
      {
        "name": "EE",
        "url": "https://www.ericekholm.com/"
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\r\nOne of my personal goals for 2020 is to improve my proficiency doing data-y things – mostly using R, but potentially other software as well. Typically, I’ve been using data from the #TidyTuesday project to practice data visualization and data from Kaggle, personal research projects, and other potentially interesting datasets to work on statistical modeling. I recently discovered The Riddler series – a weekly math/logic puzzle – that seems to be a good medium for brushing up on other skills (e.g. certain types of math and programming) that may not come up as often when I do visualizations or statistics.\r\nThe Problem\r\nAnyway, this post solves the Riddler Express puzzle from March 20, 2020. The problem is this:\r\n\r\nA manager is trying to produce sales of his company’s widget, so he instructs his team to hold a sale every morning, lowering the price of the widget by 10 percent. However, he gives very specific instructions as to what should happen in the afternoon: Increase the price by 10 percent from the sale price, with the (incorrect) idea that it would return it to the original price. The team follows his instructions quite literally, lowering and then raising the price by 10 percent every day.\r\n\r\n\r\nAfter N days, the manager walks through the store in the evening, horrified to see that the widgets are marked more than 50 percent off of their original price. What is the smallest possible value of N?\r\n\r\nI’ll walk through a couple of ways to solve this – first, I’ll solve it algebraically, and next, I’ll solve it by “brute force” using the accumulate() function from the {purrr} package.\r\nSolving Algebraically\r\nSo, the first thing that strikes me when reading this is that it’s essentially a compounding interest problem, except in this case the interest is negative. That is, rather than gaining value exponentially over the number of compounding periods, we’re losing value exponentially. The formula for calculating compound interest is:\r\n\\[A = P(1 + r)^n\\]\r\nwhere A equals the final amount, P equals the principal (our initial value), r equals the interest rate, and n equals the number of compounding periods (the number of days in this case). We’re interested in solving for the value of n where our final amount, A, is less than .5. Our principal amount, P, in this case, is 1 (i.e. 100% of the value). So, our equation looks like this:\r\n\\[.5 > ((1-1*.1)*1.1)^n\\]\r\nThe internal logic here is that we subtract 10% from our initial value (1-1*.1) to represent the 10% decrease in price in the morning, then multiply this resulting value by 1.1 to represent the subsequent 10& increase in price in the afternoon. This simplifies to:\r\n\\[.5 > .99^n\\]\r\nFrom here, we can just solve by taking the log of each side and then dividing, which get us our answer\r\n\r\n[1] 68.96756\r\n\r\nRounding this up (since we’re dealing in full days), we can say that after 69 days, the price of the widget will be below 50% of its initial price.\r\nSolving using accumulate()\r\nWe can also solve this problem using the accumulate() function from the {purrr} package, which is part of the {tidyverse}. Essentially, accumulate() will take a function, evaluate it, and then pass the result of the evaluation back into the function, evaluate it again, pass the new result back into the function, etc. This makes it useful for solving problems like this one, where the end price of the widget on the previous day is the starting price of the widget on the current day.\r\nFirst, let’s load our packages. For this, we’ll just use {tidyverse}\r\n\r\n\r\n\r\nNext, let’s set up a function that, if we give it the price of the widget at the beginning of the day, will calculate the price of the widget at the end of the day.\r\n\r\n\r\ndiscount_func <- function(x) {\r\n  (x-x*.1)*1.1\r\n}\r\n\r\n\r\n\r\nAnd then let’s test this function manually a few times.\r\n\r\n\r\ndiscount_func(1)\r\n\r\n\r\n[1] 0.99\r\n\r\ndiscount_func(.99)\r\n\r\n\r\n[1] 0.9801\r\n\r\ndiscount_func(.9801)\r\n\r\n\r\n[1] 0.970299\r\n\r\nNow, we can use accumulate() to automate what we just did manually. The first argument in accumulate() is, in this case, each day that we want to pass into the function. In the code below, I’m testing this for days 0-3 (but coded as 1-4 because we want the start value to be 1). The second argument is the function we just wrote.\r\n\r\n\r\naccumulate(1:4, ~discount_func(.))\r\n\r\n\r\n[1] 1.000000 0.990000 0.980100 0.970299\r\n\r\nAnd we can see that the values returned match our manual tests above, which is good!\r\nNow, we can use accumulate() to make a table with the end price of the widget each day. Note that because we want to start the widget price at 1, our first “day” in the table is day 0, which represents the beginning price of the widget on day 1.\r\n\r\n\r\ndays_tbl <- tibble(\r\n  day = c(0:1000),\r\n  end_price = accumulate(c(1:1001), ~discount_func(.))\r\n)\r\nhead(days_tbl)\r\n\r\n\r\n# A tibble: 6 x 2\r\n    day end_price\r\n  <int>     <dbl>\r\n1     0     1    \r\n2     1     0.99 \r\n3     2     0.980\r\n4     3     0.970\r\n5     4     0.961\r\n6     5     0.951\r\n\r\nAnd then we can plot the end price over time. I’ve added a little bit of transparency to each point so we can more easily see the clustering/overlap.\r\n\r\n\r\nggplot(days_tbl, aes(x = day, y = end_price)) +\r\n  geom_point(alpha = .3) +\r\n  theme_minimal() +\r\n  labs(\r\n    title = \"End Price of Widget over Time\"\r\n  )\r\n\r\n\r\n\r\n\r\nFinally, we can find the day where the end price is below .5 by filtering our table to only those where the price is less than .5 and then returning the first row.\r\n\r\n\r\ndays_tbl %>%\r\n  filter(end_price <= .5) %>%\r\n  slice(1)\r\n\r\n\r\n# A tibble: 1 x 2\r\n    day end_price\r\n  <int>     <dbl>\r\n1    69     0.500\r\n\r\nAnd we can see that this matches our algebraic result – great success!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-04-riddler-express-march-20-2020/riddler-express-march-20-2020_files/figure-html5/plot results-1.png",
    "last_modified": "2021-01-04T06:20:36-05:00",
    "input_file": "riddler-express-march-20-2020.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
