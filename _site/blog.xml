<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Eric Ekholm</title>
    <link>https://www.ericekholm.com/</link>
    <atom:link href="https://www.ericekholm.com/blog.xml" rel="self" type="application/rss+xml"/>
    <description>Using R for statistical modeling, data visualization, and other personal projects.
</description>
    <generator>Distill</generator>
    <lastBuildDate>Wed, 03 Feb 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Dungeons and Dragons - Part 3</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-02-03-dungeons-and-dragons-part-3</link>
      <description>Grouping D&amp;D monsters using latent profile analysis.</description>
      <guid>https://www.ericekholm.com/posts/2021-02-03-dungeons-and-dragons-part-3</guid>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-02-03-dungeons-and-dragons-part-3/dungeons-and-dragons-part-3_files/figure-html5/cr_distrib-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Dungeons and Dragons - Part 2</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-dungeons-and-dragons-part-2</link>
      <description>Exploring monster stats in D&amp;D 5e</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-dungeons-and-dragons-part-2</guid>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-11-dungeons-and-dragons-part-2/dungeons-and-dragons-part-2_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Dungeons and Dragons - Part 1</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-dungeons-and-dragons-part-1</link>
      <description>Wrangling JSON data from an API.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-dungeons-and-dragons-part-1</guid>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Predicting Mobile Phone Subscription Growth</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-predicting-mobile-phone-subscription-growth</link>
      <description>Using splines and iteration via map() to fit and interrogate models.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-predicting-mobile-phone-subscription-growth</guid>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-11-predicting-mobile-phone-subscription-growth/predicting-mobile-phone-subscription-growth_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Rstudio Table Contest Submission</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-rstudio-table-contest-submission</link>
      <description>And impressions of the {gt} package.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-rstudio-table-contest-submission</guid>
      <pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Simulating Triangles?</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-simulating-triangles</link>
      <description>Determining whether random numbers can form a triangle using simulation.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-simulating-triangles</guid>
      <pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-11-simulating-triangles/simulating-triangles_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Your Mom's House Analysis</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-your-moms-house-analysis</link>
      <description>An exploratory analysis of transcripts from the YMH podcast.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-your-moms-house-analysis</guid>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-11-your-moms-house-analysis/your-moms-house-analysis_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Chopped Episode Simulator</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-chopped-episode-simulator</link>
      <description>Simple app to simulate an episode of Chopped."</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-chopped-episode-simulator</guid>
      <pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Scrantonicity - Part 3</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-scrantonicity-part-3</link>
      <description>Predicting the speaker of dialogue from The Office.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-scrantonicity-part-3</guid>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-11-scrantonicity-part-3/scrantonicity-part-3_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Intro to {rvest}</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-11-intro-to-rvest</link>
      <description>Using {rvest} to find child care in Virginia.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-11-intro-to-rvest</guid>
      <pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Unconsciousness in the Xmen</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-06-unconsciousness-in-the-xmen</link>
      <description>Practicing poisson regression using Xmen data.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-06-unconsciousness-in-the-xmen</guid>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-06-unconsciousness-in-the-xmen/unconsciousness-in-the-xmen_files/figure-html5/unnamed-chunk-4-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Exploring Emily Oster's COVID Data</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-06-exploring-emily-osters-covid-data</link>
      <description>Examining child care survey data from Emily Oster</description>
      <guid>https://www.ericekholm.com/posts/2021-01-06-exploring-emily-osters-covid-data</guid>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-06-exploring-emily-osters-covid-data/exploring-emily-osters-covid-data_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Pulling YouTube Transcripts</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-06-pulling-youtube-transcripts</link>
      <description>Example of pulling transcripts for an entire YouTube playlist.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-06-pulling-youtube-transcripts</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Writing Window Functions</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-04-writing-window-functions</link>
      <description>Examples and tutorial of writing rolling aggregate functions.</description>
      <guid>https://www.ericekholm.com/posts/2021-01-04-writing-window-functions</guid>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>RVA Pets</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-04-rva-pets</link>
      <description>


&lt;p&gt;I recently stumbled across the &lt;a href="https://data.richmondgov.com/"&gt;RVA Open Data Portal&lt;/a&gt; and, when browsing through the datasets available, noticed they had one on pet licenses issued by the city. Since I’m a huge dog fan &amp;amp; love our pitty Nala more than most people in my life, I figured I’d splash around in the data a little bit to see what I can learn about pets in RVA. You can get the data &lt;a href="https://data.richmondgov.com/Unique-and-Inclusive-Neighborhoods/Pet-Licenses/fgv9-bhf4"&gt;here&lt;/a&gt;, although note that the most recent data is from April 2019.&lt;/p&gt;
&lt;p&gt;First, let’s load our packages and set our plot themes/colors&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(osmdata)
library(sf)
library(extrafont)
library(janitor)
library(hrbrthemes)
library(wesanderson)
library(tidytext)
library(kableExtra)
library(ggtext)
theme_set(theme_ipsum())
pal &amp;lt;- wes_palette(&amp;quot;Zissou1&amp;quot;)
colors &amp;lt;- c(&amp;quot;Dog&amp;quot; = pal[1], &amp;quot;Cat&amp;quot; = pal[3])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll read in the data and clean it up a little bit. In this dataset, each row represents a licensed pet in Richmond, Virginia. The dataset includes animal type (dog, cat, puppy, kitten) and the address of the owners. Whoever set up the data was also nice enough to include longitude and latitude for each address in the dataset, which means I don’t need to go out and get it. For our purposes here, I’m going to lump puppies in with dogs and kittens in with cats. I’m also going to extract the “location” column into a few separate columns. Let’s take a look at the first few entries.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pets_raw &amp;lt;- read_csv(&amp;quot;~/Data/ee_website/data/rva_pets_2019.csv&amp;quot;)
pets_clean &amp;lt;- pets_raw %&amp;gt;%
  clean_names() %&amp;gt;%
  extract(col = location_1,
          into = c(&amp;quot;address&amp;quot;, &amp;quot;zip&amp;quot;, &amp;quot;lat&amp;quot;, &amp;quot;long&amp;quot;),
          regex = &amp;quot;(.*)\n.*(\\d{5})\n\\((.*), (.*)\\)&amp;quot;) %&amp;gt;%
  mutate(animal_type = str_replace_all(animal_type, c(&amp;quot;Puppy&amp;quot; = &amp;quot;Dog&amp;quot;, &amp;quot;Kitten&amp;quot; = &amp;quot;Cat&amp;quot;)))
head(pets_clean) %&amp;gt;%
  kable(format = &amp;quot;html&amp;quot;) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed" style="margin-left: auto; margin-right: auto;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
animal_type
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
animal_name
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
address
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
zip
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
lat
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
long
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
load_date
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Cat
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Molly
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
301 Virginia Street APT 1008
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
23219
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
37.53294
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
-77.433825
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
20180627
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Dog
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Sam
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
1407 Wilmington Avenue
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
23227
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
37.58294
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
-77.455213
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
20180627
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Cat
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Taffy
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
114 N Harvie Street
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
23220
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
37.548414
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
-77.45745
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
20180627
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Dog
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Jackson
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
4804 Riverside Drive
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
23225
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
37.527326
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
-77.483249
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
20180627
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Dog
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Cirrus
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
3107 E Marshall Street
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
23223
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
37.52904
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
-77.412272
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
20180627
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Dog
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Henri
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
1900 Maple Shade Lane
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
23227
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
37.581979
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
-77.466207
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
20180627
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ok, now that our data is set up, let’s see if there are more cats or dogs in the city.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pets_clean %&amp;gt;%
  count(animal_type) %&amp;gt;%
  ggplot(aes(x = n, y = animal_type)) +
  geom_col(color = pal[1], fill = pal[1]) +
  geom_text(aes(x = n-50, label = n), hjust = 1, color = &amp;quot;white&amp;quot;, fontface = &amp;quot;bold&amp;quot;) +
  labs(
    title = &amp;quot;Number of Cats vs Dogs&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file22782b7c45dc_files/figure-html/counts-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Alright, so, lots more dogs. Like almost 4 to 1 dogs to cats. Which is something I can get behind. I’m a firm believer in the fact that dogs are wayyy better than cats.&lt;/p&gt;
&lt;p&gt;I’m also interested in the most common names for pets in RVA.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pets_clean %&amp;gt;%
  group_by(animal_type) %&amp;gt;%
  count(animal_name, sort = TRUE) %&amp;gt;%
  slice(1:15) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(x = n, y = reorder_within(animal_name, n, animal_type))) +
    geom_col(color = pal[1], fill = pal[1]) +
    geom_text(aes(x = if_else(animal_type == &amp;quot;Cat&amp;quot;, n - .25, n - 1), label = n), hjust = 1, color = &amp;quot;white&amp;quot;, fontface = &amp;quot;bold&amp;quot;) +
    facet_wrap(~animal_type, scales = &amp;quot;free&amp;quot;) +
    scale_y_reordered() +
    labs(
      title = &amp;quot;Top Pet Names&amp;quot;,
      y = NULL
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file22782b7c45dc_files/figure-html/names-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;These seem pretty standard to me, and unfortunately, nothing is screaming “RVA” here. No “Bagels,” no “Gwars,” etc.&lt;/p&gt;
&lt;p&gt;I also pulled out zip codes into their own column earlier, so we can take a look at which zip codes have the most dogs and cats.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pets_clean %&amp;gt;%
  filter(!is.na(zip)) %&amp;gt;%
  group_by(zip) %&amp;gt;%
  count(animal_type, sort = TRUE)%&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(animal_type) %&amp;gt;%
  top_n(n = 10) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(x = n, y = reorder_within(zip, n, animal_type))) +
    geom_col(color = pal[1], fill = pal[1]) +
    geom_text(aes(x = if_else(animal_type == &amp;quot;Cat&amp;quot;, n - 1, n - 4), label = n), hjust = 1, color = &amp;quot;white&amp;quot;, fontface = &amp;quot;bold&amp;quot;) +
    facet_wrap(~animal_type, scales = &amp;quot;free&amp;quot;) +
    scale_y_reordered() +
    labs(
      title = &amp;quot;Number of Pets by Zipcode&amp;quot;,
      y = NULL
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file22782b7c45dc_files/figure-html/by%20zip-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Alright, so most of the pets here live in Forest Hill/generally south of the river in 23225, and another big chunk live in 23220, which covers a few neighborhoods &amp;amp; includes The Fan, which is probably where most of the pet action is.&lt;/p&gt;
&lt;p&gt;And finally, since we have the latitude and longitude, I can put together a streetmap of the city showing where all of these little critters live. To do this, I’m going to grab some shape files through the OpenStreetMaps API and plot the pet datapoints on top of those.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pets_map &amp;lt;- st_as_sf(pets_clean %&amp;gt;%
                       filter(!is.na(long)), coords = c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;),
                     crs = 4326)
get_rva_maps &amp;lt;- function(key, value) {
  getbb(&amp;quot;Richmond Virginia United States&amp;quot;) %&amp;gt;%
    opq(timeout = 80) %&amp;gt;%
    add_osm_feature(key = key,
                    value = value) %&amp;gt;%
    osmdata_sf()
}
rva_streets &amp;lt;- get_rva_maps(key = &amp;quot;highway&amp;quot;, value = c(&amp;quot;motorway&amp;quot;, &amp;quot;primary&amp;quot;, &amp;quot;secondary&amp;quot;, &amp;quot;tertiary&amp;quot;))
small_streets &amp;lt;- get_rva_maps(key = &amp;quot;highway&amp;quot;, value = c(&amp;quot;residential&amp;quot;, &amp;quot;living_street&amp;quot;,
                                                         &amp;quot;unclassified&amp;quot;,
                                                         &amp;quot;service&amp;quot;, &amp;quot;footway&amp;quot;, &amp;quot;cycleway&amp;quot;))
river &amp;lt;- get_rva_maps(key = &amp;quot;waterway&amp;quot;, value = &amp;quot;river&amp;quot;)
df &amp;lt;- tibble(
  type = c(&amp;quot;big_streets&amp;quot;, &amp;quot;small_streets&amp;quot;, &amp;quot;river&amp;quot;),
  lines = map(
    .x = lst(rva_streets, small_streets, river),
    .f = ~pluck(., &amp;quot;osm_lines&amp;quot;)
  )
)
coords &amp;lt;- pluck(rva_streets, &amp;quot;bbox&amp;quot;)
annotations &amp;lt;- tibble(
  label = c(&amp;quot;&amp;lt;span style=&amp;#39;color:#FFFFFF&amp;#39;&amp;gt;&amp;lt;span style=&amp;#39;color:#EBCC2A&amp;#39;&amp;gt;**Cats**&amp;lt;/span&amp;gt; and &amp;lt;span style=&amp;#39;color:#3B9AB2&amp;#39;&amp;gt;**Dogs**&amp;lt;/span&amp;gt; in RVA&amp;lt;/span&amp;gt;&amp;quot;),
  x = c(-77.555),
  y = c(37.605),
  hjust = c(0)
)
rva_pets &amp;lt;- ggplot() +
  geom_sf(data = df$lines[[1]],
          inherit.aes = FALSE,
          size = .3,
          alpha = .8, 
          color = &amp;quot;white&amp;quot;) +
  geom_sf(data = pets_map, aes(color = animal_type), alpha = .6, size = .75) +
  geom_richtext(data = annotations, aes(x = x, y = y, label = label, hjust = hjust), fill = NA, label.color = NA, 
                label.padding = grid::unit(rep(0, 4), &amp;quot;pt&amp;quot;), size = 11, family = &amp;quot;Bahnschrift&amp;quot;) + 
  coord_sf(
    xlim = c(-77.55, -77.4),
    ylim = c(37.5, 37.61),
    expand = TRUE
  ) +
  theme_void() +
  scale_color_manual(
    values = colors
  ) +
  theme(
    legend.position = &amp;quot;none&amp;quot;,
    plot.background = element_rect(fill = &amp;quot;grey10&amp;quot;),
    panel.background = element_rect(fill = &amp;quot;grey10&amp;quot;),
    text = element_markdown(family = &amp;quot;Bahnschrift&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://github.com/ekholme/viz_portfolio/blob/master/RVA-VA%20Data/rva_pets_map.png?raw=true"&gt;&lt;img src="https://github.com/ekholme/viz_portfolio/blob/master/RVA-VA%20Data/rva_pets_map.png?raw=true" style="width:100.0%" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>cb02dd763bf56647301f94753b4f2807</distill:md5>
      <guid>https://www.ericekholm.com/posts/2021-01-04-rva-pets</guid>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-04-rva-pets/rva-pets_files/figure-html5/counts-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Scrantonicity - Part 2</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-04-scrantonicity-part-2</link>
      <description>


&lt;h2 id="would-i-rather-be-feared-or-loved-easy.-both."&gt;Would I rather be feared or loved? Easy. Both.&lt;/h2&gt;
&lt;p&gt;A few weeks ago, I did some &lt;a href="https://ericekholm.com/blog/office-part1/"&gt;exploratory analyses&lt;/a&gt; of dialogue from The Office. That blog could easily have been a lot longer than it was, and so instead of writing some gigantic post that would have taken 30 minutes+ to read, I decided to separate it out into several different blog posts. And so here’s volume 2.&lt;/p&gt;
&lt;p&gt;In this post, I want to try using k-means clustering to identify patterns in who talks to whom in different episodes.&lt;/p&gt;
&lt;p&gt;Once again, huge thanks to Brad Lindblad, the creator of the &lt;code&gt;{schrute}&lt;/code&gt; package for R, which makes the dialogue from The Office easy to work with.&lt;/p&gt;
&lt;h3 id="setup"&gt;Setup&lt;/h3&gt;
&lt;p&gt;As in the previous blog, I’ll be using the &lt;code&gt;{schrute}&lt;/code&gt; package to get the transcripts from the show, and I’m going to limit the dialogue to the first 7 seasons of the show, which is when Michael Scott was around. I’ll also use a handful of other packages for data cleaning, analysis, and visualization. Let’s load all of this in and do some general setup.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE)
set.seed(0408)
library(schrute) #office dialogue
library(tidyverse) #data wrangling tools
library(broom) #tidying models
library(tidytext) #tools for working with text data
library(knitr) #markdown functionality
library(kableExtra) #styling for tables
library(hrbrthemes) #ggplot themes
theme_set(theme_ipsum())
office &amp;lt;- theoffice %&amp;gt;%
  filter(as.numeric(season) &amp;lt;= 7)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="im-not-superstitious-but-i-am-a-little-stitious."&gt;I’m not superstitious, but I am a little stitious.&lt;/h2&gt;
&lt;p&gt;Now that we have our data read in and our packages loaded, let’s start with the cluster analysis. The goal here is going to be to figure out if there are certain “types” (clusters, groups, whatever you want to call them) of episodes. There are several frameworks we could use to go about doing this. One approach would be a mixture modeling approach (e.g. latent profile analysis, latent class analysis). I’m not doing that here because I want each episode to be an observation when we cluster, and I’m not sure we have enough episodes here to get good model fits using this approach. Instead, I’m going to use k-means clustering, which basically places observations (episodes, in this case) into one of &lt;em&gt;k&lt;/em&gt; groups (where &lt;em&gt;k&lt;/em&gt; is supplied by the user) by trying to minimize the “distance” between each observation and the center of the group. The algorithm iteratively assigns observations to groups, updates the center of each group, reassigns observations to groups, etc. until it reaches a stable solution.&lt;/p&gt;
&lt;p&gt;We can also include all sorts of different variables in the k-means algorithm to serve as indicators. For this analysis, I’m going to use the number of exchanges between different characters per episode – i.e. the number of exchanges between Michael and Jim, between Jim and Dwight, etc. – to estimate groups. This could tell us, for instance, that one “type” of Office episode features lots of exchanges between Michael and Dwight, lots between Pam and Jim, and few between Pam and Michael. One consideration when we use the k-means algorithm is that, because we’re looking at distance between observations, we typically want our observations to be on the same scale. Fortunately, since all of our indicators will be “number of lines per episode,” they’re already on the same scale, so we don’t need to worry about standardizing.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and set up our data. I’m also going to decide to only use the 5 characters who speak the most during the first 7 seasons in this analysis, otherwise the number of combinations of possible exchanges would be huge. These five characters are:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;top5_chars &amp;lt;- office %&amp;gt;%
  count(character, sort = TRUE) %&amp;gt;%
  top_n(5) %&amp;gt;%
  pull(character)
top5_chars&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;Michael&amp;quot; &amp;quot;Dwight&amp;quot;  &amp;quot;Jim&amp;quot;     &amp;quot;Pam&amp;quot;     &amp;quot;Andy&amp;quot;   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so our top 5 characters here are Michael, Dwight, Jim, Pam, and Andy. Since Andy doesn’t join the show until season 3, I’m actually going to narrow our window of usable episodes to those in seasons 3-7. Otherwise, the clustering algorithm would likely group episodes with a focus on those in seasons 1 and 2, where Andy will obviously have 0 lines, vs episodes in later seasons.&lt;/p&gt;
&lt;p&gt;Additionally, we want to code our changes so that Michael &amp;amp; Jim is the same as Jim &amp;amp; Michael.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;combos &amp;lt;- t(combn(top5_chars, 2)) %&amp;gt;%
  as_tibble() %&amp;gt;%
  mutate(comb = glue::glue(&amp;quot;{V1}&amp;amp;{V2}&amp;quot;),
         comb_inv = glue::glue(&amp;quot;{V2}&amp;amp;{V1}&amp;quot;))
replace_comb &amp;lt;- combos$comb
names(replace_comb) &amp;lt;- combos$comb_inv
office_exchanges &amp;lt;- office %&amp;gt;%
  filter(as.numeric(season) &amp;gt;= 3) %&amp;gt;%
  mutate(char2 = lead(character)) %&amp;gt;% #this will tell us who the speaker is talking to
  filter(character %in% top5_chars &amp;amp;
         char2 %in% top5_chars &amp;amp;
         character != char2) %&amp;gt;% #this filters down to just exchanges between our top 5 characters
  mutate(exchange = glue::glue(&amp;quot;{character}&amp;amp;{char2}&amp;quot;) %&amp;gt;%
           str_replace_all(replace_comb)) %&amp;gt;% #these lines ensure that, e.g. Michael &amp;amp; Jim is coded the same as Jim &amp;amp; Michael
  select(season, episode_name, character, char2, exchange) %&amp;gt;%
  count(season, episode_name, exchange) %&amp;gt;%
  pivot_wider(names_from = exchange,
              values_from = n,
              values_fill = list(n = 0))
head(office_exchanges)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 12
  season episode_name `Dwight&amp;amp;Andy` `Dwight&amp;amp;Jim` `Dwight&amp;amp;Pam`
   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;
1      3 A Benihana ~             6           10           17
2      3 Back from V~             1           16            6
3      3 Beach Games              8            8            3
4      3 Ben Franklin             0           14            2
5      3 Branch Clos~             0            5            1
6      3 Business Sc~             0           10            3
# ... with 7 more variables: `Jim&amp;amp;Andy` &amp;lt;int&amp;gt;, `Jim&amp;amp;Pam` &amp;lt;int&amp;gt;,
#   `Michael&amp;amp;Andy` &amp;lt;int&amp;gt;, `Michael&amp;amp;Dwight` &amp;lt;int&amp;gt;,
#   `Michael&amp;amp;Jim` &amp;lt;int&amp;gt;, `Michael&amp;amp;Pam` &amp;lt;int&amp;gt;, `Pam&amp;amp;Andy` &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great – now our data is all set up so that we know the number of lines exchanged between main characters in each episode. We can run some clustering algorithms now to see if there are patterns in these exchanges. To do this, we’ll fit models testing out 1-10 clusters. We’ll then look at the error for each of these models graphically and use this to choose how many clusters we want to include in our final model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clusters_fit &amp;lt;- tibble(
  k = c(1:10),
  km_fit = map(c(1:10), ~kmeans(office_exchanges %&amp;gt;% select(-c(1:2)), centers = .))
) %&amp;gt;%
  mutate(within_ss = map_dbl(km_fit, ~pluck(., 5)))
clusters_fit %&amp;gt;%
  ggplot(aes(x = k, y = within_ss)) +
  geom_point() +
  geom_line() +
  labs(
    title = &amp;quot;Within Cluster Sum of Squares vs K&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file22782ecc6e88_files/figure-html/cluster-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that error decreases as we add more clusters, and error will &lt;em&gt;always&lt;/em&gt; decrease as k increases. But we can also see that the rate of decrease slows down a bit as we increase our number of clusters. Ideally, there would be a definitive bend, or “elbow” in this plot where the rate of decrease levels off (which is also the number of clusters we’d choose), but that’s not quite the case here. It seems like there’s some slight elbow-ing at 5 clusters, so let’s just go ahead and choose that. Now we can look at the patterns of exchanges in each of these clusters.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office_clustered &amp;lt;- augment(clusters_fit$km_fit[[5]], data = office_exchanges)
clusters_long &amp;lt;- office_clustered %&amp;gt;%
  mutate(season = as_factor(season)) %&amp;gt;%
  group_by(.cluster) %&amp;gt;%
  summarize_if(is.numeric, mean, na.rm = TRUE) %&amp;gt;%
  ungroup() %&amp;gt;%
  pivot_longer(cols = -c(&amp;quot;.cluster&amp;quot;),
               names_to = &amp;quot;chars&amp;quot;,
               values_to = &amp;quot;lines&amp;quot;)
clusters_long %&amp;gt;%
  ggplot(aes(x = lines, y = chars, fill = .cluster)) +
    geom_col() +
    facet_wrap(~.cluster, ncol = 2, scales = &amp;quot;free_y&amp;quot;) +
    #scale_y_reordered() +
    scale_fill_ipsum() +
    theme_minimal() +
    labs(
      title = &amp;quot;Types of Office Episodes&amp;quot;
    ) +
    theme(
      legend.position = &amp;quot;none&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file22782ecc6e88_files/figure-html/unnamed-chunk-1-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;So, these plots show us the average number of exchanges between characters by cluster. Cluster 1 episodes seem to center around exchanges between Michael and Pam, and we also see a fair amount of exchanges between Michael &amp;amp; Jim, Michael &amp;amp; Dwight, and Jim &amp;amp; Pam. Cluster 2 episodes overwhelmingly feature interactions between Michael and Dwight. Cluster 3 episodes have relatively few exchanges between all of our main characters – this probably means that there’s a lot of side character action going on (recall that we didn’t include exchanges between anyone other than Michael, Dwight, Jim, Pam, and Andy in our clustering algorithm). Cluster 4 episodes have a lot of Michael and Andy interactions, along with a fair number of Michael-Dwight and Jim-Pam interactions. And Cluster 5 seems to be predominantly Michael and Jim, but also a fair amount of Michael-Dwight and Dwight-Jim, which makes sense. Usually when Jim talks to Michael in the show, Dwight finds a way to intrude.&lt;/p&gt;
&lt;p&gt;One thing to remember is that these clusters aren’t necessarily balanced. As the table below shows, &lt;em&gt;most&lt;/em&gt; episodes fit into Cluster 3.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office_clustered %&amp;gt;%
  count(.cluster, name = &amp;quot;num_episodes&amp;quot;) %&amp;gt;%
  kable(format = &amp;quot;html&amp;quot;) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;condensed&amp;quot;, &amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-condensed table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
.cluster
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
num_episodes
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
2
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
60
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
4
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
5
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Another thing to keep in mind is that, across the all of the characters, Michael has far and away the most lines, so his interactions tend to drive this clustering. If we centered and scaled our variables, this would likely change, but we’d also lose some of the interpretability that comes with working in the raw metrics.&lt;/p&gt;
&lt;p&gt;Finally, let’s just choose a random episode from each cluster to see which episodes are falling into which categories.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office_clustered %&amp;gt;%
  group_by(.cluster) %&amp;gt;%
  sample_n(size = 1) %&amp;gt;%
  select(.cluster, season, episode_name) %&amp;gt;%
  kable(format = &amp;quot;html&amp;quot;) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;striped&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-hover table-condensed table-striped" style="margin-left: auto; margin-right: auto;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
.cluster
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
season
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
episode_name
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Women’s Appreciation
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
2
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
The Coup
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Diwali
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
4
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
7
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
Andy’s Play
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
5
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
The Merger
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That’s all for now. I might do one more with some predictive modeling in the future.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>1fc317e881ce83ad018466e8ab0575ed</distill:md5>
      <guid>https://www.ericekholm.com/posts/2021-01-04-scrantonicity-part-2</guid>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-04-scrantonicity-part-2/scrantonicity-part-2_files/figure-html5/cluster-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Riddler Express - March 20, 2020</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-04-riddler-express-march-20-2020</link>
      <description>


&lt;p&gt;One of my personal goals for 2020 is to improve my proficiency doing data-y things – mostly using R, but potentially other software as well. Typically, I’ve been using data from the &lt;a href="https://github.com/rfordatascience/tidytuesday/blob/master/README.md"&gt;#TidyTuesday&lt;/a&gt; project to practice data visualization and data from &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;, personal research projects, and other potentially interesting datasets to work on statistical modeling. I recently discovered &lt;a href="https://fivethirtyeight.com/tag/the-riddler/"&gt;The Riddler&lt;/a&gt; series – a weekly math/logic puzzle – that seems to be a good medium for brushing up on other skills (e.g. certain types of math and programming) that may not come up as often when I do visualizations or statistics.&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Anyway, this post solves the &lt;a href="https://fivethirtyeight.com/features/how-many-sets-of-cards-can-you-find/"&gt;Riddler Express puzzle from March 20, 2020&lt;/a&gt;. The problem is this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A manager is trying to produce sales of his company’s widget, so he instructs his team to hold a sale every morning, lowering the price of the widget by 10 percent. However, he gives very specific instructions as to what should happen in the afternoon: Increase the price by 10 percent from the sale price, with the (incorrect) idea that it would return it to the original price. The team follows his instructions quite literally, lowering and then raising the price by 10 percent every day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;After N days, the manager walks through the store in the evening, horrified to see that the widgets are marked more than 50 percent off of their original price. What is the smallest possible value of N?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’ll walk through a couple of ways to solve this – first, I’ll solve it algebraically, and next, I’ll solve it by “brute force” using the &lt;code&gt;accumulate()&lt;/code&gt; function from the &lt;code&gt;{purrr}&lt;/code&gt; package.&lt;/p&gt;
&lt;h2 id="solving-algebraically"&gt;Solving Algebraically&lt;/h2&gt;
&lt;p&gt;So, the first thing that strikes me when reading this is that it’s essentially a compounding interest problem, except in this case the interest is negative. That is, rather than &lt;em&gt;gaining&lt;/em&gt; value exponentially over the number of compounding periods, we’re &lt;em&gt;losing&lt;/em&gt; value exponentially. The formula for calculating compound interest is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[A = P(1 + r)^n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;A&lt;/em&gt; equals the final amount, &lt;em&gt;P&lt;/em&gt; equals the principal (our initial value), &lt;em&gt;r&lt;/em&gt; equals the interest rate, and &lt;em&gt;n&lt;/em&gt; equals the number of compounding periods (the number of days in this case). We’re interested in solving for the value of &lt;em&gt;n&lt;/em&gt; where our final amount, &lt;em&gt;A&lt;/em&gt;, is less than .5. Our principal amount, &lt;em&gt;P&lt;/em&gt;, in this case, is 1 (i.e. 100% of the value). So, our equation looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[.5 &amp;gt; ((1-1*.1)*1.1)^n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The internal logic here is that we subtract 10% from our initial value (1-1*.1) to represent the 10% decrease in price in the morning, then multiply this resulting value by 1.1 to represent the subsequent 10&amp;amp; increase in price in the afternoon. This simplifies to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[.5 &amp;gt; .99^n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From here, we can just solve by taking the log of each side and then dividing, which get us our answer&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- log(.5)/log(.99)
n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 68.96756&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rounding this up (since we’re dealing in full days), we can say that after &lt;strong&gt;69&lt;/strong&gt; days, the price of the widget will be below 50% of its initial price.&lt;/p&gt;
&lt;h2 id="solving-using-accumulate"&gt;Solving using &lt;code&gt;accumulate()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We can also solve this problem using the &lt;code&gt;accumulate()&lt;/code&gt; function from the &lt;code&gt;{purrr}&lt;/code&gt; package, which is part of the &lt;code&gt;{tidyverse}&lt;/code&gt;. Essentially, &lt;code&gt;accumulate()&lt;/code&gt; will take a function, evaluate it, and then pass the result of the evaluation back into the function, evaluate it again, pass the new result back into the function, etc. This makes it useful for solving problems like this one, where the end price of the widget on the previous day is the starting price of the widget on the current day.&lt;/p&gt;
&lt;p&gt;First, let’s load our packages. For this, we’ll just use &lt;code&gt;{tidyverse}&lt;/code&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s set up a function that, if we give it the price of the widget at the beginning of the day, will calculate the price of the widget at the end of the day.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discount_func &amp;lt;- function(x) {
  (x-x*.1)*1.1
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then let’s test this function manually a few times.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discount_func(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.99&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;discount_func(.99)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9801&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;discount_func(.9801)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.970299&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can use &lt;code&gt;accumulate()&lt;/code&gt; to automate what we just did manually. The first argument in &lt;code&gt;accumulate()&lt;/code&gt; is, in this case, each day that we want to pass into the function. In the code below, I’m testing this for days 0-3 (but coded as 1-4 because we want the start value to be 1). The second argument is the function we just wrote.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;accumulate(1:4, ~discount_func(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.000000 0.990000 0.980100 0.970299&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can see that the values returned match our manual tests above, which is good!&lt;/p&gt;
&lt;p&gt;Now, we can use &lt;code&gt;accumulate()&lt;/code&gt; to make a table with the end price of the widget each day. &lt;em&gt;Note that because we want to start the widget price at 1, our first “day” in the table is day 0, which represents the beginning price of the widget on day 1.&lt;/em&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;days_tbl &amp;lt;- tibble(
  day = c(0:1000),
  end_price = accumulate(c(1:1001), ~discount_func(.))
)
head(days_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 2
    day end_price
  &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
1     0     1    
2     1     0.99 
3     2     0.980
4     3     0.970
5     4     0.961
6     5     0.951&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we can plot the end price over time. I’ve added a little bit of transparency to each point so we can more easily see the clustering/overlap.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(days_tbl, aes(x = day, y = end_price)) +
  geom_point(alpha = .3) +
  theme_minimal() +
  labs(
    title = &amp;quot;End Price of Widget over Time&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file227864231948_files/figure-html/plot%20results-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can find the day where the end price is below .5 by filtering our table to only those where the price is less than .5 and then returning the first row.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;days_tbl %&amp;gt;%
  filter(end_price &amp;lt;= .5) %&amp;gt;%
  slice(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 2
    day end_price
  &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
1    69     0.500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can see that this matches our algebraic result – great success!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>a4ba8e383c5cde6c4b9ccfdfb1879fc1</distill:md5>
      <guid>https://www.ericekholm.com/posts/2021-01-04-riddler-express-march-20-2020</guid>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-04-riddler-express-march-20-2020/riddler-express-march-20-2020_files/figure-html5/plot results-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Scrantonicity - Part 1</title>
      <dc:creator>EE</dc:creator>
      <link>https://www.ericekholm.com/posts/2021-01-02-scrantonicity-part-1</link>
      <description>


&lt;h2 id="i-just-want-to-lie-on-the-beach-and-eat-hog-dogs"&gt;I Just Want to Lie on the Beach and Eat Hog Dogs&lt;/h2&gt;
&lt;p&gt;Who doesn’t love &lt;em&gt;The Office&lt;/em&gt;? I went through high school and college following the on-again off-again romance of Jim and Pam, the Icarus-esque ascendancy and fall of Ryan the Temp, and the perpetual cringey-ness of Michael Scott. And aside from that handful of people who fled the room in a cold panic at even the mention of “Scott’s Tots,” I think this was probably true for most of my generation. You’d be hard pressed to go to a Halloween party in the late aughts without seeing someone dressed in the tan-and-yellow palette of Dwight Schrute, and before the modern era of Netflix and Hulu, we regularly set aside Thursday nights to tune into NBC.&lt;/p&gt;
&lt;p&gt;And although I was a big &lt;em&gt;Office&lt;/em&gt; fan several years ago, I haven’t thought too too much about it recently – at least until I stumbled across the release of the &lt;code&gt;{schrute}&lt;/code&gt; package recently. &lt;a href="https://CRAN.R-project.org/package=schrute"&gt;&lt;code&gt;{schrute}&lt;/code&gt;&lt;/a&gt; is an R package with one purpose – presenting the entire transcripts of &lt;em&gt;The Office&lt;/em&gt; in tibble format, making the dialogue of the show much easier to analyze. I played around with the package and a &lt;a href="https://github.com/ekholme/TidyTuesday/blob/master/53%20-%20the%20office/jim%20pam%20script.Rmd"&gt;quick sentiment analysis&lt;/a&gt; back in December when I looked at the sentiments expressed by Jim and Pam over the course of the series:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ekholme/TidyTuesday/blob/master/53%20-%20the%20office/jim_pam_sentiments.jpg?raw=true"&gt;&lt;img src="https://github.com/ekholme/TidyTuesday/blob/master/53%20-%20the%20office/jim_pam_sentiments.jpg?raw=true" style="width:100.0%" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There’s a ton more we can do with the package, though, and with the transcripts available and in a clean format, plus all of the tools &lt;code&gt;R&lt;/code&gt; has available for text analysis, I figured I’d do a mini-series of blog posts analyzing some of the data. The plan (as of now) is to start this first post with some exploratory analyses and visualizations, then move into some other types of modeling in later posts. I’ll also include all of my code throughout.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;As a quick aside, a lot of the text analyses I’m going to work through in this first post come from the &lt;a href="https://www.tidytextmining.com/"&gt;Text Mining with R book by Julia Silge and David Robinson.&lt;/a&gt; I’d strongly recommend this to anyone looking to dive into analyzing text data.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;p&gt;First, let’s read in the data. I’m also going to limit the data to the first seven seasons, which spans the “Michael Scott” era. Not only because these are the best seasons (which they undoubtedly are), but also because doing so eliminates a major confounding factor (i.e. Steve Carell leaving the show) from the analysis.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office &amp;lt;- theoffice %&amp;gt;%
  filter(as.numeric(season) &amp;lt;= 7)

glimpse(office)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 41,348
Columns: 12
$ index            &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...
$ season           &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
$ episode          &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
$ episode_name     &amp;lt;chr&amp;gt; &amp;quot;Pilot&amp;quot;, &amp;quot;Pilot&amp;quot;, &amp;quot;Pilot&amp;quot;, &amp;quot;Pilot&amp;quot;, &amp;quot;Pil...
$ director         &amp;lt;chr&amp;gt; &amp;quot;Ken Kwapis&amp;quot;, &amp;quot;Ken Kwapis&amp;quot;, &amp;quot;Ken Kwapis&amp;quot;...
$ writer           &amp;lt;chr&amp;gt; &amp;quot;Ricky Gervais;Stephen Merchant;Greg Dan...
$ character        &amp;lt;chr&amp;gt; &amp;quot;Michael&amp;quot;, &amp;quot;Jim&amp;quot;, &amp;quot;Michael&amp;quot;, &amp;quot;Jim&amp;quot;, &amp;quot;Mic...
$ text             &amp;lt;chr&amp;gt; &amp;quot;All right Jim. Your quarterlies look ve...
$ text_w_direction &amp;lt;chr&amp;gt; &amp;quot;All right Jim. Your quarterlies look ve...
$ imdb_rating      &amp;lt;dbl&amp;gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, ...
$ total_votes      &amp;lt;int&amp;gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706...
$ air_date         &amp;lt;fct&amp;gt; 2005-03-24, 2005-03-24, 2005-03-24, 2005...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to check that the data we have matches what we’re expecting, let’s take a look at which seasons we have, plus how many episodes we have per season.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office %&amp;gt;%
  distinct(season, episode) %&amp;gt;%
  count(season, name = &amp;quot;num_episodes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 7 x 2
  season num_episodes
*  &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;
1      1            6
2      2           22
3      3           23
4      4           14
5      5           26
6      6           24
7      7           24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This generally matches what Wikipedia is telling me once we account for two-part episodes, and we can see that we only have the first seven seasons.&lt;/p&gt;
&lt;h3 id="me-think-why-waste-time-say-lot-word-when-few-word-do-trick"&gt;Me think, why waste time say lot word, when few word do trick&lt;/h3&gt;
&lt;p&gt;A few questions we can ask here involve how much/how often different characters speak. Probably the most basic question is: who has the most lines?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;top_20_chars &amp;lt;- office %&amp;gt;%
  count(character, sort = TRUE) %&amp;gt;%
  top_n(20) %&amp;gt;%
  pull(character)
office %&amp;gt;%
  filter(is.element(character, top_20_chars)) %&amp;gt;%
  count(character, sort = TRUE) %&amp;gt;%
  ggplot(aes(x = fct_reorder(character, n), y = n)) +
  geom_col(fill = purple) +
  labs(
    x = &amp;quot;&amp;quot;,
    y = &amp;quot;Number of Lines&amp;quot;,
    title = &amp;quot;Who Has the Most Lines?&amp;quot;
  ) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/most%20lines-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;It’s not surprising to me that Michael has the most lines, but the magnitude of the difference between him and Dwight is a bit surprising.&lt;/p&gt;
&lt;p&gt;What if we look at the number of lines per season?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office %&amp;gt;%
  filter(is.element(character, top_20_chars)) %&amp;gt;%
  count(character, season, sort = TRUE) %&amp;gt;%
  ggplot(aes(x = as.numeric(season), y = n, color = character)) +
    geom_line() +
    geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/character%20lines%20lineplot-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This isn’t terribly informative – let’s go back to our bar graph.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office %&amp;gt;%
  filter(is.element(character, top_20_chars)) %&amp;gt;%
  count(character, season, sort = TRUE) %&amp;gt;%
  group_by(season) %&amp;gt;%
  top_n(n = 5) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(x = fct_reorder(character, n), y = n)) +
    geom_col(fill = purple) +
    coord_flip() +
    facet_wrap(~season, scales = &amp;quot;free&amp;quot;) +
    labs(
      title = &amp;quot;Number of Lines by Season&amp;quot;,
      x = &amp;quot;&amp;quot;,
      y = &amp;quot;&amp;quot;
    ) +
    theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/lines%20by%20season-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Again, not surprising that Michael has the most lines across all seasons. Dwight, Jim, and Pam are always the next three, but the orders change a bit between seasons. The fifth spot is where we see some movement, with Oscar and Jan sneaking in before Andy joins the show in Season 3. And check out Ryan in S4!&lt;/p&gt;
&lt;h3 id="sometimes-ill-start-a-sentence-and-i-dont-even-know-where-its-going"&gt;Sometimes I’ll start a sentence and I don’t even know where it’s going&lt;/h3&gt;
&lt;p&gt;So, above, we just looked at the number of &lt;em&gt;lines&lt;/em&gt; each character had. Another option is to do some analyses at the word level. For instance, we can look at patterns of word usage for individual characters, between characters, and over time.&lt;/p&gt;
&lt;p&gt;To start with this, I’m going to restructure the data so we have one word per row in our tibble. I’m also going to remove “stop words” (e.g. “a,” “the,” “at”), since these will show up a lot but (for our purposes) aren’t actually all that meaningful:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office_words &amp;lt;- office %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words)
glimpse(office_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 125,040
Columns: 12
$ index            &amp;lt;int&amp;gt; 1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 6, 6, 6, 6...
$ season           &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
$ episode          &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
$ episode_name     &amp;lt;chr&amp;gt; &amp;quot;Pilot&amp;quot;, &amp;quot;Pilot&amp;quot;, &amp;quot;Pilot&amp;quot;, &amp;quot;Pilot&amp;quot;, &amp;quot;Pil...
$ director         &amp;lt;chr&amp;gt; &amp;quot;Ken Kwapis&amp;quot;, &amp;quot;Ken Kwapis&amp;quot;, &amp;quot;Ken Kwapis&amp;quot;...
$ writer           &amp;lt;chr&amp;gt; &amp;quot;Ricky Gervais;Stephen Merchant;Greg Dan...
$ character        &amp;lt;chr&amp;gt; &amp;quot;Michael&amp;quot;, &amp;quot;Michael&amp;quot;, &amp;quot;Michael&amp;quot;, &amp;quot;Jim&amp;quot;, ...
$ text_w_direction &amp;lt;chr&amp;gt; &amp;quot;All right Jim. Your quarterlies look ve...
$ imdb_rating      &amp;lt;dbl&amp;gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, ...
$ total_votes      &amp;lt;int&amp;gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706...
$ air_date         &amp;lt;fct&amp;gt; 2005-03-24, 2005-03-24, 2005-03-24, 2005...
$ word             &amp;lt;chr&amp;gt; &amp;quot;jim&amp;quot;, &amp;quot;quarterlies&amp;quot;, &amp;quot;library&amp;quot;, &amp;quot;told&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that we have a new column, &lt;code&gt;word&lt;/code&gt;, with one word per row. We can also see that the only words in the first line of dialogue (All right Jim. Your quarterlies look very good. How are things at the library?) that make it through the stop words filter are &lt;code&gt;jim&lt;/code&gt;, &lt;code&gt;quarterlies&lt;/code&gt;, and &lt;code&gt;library&lt;/code&gt;. We could fiddle with the stop words list if we wanted to keep words like “good” or “things,” but I’m not too concerned about that for now.&lt;/p&gt;
&lt;p&gt;As a first pass, let’s take a look at our 20 characters with the most lines of dialogue and see what each character’s most commonly-used word is:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office_words %&amp;gt;%
  filter(is.element(character, top_20_chars)) %&amp;gt;%
  count(character, word, sort = TRUE) %&amp;gt;%
  group_by(character) %&amp;gt;%
  top_n(n = 1) %&amp;gt;%
  kable(format = &amp;quot;html&amp;quot;) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;hover&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-condensed table-hover" style="margin-left: auto; margin-right: auto;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
character
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
word
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Michael
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
563
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Dwight
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
280
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Jim
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
274
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Pam
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
257
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Jan
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
159
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Andy
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
138
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Kevin
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
79
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
David
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
67
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Ryan
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
66
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Oscar
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Phyllis
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
59
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Toby
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Darryl
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
na
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
48
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Kelly
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
god
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Angela
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
dwight
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Holly
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
39
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Erin
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
37
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Karen
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
yeah
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Stanley
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
michael
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Meredith
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
wait
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So, that’s not great. We can see that our stop words didn’t pick up “yeah.” One way around this would be to filter out additional words like “yeah,” “hey,” etc. that aren’t in our stop words list. But we’ll probably still leave out some common words that we might not want to show up in our exploration. A better approach is probably to use the tf-idf statistics (term frequency-inverse document frequency), which adjusts the weight a term is given in the analysis for each character by how commonly the word is used by all characters, with more common words getting lower weights. Essentially, this lets us figure out which words are important/unique to each of our characters.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office_words %&amp;gt;%
  filter(is.element(character, top_20_chars)) %&amp;gt;%
  count(character, word, sort = TRUE) %&amp;gt;%
  bind_tf_idf(word, character, n) %&amp;gt;%
  group_by(character) %&amp;gt;%
  top_n(n = 5, wt = tf_idf) %&amp;gt;%
  slice(1:5) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot() +
    geom_col(aes(x = reorder_within(word, tf_idf, within = character), y = tf_idf), fill = purple) +
    facet_wrap(~character, scales = &amp;quot;free&amp;quot;) +
    coord_flip() +
    scale_x_reordered() +
    theme_minimal() +
    labs(
      x = &amp;quot;&amp;quot;,
      y = &amp;quot;&amp;quot;,
      title = &amp;quot;Which Words are Important to Which Characters?&amp;quot;
    ) +
    theme(
      axis.text.x = element_blank()
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/character%20common%20words-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This looks right – we see that “tuna” and “nard” are important to Andy, which totally makes sense. Some other gems in here are “wuphf” for Ryan, “wimowheh” for Jim, and “awesome” for Kevin.&lt;/p&gt;
&lt;p&gt;Next, let’s take a closer look at how Michael’s speech compares to some of the other main characters – Dwight, Jim, and Pam. We’ll also leave Kelly in here because I think she’ll be interesting to compare to Michael.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;main_char_words &amp;lt;-  office_words %&amp;gt;%
  filter(character %in% c(&amp;quot;Michael&amp;quot;, &amp;quot;Dwight&amp;quot;, &amp;quot;Jim&amp;quot;, &amp;quot;Pam&amp;quot;, &amp;quot;Kelly&amp;quot;),
         str_detect(word, &amp;quot;\\d+&amp;quot;, negate = TRUE)) %&amp;gt;%
  count(character, word) %&amp;gt;%
  group_by(character) %&amp;gt;%
  mutate(word_prop = n/sum(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(-n) %&amp;gt;%
  pivot_wider(names_from = character,
              values_from = word_prop)
char_plot &amp;lt;- function(df, char) {
  df %&amp;gt;%
  select(word, Michael, {{char}}) %&amp;gt;%
  mutate(color = log(abs(Michael-{{char}}))) %&amp;gt;%
  ggplot(aes(y = Michael, x = {{char}})) +
    geom_text(aes(label = word, color = color), check_overlap = TRUE, vjust = 1) +
    geom_abline(color = &amp;quot;grey50&amp;quot;, lty = 2) +
    scale_x_log10(labels = scales::percent_format()) +
    scale_y_log10(labels = scales::percent_format()) +
    scale_color_distiller(
      type = &amp;quot;seq&amp;quot;,
      palette = &amp;quot;Purples&amp;quot;,
      direction = 1
    ) +
    theme_minimal() +
    theme(
      legend.position = &amp;quot;none&amp;quot;
    )
}
main_char_words %&amp;gt;%
  char_plot(Dwight)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/dwight%20words-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Ok, so let’s walk through how to read this. For a given word, the y-axis shows how frequently Michael uses that word, and the x-axis shows how frequently Dwight uses that word. The diagonal dotted line represents equal usage – words that appear on or close to the line are words that Michael and Dwight use about as frequently as one another. Words &lt;em&gt;above&lt;/em&gt; the line are those that Michael uses more; words &lt;em&gt;below&lt;/em&gt; the line are those that Dwight uses more. Words closer to the line will appear lighter in the graph, whereas words farther way will have more color. So, looking at the graph, we can see that Dwight and Michael both say “hey” pretty often and use the word more or less equally. Dwight says “Mose” way more often than Michael does (because it’s farther from the line), whereas Michael says “Scott” more often than Dwight.&lt;/p&gt;
&lt;p&gt;Let’s take a look at what these graphs look like for Jim and Pam&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;main_char_words %&amp;gt;%
  char_plot(Jim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/jim%20graph-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;main_char_words %&amp;gt;%
  char_plot(Pam)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/pam%20graph-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Aand let’s throw Kelly in there too because it might be interesting.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;main_char_words %&amp;gt;%
  char_plot(Kelly)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/kelly%20plot-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;What we see here is that, at least when compared to Michael, Kelly’s speech is pretty idiosyncratic – there are lots of words (“blah”, “bitch”, “god”) that she uses waaaayy more frequently than Michael does.&lt;/p&gt;
&lt;p&gt;And finally (for this section), I would be remiss if I made it through an analysis of how characters from &lt;em&gt;The Office&lt;/em&gt; speak without giving a “that’s what she said” tally…&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;office %&amp;gt;%
  filter(str_detect(text, &amp;quot;what she said&amp;quot;)) %&amp;gt;%
  count(character) %&amp;gt;%
  ggplot(aes(x = fct_reorder(character, n), y = n)) +
    geom_col(fill = purple) +
    labs(
      x = &amp;quot;&amp;quot;,
      y = &amp;quot;Count&amp;quot;,
      title = &amp;quot;That&amp;#39;s What She Said!&amp;quot;
    ) +
    coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/thats%20what%20she%20said-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Not at all a surprise….&lt;/p&gt;
&lt;h3 id="identity-theft-is-not-a-joke-jim"&gt;Identity theft is not a joke, Jim!&lt;/h3&gt;
&lt;p&gt;Finally, I want to visualize who characters talk to. To do this, I’m going to put together a network plot showing links between characters based on how frequently they interact.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(0408)
office_links &amp;lt;- office %&amp;gt;%
  filter(character %in% top_20_chars) %&amp;gt;%
  group_by(episode) %&amp;gt;%
  mutate(to = lead(character)) %&amp;gt;%
  ungroup() %&amp;gt;%
  rename(from = character) %&amp;gt;%
  count(from, to) %&amp;gt;%
  filter(from != to,
         !is.na(to),
         n &amp;gt; 25)
office_verts &amp;lt;- office_links %&amp;gt;%
  group_by(from) %&amp;gt;%
  summarize(size = log(sum(n), base = 2)) %&amp;gt;%
  ungroup()
network_graph &amp;lt;- graph_from_data_frame(office_links, vertices = office_verts)
network_graph %&amp;gt;%
  ggraph(layout = &amp;quot;igraph&amp;quot;, algorithm = &amp;quot;fr&amp;quot;) +
  geom_edge_link(aes(edge_alpha = n^.5), color = purple, edge_width = 1) +
  geom_node_point(aes(size = size, color = size)) +
  geom_node_text(aes(label = name, size = size), repel = TRUE, family = &amp;quot;Garamond&amp;quot;, fontface = &amp;quot;bold&amp;quot;) +
  scale_color_distiller(
      type = &amp;quot;seq&amp;quot;,
      palette = &amp;quot;Purples&amp;quot;,
      direction = 1
    ) +
  labs(
    title = &amp;quot;Who Talks to Whom in The Office?&amp;quot;
  ) +
  theme_void() +
  theme(
    legend.position = &amp;quot;none&amp;quot;,
    plot.title = element_text(hjust = .5)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file2278589513c1_files/figure-html/network-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;The network graph shows links between characters. The size and color of the node (point) associated with a person corresponds to the the total number of interactions they have, with larger and purple-r nodes representing more interactions. The color of the link between characters also corresponds to the number of interactions between two characters, with darker purple links representing more interactions and lighter links representing fewer interactions. Also, characters with more total interactions are sorted toward the center of the network, which is where we see Michael, Jim, Pam, and Dwight. Finally, interactions are only shown if characters have more than 25 total interactions (this prevents the graph from showing a jillion lines).&lt;/p&gt;
&lt;p&gt;I’m going to wrap this one up here, but later on I’ll probably play around a bit with doing some statistical modeling – predicting who is speaking, who a character is speaking to, something like that.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>519866e27e295f2f168a081bce226533</distill:md5>
      <guid>https://www.ericekholm.com/posts/2021-01-02-scrantonicity-part-1</guid>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      <media:content url="https://www.ericekholm.com/posts/2021-01-02-scrantonicity-part-1/scrantonicity-part-1_files/figure-html5/most lines-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
  </channel>
</rss>
